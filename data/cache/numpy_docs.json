{
  "library": "numpy",
  "base_url": "https://numpy.org/doc/",
  "pages": [
    {
      "url": "https://numpy.org/doc/",
      "title": "NumPy Documentation",
      "content": "NumPy.org NumPy Documentation Web Latest (development) documentation NumPy Enhancement Proposals Versions: Numpy 2.3 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 2.2 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 2.1 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 2.0 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1.26 Manual [HTML+zip] Numpy 1.25 Manual [HTML+zip] Numpy 1.24 Manual [HTML+zip] Numpy 1.23 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1.22 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1.21 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1.20 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1.19 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1.18 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1.17 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1.16 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1.15 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1.14 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1.13 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Older versions (on scipy.org)  Copyright 2008-2025 NumPy. All rights reserved.",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy org NumPy Documentation Web Latest (development) documentation NumPy Enhancement Proposals Versions: Numpy 2 3 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 2 2 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 2 1 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 2 0 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1 26 Manual [HTML+zip] Numpy 1 25 Manual [HTML+zip] Numpy 1 24 Manual [HTML+zip] Numpy 1",
          "url": "https://numpy.org/doc/",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "23 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1 22 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1 21 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1 20 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1 19 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1 18 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1 17 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1",
          "url": "https://numpy.org/doc/",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "16 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1 15 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1 14 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Numpy 1 13 Manual [HTML+zip] [Reference Guide PDF] [User Guide PDF] Older versions (on scipy org)  Copyright 2008-2025 NumPy All rights reserved",
          "url": "https://numpy.org/doc/",
          "library": "numpy",
          "chunk_id": "numpy_2"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/",
      "title": "NumPy documentation \u2014 NumPy v2.3 Manual",
      "content": "NumPy documentation Version: 2.3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Getting started New to NumPy? Check out the Absolute Beginners Guide. It contains an introduction to NumPys main concepts and links to additional tutorials. To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation. To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts. To the reference guide Contributors guide Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy. To the contributors guide next NumPy user guide",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy documentation Version: 2 3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python",
          "url": "https://numpy.org/doc/stable/",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more Getting started New to NumPy Check out the Absolute Beginners Guide",
          "url": "https://numpy.org/doc/stable/",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "It contains an introduction to NumPys main concepts and links to additional tutorials To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy The reference describes how the methods work and which parameters can be used",
          "url": "https://numpy.org/doc/stable/",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "It assumes that you have an understanding of the key concepts To the reference guide Contributors guide Want to add to the codebase Can help add translation or a flowchart to the documentation The contributing guidelines will guide you through the process of improving NumPy To the contributors guide next NumPy user guide",
          "url": "https://numpy.org/doc/stable/",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/",
      "title": "NumPy documentation \u2014 NumPy v2.4.dev0 Manual",
      "content": "NumPy documentation Version: 2.4.dev0 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Getting started New to NumPy? Check out the Absolute Beginners Guide. It contains an introduction to NumPys main concepts and links to additional tutorials. To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation. To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts. To the reference guide Contributors guide Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy. To the contributors guide next NumPy user guide",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy documentation Version: 2 4 dev0 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python",
          "url": "https://numpy.org/devdocs/",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more Getting started New to NumPy Check out the Absolute Beginners Guide",
          "url": "https://numpy.org/devdocs/",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "It contains an introduction to NumPys main concepts and links to additional tutorials To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy The reference describes how the methods work and which parameters can be used",
          "url": "https://numpy.org/devdocs/",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "It assumes that you have an understanding of the key concepts To the reference guide Contributors guide Want to add to the codebase Can help add translation or a flowchart to the documentation The contributing guidelines will guide you through the process of improving NumPy To the contributors guide next NumPy user guide",
          "url": "https://numpy.org/devdocs/",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/2.3",
      "title": "NumPy documentation \u2014 NumPy v2.3 Manual",
      "content": "NumPy documentation Version: 2.3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Getting started New to NumPy? Check out the Absolute Beginners Guide. It contains an introduction to NumPys main concepts and links to additional tutorials. To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation. To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts. To the reference guide Contributors guide Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy. To the contributors guide next NumPy user guide",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy documentation Version: 2 3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python",
          "url": "https://numpy.org/doc/2.3",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more Getting started New to NumPy Check out the Absolute Beginners Guide",
          "url": "https://numpy.org/doc/2.3",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "It contains an introduction to NumPys main concepts and links to additional tutorials To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy The reference describes how the methods work and which parameters can be used",
          "url": "https://numpy.org/doc/2.3",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "It assumes that you have an understanding of the key concepts To the reference guide Contributors guide Want to add to the codebase Can help add translation or a flowchart to the documentation The contributing guidelines will guide you through the process of improving NumPy To the contributors guide next NumPy user guide",
          "url": "https://numpy.org/doc/2.3",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/2.2",
      "title": "NumPy documentation \u2014 NumPy v2.2 Manual",
      "content": "NumPy documentation Version: 2.2 Download documentation: User guide (PDF)  Reference guide (PDF)  All (ZIP)  Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Getting started New to NumPy? Check out the Absolute Beginners Guide. It contains an introduction to NumPys main concepts and links to additional tutorials. To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation. To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts. To the reference guide Contributors guide Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy. To the contributors guide next NumPy user guide",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy documentation Version: 2 2 Download documentation: User guide (PDF)  Reference guide (PDF)  All (ZIP)  Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python",
          "url": "https://numpy.org/doc/2.2",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more Getting started New to NumPy Check out the Absolute Beginners Guide",
          "url": "https://numpy.org/doc/2.2",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "It contains an introduction to NumPys main concepts and links to additional tutorials To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy The reference describes how the methods work and which parameters can be used",
          "url": "https://numpy.org/doc/2.2",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "It assumes that you have an understanding of the key concepts To the reference guide Contributors guide Want to add to the codebase Can help add translation or a flowchart to the documentation The contributing guidelines will guide you through the process of improving NumPy To the contributors guide next NumPy user guide",
          "url": "https://numpy.org/doc/2.2",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/#main-content",
      "title": "NumPy documentation \u2014 NumPy v2.3 Manual",
      "content": "NumPy documentation Version: 2.3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Getting started New to NumPy? Check out the Absolute Beginners Guide. It contains an introduction to NumPys main concepts and links to additional tutorials. To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation. To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts. To the reference guide Contributors guide Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy. To the contributors guide next NumPy user guide",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy documentation Version: 2 3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python",
          "url": "https://numpy.org/doc/stable/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more Getting started New to NumPy Check out the Absolute Beginners Guide",
          "url": "https://numpy.org/doc/stable/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "It contains an introduction to NumPys main concepts and links to additional tutorials To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy The reference describes how the methods work and which parameters can be used",
          "url": "https://numpy.org/doc/stable/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "It assumes that you have an understanding of the key concepts To the reference guide Contributors guide Want to add to the codebase Can help add translation or a flowchart to the documentation The contributing guidelines will guide you through the process of improving NumPy To the contributors guide next NumPy user guide",
          "url": "https://numpy.org/doc/stable/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/user/index.html",
      "title": "NumPy user guide \u2014 NumPy v2.3 Manual",
      "content": "NumPy user guide NumPy user guide This guide is an overview and explains the important features; details are found in NumPy reference. Getting started What is NumPy? Installation NumPy quickstart NumPy: the absolute basics for beginners Fundamentals and usage NumPy fundamentals Array creation Indexing on ndarrays I/O with NumPy Data types Broadcasting Copies and views Working with Arrays of Strings And Bytes Structured arrays Universal functions (ufunc) basics NumPy for MATLAB users NumPy tutorials NumPy how-tos Advanced usage and interoperability Using NumPy C-API F2PY user guide and reference manual Under-the-hood documentation for developers Interoperability with NumPy previous NumPy documentation next What is NumPy?",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy user guide NumPy user guide This guide is an overview and explains the important features; details are found in NumPy reference Getting started What is NumPy",
          "url": "https://numpy.org/doc/stable/user/index.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Installation NumPy quickstart NumPy: the absolute basics for beginners Fundamentals and usage NumPy fundamentals Array creation Indexing on ndarrays I/O with NumPy Data types Broadcasting Copies and views Working with Arrays of Strings And Bytes Structured arrays Universal functions (ufunc) basics NumPy for MATLAB users NumPy tutorials NumPy how-tos Advanced usage and interoperability Using NumPy C-API F2PY user guide and reference manual Under-the-hood documentation for developers Interoperability with NumPy previous NumPy documentation next What is NumPy",
          "url": "https://numpy.org/doc/stable/user/index.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/reference/index.html",
      "title": "NumPy reference \u2014 NumPy v2.3 Manual",
      "content": "NumPy reference NumPy reference Release: 2.3 Date: June 09, 2025 This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do. For learning how to use NumPy, see the complete documentation. Python API NumPys module structure Array objects The N-dimensional array (ndarray) Scalars Data type objects (dtype) Data type promotion in NumPy Iterating over arrays Standard array subclasses Masked arrays The array interface protocol Datetimes and timedeltas Universal functions (ufunc) Routines and objects by topic Constants Array creation routines Array manipulation routines Bit-wise operations String functionality Datetime support functions Data type routines Mathematical functions with automatic domain Floating point error handling Exceptions and Warnings Discrete Fourier Transform Functional programming Input and output Indexing routines Linear algebra Logic functions Masked array operations Mathematical functions Miscellaneous routines Polynomials Random sampling Set routines Sorting, searching, and counting Statistics Test support Window functions Typing (numpy.typing) Packaging C API NumPy C-API Python types and C-structures System configuration Data type API Array API Array iterator API ufunc API Generalized universal function API NpyString API NumPy core math library Datetime API C API deprecations Memory management in NumPy Other topics Array API standard compatibility CPU/SIMD optimizations Thread Safety Global Configuration Options NumPy security Status of numpy.distutils and migration advice numpy.distutils user guide NumPy and SWIG Acknowledgements Large parts of this manual originate from Travis E. Oliphants book Guide to NumPy (which generously entered Public Domain in August 2008). The reference documentation for many of the functions are written by numerous contributors and developers of NumPy. previous NumPy license next NumPys module structure On this page Python API C API Other topics Acknowledgements",
      "code_blocks": [
        "numpy.typing",
        "numpy.distutils",
        "numpy.distutils"
      ],
      "chunks": [
        {
          "content": "NumPy reference NumPy reference Release: 2 3 Date: June 09, 2025 This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do For learning how to use NumPy, see the complete documentation",
          "url": "https://numpy.org/doc/stable/reference/index.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Python API NumPys module structure Array objects The N-dimensional array (ndarray) Scalars Data type objects (dtype) Data type promotion in NumPy Iterating over arrays Standard array subclasses Masked arrays The array interface protocol Datetimes and timedeltas Universal functions (ufunc) Routines and objects by topic Constants Array creation routines Array manipulation routines Bit-wise operations String functionality Datetime support functions Data type routines Mathematical functions with automatic domain Floating point error handling Exceptions and Warnings Discrete Fourier Transform Functional programming Input and output Indexing routines Linear algebra Logic functions Masked array operations Mathematical functions Miscellaneous routines Polynomials Random sampling Set routines Sorting, searching, and counting Statistics Test support Window functions Typing (numpy",
          "url": "https://numpy.org/doc/stable/reference/index.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "typing) Packaging C API NumPy C-API Python types and C-structures System configuration Data type API Array API Array iterator API ufunc API Generalized universal function API NpyString API NumPy core math library Datetime API C API deprecations Memory management in NumPy Other topics Array API standard compatibility CPU/SIMD optimizations Thread Safety Global Configuration Options NumPy security Status of numpy distutils and migration advice numpy",
          "url": "https://numpy.org/doc/stable/reference/index.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "distutils user guide NumPy and SWIG Acknowledgements Large parts of this manual originate from Travis E Oliphants book Guide to NumPy (which generously entered Public Domain in August 2008) The reference documentation for many of the functions are written by numerous contributors and developers of NumPy previous NumPy license next NumPys module structure On this page Python API C API Other topics Acknowledgements",
          "url": "https://numpy.org/doc/stable/reference/index.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/building/index.html",
      "title": "Building from source \u2014 NumPy v2.3 Manual",
      "content": "Building from source Building from source Note If you are only trying to install NumPy, we recommend using binaries - see Installation for details on that. Building NumPy from source requires setting up system-level dependencies (compilers, BLAS/LAPACK libraries, etc.) first, and then invoking a build. The build may be done in order to install NumPy for local usage, develop NumPy itself, or build redistributable binary packages. And it may be desired to customize aspects of how the build is done. This guide will cover all these aspects. In addition, it provides background information on how the NumPy build works, and links to up-to-date guides for generic Python build  packaging documentation that is relevant. System-level dependencies NumPy uses compiled code for speed, which means you need compilers and some other system-level (i.e, non-Python / non-PyPI) dependencies to build it on your system. Note If you are using Conda, you can skip the steps in this section - with the exception of installing compilers for Windows or the Apple Developer Tools for macOS. All other dependencies will be installed automatically by the mamba env create -f environment.yml command. Linux If you want to use the system Python and pip, you will need: C and C++ compilers (typically GCC). Python header files (typically a package named python3-dev or python3-devel) BLAS and LAPACK libraries. OpenBLAS is the NumPy default; other variants include Apple Accelerate, MKL, ATLAS and Netlib (or Reference) BLAS and LAPACK. pkg-config for dependency detection. A Fortran compiler is needed only for running the f2py tests. The instructions below include a Fortran compiler, however you can safely leave it out. Debian/Ubuntu Linux To install NumPy build requirements, you can do: sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev Alternatively, you can do: sudo apt build-dep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. Fedora To install NumPy build requirements, you can do: sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo dnf builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. CentOS/RHEL To install NumPy build requirements, you can do: sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo yum-builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. Arch To install NumPy build requirements, you can do: sudo pacman -S gcc-fortran openblas pkgconf macOS Install Apple Developer Tools. An easy way to do this is to open a terminal window, enter the command: xcode-select --install and follow the prompts. Apple Developer Tools includes Git, the Clang C/C++ compilers, and other development utilities that may be required. Do not use the macOS system Python. Instead, install Python with the python.org installer or with a package manager like Homebrew, MacPorts or Fink. On macOS =13.3, the easiest build option is to use Accelerate, which is already installed and will be automatically used by default. On older macOS versions you need a different BLAS library, most likely OpenBLAS, plus pkg-config to detect OpenBLAS. These are easiest to install with Homebrew: brew install openblas pkg-config gfortran Windows On Windows, the use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together. If you dont need to run the f2py tests, simply using MSVC is easiest. Otherwise, you will need one of these sets of compilers: MSVC + Intel Fortran (ifort) Intel compilers (icc, ifort) Mingw-w64 compilers (gcc, g++, gfortran) Compared to macOS and Linux, building NumPy on Windows is a little more difficult, due to the need to set up these compilers. It is not possible to just call a one-liner on the command prompt as you would on other platforms. First, install Microsoft Visual Studio - the 2019 Community Edition or any newer version will work (see the Visual Studio download site). This is needed even if you use the MinGW-w64 or Intel compilers, in order to ensure you have the Windows Universal C Runtime (the other components of Visual Studio are not needed when using Mingw-w64, and can be deselected if desired, to save disk space). The recommended version of the UCRT is = 10.0.22621.0. MSVC The MSVC installer does not put the compilers on the system path, and the install location may change. To query the install location, MSVC comes with a vswhere.exe command-line utility. And to make the C/C++ compilers available inside the shell you are using, you need to run a .bat file for the correct bitness and architecture (e.g., for 64-bit Intel CPUs, use vcvars64.bat). If using a Conda environment while a version of Visual Studio 2019+ is installed that includes the MSVC v142 package (VS 2019 C++ x86/x64 build tools), activating the conda environment should cause Visual Studio to be found and the appropriate .bat file executed to set these variables. For detailed guidance, see Use the Microsoft C++ toolset from the command line. Intel Similar to MSVC, the Intel compilers are designed to be used with an activation script (Intel\\oneAPI\\setvars.bat) that you run in the shell you are using. This makes the compilers available on the path. For detailed guidance, see Get Started with the Intel oneAPI HPC Toolkit for Windows. MinGW-w64 There are several sources of binaries for MinGW-w64. We recommend the RTools versions, which can be installed with Chocolatey (see Chocolatey install instructions here): choco install rtools -y --no-progress --force --version=4.0.0.20220206 Note Compilers should be on the system path (i.e., the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH. You can use any shell (e.g., Powershell, cmd or Git Bash) to invoke a build. To check that this is the case, try invoking a Fortran compiler in the shell you use (e.g., gfortran --version or ifort --version). Warning When using a conda environment it is possible that the environment creation will not work due to an outdated Fortran compiler. If that happens, remove the compilers entry from environment.yml and try again. The Fortran compiler should be installed as described in this section. Windows on ARM64 In Windows on ARM64, the set of a compiler options that are available for building NumPy are limited. Compilers such as GCC and GFortran are not yet supported for Windows on ARM64. Currently, the NumPy build for Windows on ARM64 is supported with MSVC and LLVM toolchains. The use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together. If you dont need to run the f2py tests, simply using MSVC is easiest. Otherwise, you will need the following set of compilers: MSVC + flang (cl, flang) LLVM + flang (clang-cl, flang) First, install Microsoft Visual Studio - the 2022 Community Edition will work (see the Visual Studio download site). Ensure that you have installed necessary Visual Studio components for building NumPy on WoA from here. To use the flang compiler for Windows on ARM64, install Latest LLVM toolchain for WoA from here. MSVC The MSVC installer does not put the compilers on the system path, and the install location may change. To query the install location, MSVC comes with a vswhere.exe command-line utility. And to make the C/C++ compilers available inside the shell you are using, you need to run a .bat file for the correct bitness and architecture (e.g., for ARM64-based CPUs, use vcvarsarm64.bat). For detailed guidance, see Use the Microsoft C++ toolset from the command line. LLVM Similar to MSVC, LLVM does not put the compilers on the system path. To set system path for LLVM compilers, users may need to use set command to put compilers on the system path. To check compilers path for LLVMs clang-cl, try invoking LLVMs clang-cl compiler in the shell you use (clang-cl --version). Note Compilers should be on the system path (i.e., the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH. You can use any shell (e.g., Powershell, cmd or Git Bash) to invoke a build. To check that this is the case, try invoking a Fortran compiler in the shell you use (e.g., flang --version). Warning Currently, Conda environment is not yet supported officially on Windows on ARM64. The present approach uses virtualenv for building NumPy from source on Windows on ARM64. Building NumPy from source If you want to only install NumPy from source once and not do any development work, then the recommended way to build and install is to use pip. Otherwise, conda is recommended. Note If you dont have a conda installation yet, we recommend using Miniforge; any conda flavor will work though. Building from source to use NumPy Conda env If you are using a conda environment, pip is still the tool you use to invoke a from-source build of NumPy. It is important to always use the --no-build-isolation flag to the pip install command, to avoid building against a numpy wheel from PyPI. In order for that to work you must first install the remaining build dependencies into the conda environment:  Either install all NumPy dev dependencies into a fresh conda environment mamba env create -f environment.yml  Or, install only the required build dependencies mamba install python numpy cython compilers openblas meson-python pkg-config  To build the latest stable release: pip install numpy --no-build-isolation --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init pip install . --no-build-isolation Warning On Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the pip install command to fail. These variables are only needed for flang and can be safely unset prior to running pip install. Virtual env or system Python  To build the latest stable release: pip install numpy --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init pip install . Building from source for NumPy development If you want to build from source in order to work on NumPy itself, first clone the NumPy repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init Then you want to do the following: Create a dedicated development environment (virtual environment or conda environment), Install all needed dependencies (build, and also test, doc and optional dependencies), Build NumPy with the spin developer interface. Step (3) is always the same, steps (1) and (2) are different between conda and virtual environments: Conda env To create a numpy-dev development environment with every required and optional dependency installed, run: mamba env create -f environment.yml mamba activate numpy-dev Virtual env or system Python Note There are many tools to manage virtual environments, like venv, virtualenv/virtualenvwrapper, pyenv/pyenv-virtualenv, Poetry, PDM, Hatch, and more. Here we use the basic venv tool that is part of the Python stdlib. You can use any other tool; all we need is an activated Python environment. Create and activate a virtual environment in a new directory named venv ( note that the exact activation command may be different based on your OS and shell - see How venvs work in the venv docs). Linux python -m venv venv source venv/bin/activate macOS python -m venv venv source venv/bin/activate Windows python -m venv venv .\\venv\\Scripts\\activate Windows on ARM64 python -m venv venv .\\venv\\Scripts\\activate Note Building NumPy with BLAS and LAPACK functions requires OpenBLAS library at Runtime. In Windows on ARM64, this can be done by setting up pkg-config for OpenBLAS dependency. The build steps for OpenBLAS for Windows on ARM64 can be found here. Then install the Python-level dependencies from PyPI with: python -m pip install -r requirements/build_requirements.txt To build NumPy in an activated development environment, run: spin build This will install NumPy inside the repository (by default in a build-install directory). You can then run tests (spin test), drop into IPython (spin ipython), or take other development steps like build the html documentation or running benchmarks. The spin interface is self-documenting, so please see spin --help and spin subcommand --help for detailed guidance. Warning In an activated conda environment on Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the build to fail. These variables are only needed for flang and can be safely unset for build. IDE support  editable installs While the spin interface is our recommended way of working on NumPy, it has one limitation: because of the custom install location, NumPy installed using spin will not be recognized automatically within an IDE (e.g., for running a script via a run button, or setting breakpoints visually). This will work better with an in-place build (or editable install). Editable installs are supported. It is important to understand that you may use either an editable install or ``spin`` in a given repository clone, but not both. If you use editable installs, you have to use pytest and other development tools directly instead of using spin. To use an editable install, ensure you start from a clean repository (run git clean -xdf if youve built with spin before) and have all dependencies set up correctly as described higher up on this page. Then do:  Note: the --no-build-isolation is important! pip install -e . --no-build-isolation  To run the tests for, e.g., the `numpy.linalg` module: pytest numpy/linalg When making changes to NumPy code, including to compiled code, there is no need to manually rebuild or reinstall. NumPy is automatically rebuilt each time NumPy is imported by the Python interpreter; see the meson-python documentation on editable installs for more details on how that works under the hood. When you run git clean -xdf, which removes the built extension modules, remember to also uninstall NumPy with pip uninstall numpy. Warning Note that editable installs are fundamentally incomplete installs. Their only guarantee is that import numpy works - so they are suitable for working on NumPy itself, and for working on pure Python packages that depend on NumPy. Headers, entrypoints, and other such things may not be available from an editable install. Customizing builds Compiler selection and customizing a build BLAS and LAPACK Cross compilation Building redistributable binaries Background information Understanding Meson Introspecting build steps Meson and distutils ways of doing things previous Testing the numpy.i typemaps next Compiler selection and customizing a build On this page System-level dependencies Building NumPy from source Building from source to use NumPy Building from source for NumPy development Customizing builds Background information",
      "code_blocks": [
        "mamba env create -f environment.yml",
        "python3-dev",
        "python3-devel",
        "sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev",
        "sudo apt build-dep numpy",
        "sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig",
        "sudo dnf builddep numpy",
        "sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig",
        "sudo yum-builddep numpy",
        "sudo pacman -S gcc-fortran openblas pkgconf",
        "xcode-select --install",
        "brew install openblas pkg-config gfortran",
        "vswhere.exe",
        "vcvars64.bat",
        "Intel\\oneAPI\\setvars.bat",
        "choco install rtools -y --no-progress --force --version=4.0.0.20220206",
        "gfortran\n--version",
        "ifort --version",
        "environment.yml",
        "vswhere.exe",
        "vcvarsarm64.bat",
        "clang-cl --version",
        "flang\n--version",
        "--no-build-isolation",
        "pip install",
        "# Either install all NumPy dev dependencies into a fresh conda environment\nmamba env create -f environment.yml\n\n# Or, install only the required build dependencies\nmamba install python numpy cython compilers openblas meson-python pkg-config\n\n# To build the latest stable release:\npip install numpy --no-build-isolation --no-binary numpy\n\n# To build a development version, you need a local clone of the NumPy git repository:\ngit clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init\npip install . --no-build-isolation",
        "# To build the latest stable release:\npip install numpy --no-binary numpy\n\n# To build a development version, you need a local clone of the NumPy git repository:\ngit clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init\npip install .",
        "git clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init",
        "mamba env create -f environment.yml\nmamba activate numpy-dev",
        "virtualenvwrapper",
        "pyenv-virtualenv",
        "python -m venv venv\nsource venv/bin/activate",
        "python -m venv venv\nsource venv/bin/activate",
        "python -m venv venv\n.\\venv\\Scripts\\activate",
        "python -m venv venv\n.\\venv\\Scripts\\activate",
        "python -m pip install -r requirements/build_requirements.txt",
        "build-install",
        "spin ipython",
        "spin --help",
        "spin <subcommand> --help",
        "git clean -xdf",
        "# Note: the --no-build-isolation is important!\npip install -e . --no-build-isolation\n\n# To run the tests for, e.g., the `numpy.linalg` module:\npytest numpy/linalg",
        "git clean -xdf",
        "pip uninstall numpy",
        "import numpy"
      ],
      "chunks": [
        {
          "content": "Building from source Building from source Note If you are only trying to install NumPy, we recommend using binaries - see Installation for details on that Building NumPy from source requires setting up system-level dependencies (compilers, BLAS/LAPACK libraries, etc ) first, and then invoking a build The build may be done in order to install NumPy for local usage, develop NumPy itself, or build redistributable binary packages And it may be desired to customize aspects of how the build is done",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "This guide will cover all these aspects In addition, it provides background information on how the NumPy build works, and links to up-to-date guides for generic Python build  packaging documentation that is relevant System-level dependencies NumPy uses compiled code for speed, which means you need compilers and some other system-level (i e, non-Python / non-PyPI) dependencies to build it on your system",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "Note If you are using Conda, you can skip the steps in this section - with the exception of installing compilers for Windows or the Apple Developer Tools for macOS All other dependencies will be installed automatically by the mamba env create -f environment yml command Linux If you want to use the system Python and pip, you will need: C and C++ compilers (typically GCC) Python header files (typically a package named python3-dev or python3-devel) BLAS and LAPACK libraries",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "OpenBLAS is the NumPy default; other variants include Apple Accelerate, MKL, ATLAS and Netlib (or Reference) BLAS and LAPACK pkg-config for dependency detection A Fortran compiler is needed only for running the f2py tests The instructions below include a Fortran compiler, however you can safely leave it out",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "Debian/Ubuntu Linux To install NumPy build requirements, you can do: sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev Alternatively, you can do: sudo apt build-dep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "Fedora To install NumPy build requirements, you can do: sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo dnf builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "CentOS/RHEL To install NumPy build requirements, you can do: sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo yum-builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers Arch To install NumPy build requirements, you can do: sudo pacman -S gcc-fortran openblas pkgconf macOS Install Apple Developer Tools",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "An easy way to do this is to open a terminal window, enter the command: xcode-select --install and follow the prompts Apple Developer Tools includes Git, the Clang C/C++ compilers, and other development utilities that may be required Do not use the macOS system Python Instead, install Python with the python org installer or with a package manager like Homebrew, MacPorts or Fink On macOS =13",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "3, the easiest build option is to use Accelerate, which is already installed and will be automatically used by default On older macOS versions you need a different BLAS library, most likely OpenBLAS, plus pkg-config to detect OpenBLAS These are easiest to install with Homebrew: brew install openblas pkg-config gfortran Windows On Windows, the use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "If you dont need to run the f2py tests, simply using MSVC is easiest Otherwise, you will need one of these sets of compilers: MSVC + Intel Fortran (ifort) Intel compilers (icc, ifort) Mingw-w64 compilers (gcc, g++, gfortran) Compared to macOS and Linux, building NumPy on Windows is a little more difficult, due to the need to set up these compilers It is not possible to just call a one-liner on the command prompt as you would on other platforms",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "First, install Microsoft Visual Studio - the 2019 Community Edition or any newer version will work (see the Visual Studio download site) This is needed even if you use the MinGW-w64 or Intel compilers, in order to ensure you have the Windows Universal C Runtime (the other components of Visual Studio are not needed when using Mingw-w64, and can be deselected if desired, to save disk space) The recommended version of the UCRT is = 10 0 22621 0",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "MSVC The MSVC installer does not put the compilers on the system path, and the install location may change To query the install location, MSVC comes with a vswhere exe command-line utility And to make the C/C++ compilers available inside the shell you are using, you need to run a bat file for the correct bitness and architecture (e g , for 64-bit Intel CPUs, use vcvars64 bat)",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "If using a Conda environment while a version of Visual Studio 2019+ is installed that includes the MSVC v142 package (VS 2019 C++ x86/x64 build tools), activating the conda environment should cause Visual Studio to be found and the appropriate bat file executed to set these variables For detailed guidance, see Use the Microsoft C++ toolset from the command line Intel Similar to MSVC, the Intel compilers are designed to be used with an activation script (Intel\\oneAPI\\setvars",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "bat) that you run in the shell you are using This makes the compilers available on the path For detailed guidance, see Get Started with the Intel oneAPI HPC Toolkit for Windows MinGW-w64 There are several sources of binaries for MinGW-w64 We recommend the RTools versions, which can be installed with Chocolatey (see Chocolatey install instructions here): choco install rtools -y --no-progress --force --version=4 0 0 20220206 Note Compilers should be on the system path (i e",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": ", the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH You can use any shell (e g , Powershell, cmd or Git Bash) to invoke a build To check that this is the case, try invoking a Fortran compiler in the shell you use (e g , gfortran --version or ifort --version)",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "Warning When using a conda environment it is possible that the environment creation will not work due to an outdated Fortran compiler If that happens, remove the compilers entry from environment yml and try again The Fortran compiler should be installed as described in this section Windows on ARM64 In Windows on ARM64, the set of a compiler options that are available for building NumPy are limited Compilers such as GCC and GFortran are not yet supported for Windows on ARM64",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "Currently, the NumPy build for Windows on ARM64 is supported with MSVC and LLVM toolchains The use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together If you dont need to run the f2py tests, simply using MSVC is easiest",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Otherwise, you will need the following set of compilers: MSVC + flang (cl, flang) LLVM + flang (clang-cl, flang) First, install Microsoft Visual Studio - the 2022 Community Edition will work (see the Visual Studio download site) Ensure that you have installed necessary Visual Studio components for building NumPy on WoA from here To use the flang compiler for Windows on ARM64, install Latest LLVM toolchain for WoA from here",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "MSVC The MSVC installer does not put the compilers on the system path, and the install location may change To query the install location, MSVC comes with a vswhere exe command-line utility And to make the C/C++ compilers available inside the shell you are using, you need to run a bat file for the correct bitness and architecture (e g , for ARM64-based CPUs, use vcvarsarm64 bat) For detailed guidance, see Use the Microsoft C++ toolset from the command line",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "LLVM Similar to MSVC, LLVM does not put the compilers on the system path To set system path for LLVM compilers, users may need to use set command to put compilers on the system path To check compilers path for LLVMs clang-cl, try invoking LLVMs clang-cl compiler in the shell you use (clang-cl --version) Note Compilers should be on the system path (i e",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": ", the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH You can use any shell (e g , Powershell, cmd or Git Bash) to invoke a build To check that this is the case, try invoking a Fortran compiler in the shell you use (e g , flang --version)",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "Warning Currently, Conda environment is not yet supported officially on Windows on ARM64 The present approach uses virtualenv for building NumPy from source on Windows on ARM64 Building NumPy from source If you want to only install NumPy from source once and not do any development work, then the recommended way to build and install is to use pip Otherwise, conda is recommended Note If you dont have a conda installation yet, we recommend using Miniforge; any conda flavor will work though",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "Building from source to use NumPy Conda env If you are using a conda environment, pip is still the tool you use to invoke a from-source build of NumPy It is important to always use the --no-build-isolation flag to the pip install command, to avoid building against a numpy wheel from PyPI",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "In order for that to work you must first install the remaining build dependencies into the conda environment:  Either install all NumPy dev dependencies into a fresh conda environment mamba env create -f environment",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "yml  Or, install only the required build dependencies mamba install python numpy cython compilers openblas meson-python pkg-config  To build the latest stable release: pip install numpy --no-build-isolation --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github com/numpy/numpy git cd numpy git submodule update --init pip install",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": "--no-build-isolation Warning On Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the pip install command to fail These variables are only needed for flang and can be safely unset prior to running pip install Virtual env or system Python  To build the latest stable release: pip install numpy --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github com/numpy/numpy",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "git cd numpy git submodule update --init pip install Building from source for NumPy development If you want to build from source in order to work on NumPy itself, first clone the NumPy repository: git clone https://github com/numpy/numpy",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "git cd numpy git submodule update --init Then you want to do the following: Create a dedicated development environment (virtual environment or conda environment), Install all needed dependencies (build, and also test, doc and optional dependencies), Build NumPy with the spin developer interface",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "Step (3) is always the same, steps (1) and (2) are different between conda and virtual environments: Conda env To create a numpy-dev development environment with every required and optional dependency installed, run: mamba env create -f environment yml mamba activate numpy-dev Virtual env or system Python Note There are many tools to manage virtual environments, like venv, virtualenv/virtualenvwrapper, pyenv/pyenv-virtualenv, Poetry, PDM, Hatch, and more",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "Here we use the basic venv tool that is part of the Python stdlib You can use any other tool; all we need is an activated Python environment Create and activate a virtual environment in a new directory named venv ( note that the exact activation command may be different based on your OS and shell - see How venvs work in the venv docs) Linux python -m venv venv source venv/bin/activate macOS python -m venv venv source venv/bin/activate Windows python -m venv venv",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "\\venv\\Scripts\\activate Windows on ARM64 python -m venv venv \\venv\\Scripts\\activate Note Building NumPy with BLAS and LAPACK functions requires OpenBLAS library at Runtime In Windows on ARM64, this can be done by setting up pkg-config for OpenBLAS dependency The build steps for OpenBLAS for Windows on ARM64 can be found here Then install the Python-level dependencies from PyPI with: python -m pip install -r requirements/build_requirements",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "txt To build NumPy in an activated development environment, run: spin build This will install NumPy inside the repository (by default in a build-install directory) You can then run tests (spin test), drop into IPython (spin ipython), or take other development steps like build the html documentation or running benchmarks The spin interface is self-documenting, so please see spin --help and spin subcommand --help for detailed guidance",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "Warning In an activated conda environment on Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the build to fail These variables are only needed for flang and can be safely unset for build IDE support  editable installs While the spin interface is our recommended way of working on NumPy, it has one limitation: because of the custom install location, NumPy installed using spin will not be recognized automatically within an IDE (e g",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": ", for running a script via a run button, or setting breakpoints visually) This will work better with an in-place build (or editable install) Editable installs are supported It is important to understand that you may use either an editable install or ``spin`` in a given repository clone, but not both If you use editable installs, you have to use pytest and other development tools directly instead of using spin",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": "To use an editable install, ensure you start from a clean repository (run git clean -xdf if youve built with spin before) and have all dependencies set up correctly as described higher up on this page Then do:  Note: the --no-build-isolation is important pip install -e --no-build-isolation  To run the tests for, e g , the `numpy linalg` module: pytest numpy/linalg When making changes to NumPy code, including to compiled code, there is no need to manually rebuild or reinstall",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "NumPy is automatically rebuilt each time NumPy is imported by the Python interpreter; see the meson-python documentation on editable installs for more details on how that works under the hood When you run git clean -xdf, which removes the built extension modules, remember to also uninstall NumPy with pip uninstall numpy Warning Note that editable installs are fundamentally incomplete installs",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "Their only guarantee is that import numpy works - so they are suitable for working on NumPy itself, and for working on pure Python packages that depend on NumPy Headers, entrypoints, and other such things may not be available from an editable install",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "Customizing builds Compiler selection and customizing a build BLAS and LAPACK Cross compilation Building redistributable binaries Background information Understanding Meson Introspecting build steps Meson and distutils ways of doing things previous Testing the numpy",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_37"
        },
        {
          "content": "i typemaps next Compiler selection and customizing a build On this page System-level dependencies Building NumPy from source Building from source to use NumPy Building from source for NumPy development Customizing builds Background information",
          "url": "https://numpy.org/doc/stable/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_38"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/dev/index.html",
      "title": "Contributing to NumPy \u2014 NumPy v2.3 Manual",
      "content": "Contributing to NumPy Contributing to NumPy Not a coder? Not a problem! NumPy is multi-faceted, and we can use a lot of help. These are all activities wed like to get help with (theyre all important, so we list them in alphabetical order): Code maintenance and development Community coordination DevOps Developing educational content  narrative documentation Fundraising Marketing Project management Translating content Website design and development Writing technical documentation We understand that everyone has a different level of experience, also NumPy is a pretty well-established project, so its hard to make assumptions about an ideal first-time-contributor. So, thats why we dont mark issues with the good-first-issue label. Instead, youll find issues labeled Sprintable. These issues can either be: Easily fixed when you have guidance from an experienced contributor (perfect for working in a sprint). A learning opportunity for those ready to dive deeper, even if youre not in a sprint. Additionally, depending on your prior experience, some Sprintable issues might be easy, while others could be more challenging for you. The rest of this document discusses working on the NumPy code base and documentation. Were in the process of updating our descriptions of other activities and roles. If you are interested in these other activities, please contact us! You can do this via the numpy-discussion mailing list, or on GitHub (open an issue or comment on a relevant issue). These are our preferred communication channels (open source is open by nature!), however if you prefer to discuss in a more private space first, you can do so on Slack (see numpy.org/contribute for details). Development process - summary Heres the short summary, complete TOC links are below: If you are a first-time contributor: Go to numpy/numpy and click the fork button to create your own copy of the project. Clone the project to your local computer: git clone --recurse-submodules https://github.com/your-username/numpy.git Change the directory: cd numpy Add the upstream repository: git remote add upstream https://github.com/numpy/numpy.git Now, git remote -v will show two remote repositories named: upstream, which refers to the numpy repository origin, which refers to your personal fork Pull the latest changes from upstream, including tags: git checkout main git pull upstream main --tags Initialize numpys submodules: git submodule update --init Develop your contribution: Create a branch for the feature you want to work on. Since the branch name will appear in the merge message, use a sensible name such as linspace-speedups: git checkout -b linspace-speedups Commit locally as you progress (git add and git commit) Use a properly formatted commit message, write tests that fail before your change and pass afterward, run all the tests locally. Be sure to document any changed behavior in docstrings, keeping to the NumPy docstring standard. To submit your contribution: Push your changes back to your fork on GitHub: git push origin linspace-speedups Go to GitHub. The new branch will show up with a green Pull Request button. Make sure the title and message are clear, concise, and self- explanatory. Then click the button to submit it. If your commit introduces a new feature or changes functionality, post on the mailing list to explain your changes. For bug fixes, documentation updates, etc., this is generally not necessary, though if you do not get any reaction, do feel free to ask for review. Review process: Reviewers (the other developers and interested community members) will write inline and/or general comments on your Pull Request (PR) to help you improve its implementation, documentation and style. Every single developer working on the project has their code reviewed, and weve come to see it as friendly conversation from which we all learn and the overall code quality benefits. Therefore, please dont let the review discourage you from contributing: its only aim is to improve the quality of project, not to criticize (we are, after all, very grateful for the time youre donating!). See our Reviewer Guidelines for more information. To update your PR, make your changes on your local repository, commit, run tests, and only if they succeed push to your fork. As soon as those changes are pushed up (to the same branch as before) the PR will update automatically. If you have no idea how to fix the test failures, you may push your changes anyway and ask for help in a PR comment. Various continuous integration (CI) services are triggered after each PR update to build the code, run unit tests, measure code coverage and check coding style of your branch. The CI tests must pass before your PR can be merged. If CI fails, you can find out why by clicking on the failed icon (red cross) and inspecting the build and test log. To avoid overuse and waste of this resource, test your work locally before committing. A PR must be approved by at least one core team member before merging. Approval means the core team member has carefully reviewed the changes, and the PR is ready for merging. Document changes Beyond changes to a functions docstring and possible description in the general documentation, if your change introduces any user-facing modifications they may need to be mentioned in the release notes. To add your change to the release notes, you need to create a short file with a summary and place it in doc/release/upcoming_changes. The file doc/release/upcoming_changes/README.rst details the format and filename conventions. If your change introduces a deprecation, make sure to discuss this first on GitHub or the mailing list first. If agreement on the deprecation is reached, follow NEP 23 deprecation policy to add the deprecation. Cross referencing issues If the PR relates to any issues, you can add the text xref gh-xxxx where xxxx is the number of the issue to github comments. Likewise, if the PR solves an issue, replace the xref with closes, fixes or any of the other flavors github accepts. In the source code, be sure to preface any issue or PR reference with gh-xxxx. For a more detailed discussion, read on and follow the links at the bottom of this page. Guidelines All code should have tests (see test coverage below for more details). All code should be documented. No changes are ever committed without review and approval by a core team member. Please ask politely on the PR or on the mailing list if you get no response to your pull request within a week. Stylistic guidelines Set up your editor to follow PEP 8 (remove trailing white space, no tabs, etc.). Check code with ruff. Use NumPy data types instead of strings (np.uint8 instead of \"uint8\"). Use the following import conventions: import numpy as np For C code, see NEP 45. Test coverage Pull requests (PRs) that modify code should either have new tests, or modify existing tests to fail before the PR and pass afterwards. You should run the tests before pushing a PR. Running NumPys test suite locally requires some additional packages, such as pytest and hypothesis. The additional testing dependencies are listed in requirements/test_requirements.txt in the top-level directory, and can conveniently be installed with:  python -m pip install -r requirements/test_requirements.txt Tests for a module should ideally cover all code in that module, i.e., statement coverage should be at 100. To measure the test coverage, run:  spin test --coverage This will create a report in html format at build/coverage, which can be viewed with your browser, e.g.:  firefox build/coverage/index.html Building docs To build the HTML documentation, use: spin docs You can also run make from the doc directory. make help lists all targets. To get the appropriate dependencies and other requirements, see Building the NumPy API and reference docs. Development process - details The rest of the story Setting up and using your development environment Recommended development setup Using virtual environments Building from source Testing builds Other build options Running tests Running linting Rebuilding  cleaning the workspace Debugging Understanding the code  getting started Building the NumPy API and reference docs Development environments Prerequisites Instructions Development workflow Basic workflow Additional things you might want to do Advanced debugging tools Finding C errors with additional tooling Using GitHub Codespaces for NumPy development GitHub Codespaces What is a codespace? Forking the NumPy repository Starting GitHub Codespaces Quick workspace tour Development workflow with GitHub Codespaces Rendering the NumPy documentation FAQs and troubleshooting Reviewer guidelines Who can be a reviewer? Communication guidelines Reviewer checklist Standard replies for reviewing NumPy benchmarks Usage Benchmarking versions Writing benchmarks NumPy C style guide For downstream package authors Understanding NumPys versioning and API/ABI stability Testing against the NumPy main branch or pre-releases Adding a dependency on NumPy Releasing a version How to prepare a release Step-by-step directions Branch walkthrough NumPy governance NumPy project governance and decision-making How to contribute to the NumPy documentation Documentation team meetings Whats needed Contributing fixes Contributing new pages Contributing indirectly Documentation style Documentation reading NumPy-specific workflow is in numpy-development-workflow. previous Meson and distutils ways of doing things next Setting up and using your development environment On this page Development process - summary Guidelines Stylistic guidelines Test coverage Building docs Development process - details",
      "code_blocks": [
        "git clone --recurse-submodules https://github.com/your-username/numpy.git",
        "git remote add upstream https://github.com/numpy/numpy.git",
        "git remote -v",
        "git checkout main\ngit pull upstream main --tags",
        "git submodule update --init",
        "git checkout -b linspace-speedups",
        "git push origin linspace-speedups",
        "doc/release/upcoming_changes",
        "doc/release/upcoming_changes/README.rst",
        "xref gh-xxxx",
        "import numpy as np",
        "requirements/test_requirements.txt",
        "$ python -m pip install -r requirements/test_requirements.txt",
        "$ spin test --coverage",
        "build/coverage",
        "$ firefox build/coverage/index.html"
      ],
      "chunks": [
        {
          "content": "Contributing to NumPy Contributing to NumPy Not a coder Not a problem NumPy is multi-faceted, and we can use a lot of help",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "These are all activities wed like to get help with (theyre all important, so we list them in alphabetical order): Code maintenance and development Community coordination DevOps Developing educational content  narrative documentation Fundraising Marketing Project management Translating content Website design and development Writing technical documentation We understand that everyone has a different level of experience, also NumPy is a pretty well-established project, so its hard to make assumptions about an ideal first-time-contributor",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "So, thats why we dont mark issues with the good-first-issue label Instead, youll find issues labeled Sprintable These issues can either be: Easily fixed when you have guidance from an experienced contributor (perfect for working in a sprint) A learning opportunity for those ready to dive deeper, even if youre not in a sprint Additionally, depending on your prior experience, some Sprintable issues might be easy, while others could be more challenging for you",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "The rest of this document discusses working on the NumPy code base and documentation Were in the process of updating our descriptions of other activities and roles If you are interested in these other activities, please contact us You can do this via the numpy-discussion mailing list, or on GitHub (open an issue or comment on a relevant issue) These are our preferred communication channels (open source is open by nature",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "), however if you prefer to discuss in a more private space first, you can do so on Slack (see numpy org/contribute for details) Development process - summary Heres the short summary, complete TOC links are below: If you are a first-time contributor: Go to numpy/numpy and click the fork button to create your own copy of the project Clone the project to your local computer: git clone --recurse-submodules https://github com/your-username/numpy",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "git Change the directory: cd numpy Add the upstream repository: git remote add upstream https://github com/numpy/numpy",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "git Now, git remote -v will show two remote repositories named: upstream, which refers to the numpy repository origin, which refers to your personal fork Pull the latest changes from upstream, including tags: git checkout main git pull upstream main --tags Initialize numpys submodules: git submodule update --init Develop your contribution: Create a branch for the feature you want to work on",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "Since the branch name will appear in the merge message, use a sensible name such as linspace-speedups: git checkout -b linspace-speedups Commit locally as you progress (git add and git commit) Use a properly formatted commit message, write tests that fail before your change and pass afterward, run all the tests locally Be sure to document any changed behavior in docstrings, keeping to the NumPy docstring standard",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "To submit your contribution: Push your changes back to your fork on GitHub: git push origin linspace-speedups Go to GitHub The new branch will show up with a green Pull Request button Make sure the title and message are clear, concise, and self- explanatory Then click the button to submit it If your commit introduces a new feature or changes functionality, post on the mailing list to explain your changes For bug fixes, documentation updates, etc",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": ", this is generally not necessary, though if you do not get any reaction, do feel free to ask for review Review process: Reviewers (the other developers and interested community members) will write inline and/or general comments on your Pull Request (PR) to help you improve its implementation, documentation and style Every single developer working on the project has their code reviewed, and weve come to see it as friendly conversation from which we all learn and the overall code quality benefits",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "Therefore, please dont let the review discourage you from contributing: its only aim is to improve the quality of project, not to criticize (we are, after all, very grateful for the time youre donating ) See our Reviewer Guidelines for more information To update your PR, make your changes on your local repository, commit, run tests, and only if they succeed push to your fork As soon as those changes are pushed up (to the same branch as before) the PR will update automatically",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "If you have no idea how to fix the test failures, you may push your changes anyway and ask for help in a PR comment Various continuous integration (CI) services are triggered after each PR update to build the code, run unit tests, measure code coverage and check coding style of your branch The CI tests must pass before your PR can be merged If CI fails, you can find out why by clicking on the failed icon (red cross) and inspecting the build and test log",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "To avoid overuse and waste of this resource, test your work locally before committing A PR must be approved by at least one core team member before merging Approval means the core team member has carefully reviewed the changes, and the PR is ready for merging Document changes Beyond changes to a functions docstring and possible description in the general documentation, if your change introduces any user-facing modifications they may need to be mentioned in the release notes",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "To add your change to the release notes, you need to create a short file with a summary and place it in doc/release/upcoming_changes The file doc/release/upcoming_changes/README rst details the format and filename conventions If your change introduces a deprecation, make sure to discuss this first on GitHub or the mailing list first If agreement on the deprecation is reached, follow NEP 23 deprecation policy to add the deprecation",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "Cross referencing issues If the PR relates to any issues, you can add the text xref gh-xxxx where xxxx is the number of the issue to github comments Likewise, if the PR solves an issue, replace the xref with closes, fixes or any of the other flavors github accepts In the source code, be sure to preface any issue or PR reference with gh-xxxx For a more detailed discussion, read on and follow the links at the bottom of this page",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "Guidelines All code should have tests (see test coverage below for more details) All code should be documented No changes are ever committed without review and approval by a core team member Please ask politely on the PR or on the mailing list if you get no response to your pull request within a week Stylistic guidelines Set up your editor to follow PEP 8 (remove trailing white space, no tabs, etc ) Check code with ruff Use NumPy data types instead of strings (np uint8 instead of \"uint8\")",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "Use the following import conventions: import numpy as np For C code, see NEP 45 Test coverage Pull requests (PRs) that modify code should either have new tests, or modify existing tests to fail before the PR and pass afterwards You should run the tests before pushing a PR Running NumPys test suite locally requires some additional packages, such as pytest and hypothesis The additional testing dependencies are listed in requirements/test_requirements",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "txt in the top-level directory, and can conveniently be installed with:  python -m pip install -r requirements/test_requirements txt Tests for a module should ideally cover all code in that module, i e , statement coverage should be at 100 To measure the test coverage, run:  spin test --coverage This will create a report in html format at build/coverage, which can be viewed with your browser, e g :  firefox build/coverage/index",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "html Building docs To build the HTML documentation, use: spin docs You can also run make from the doc directory make help lists all targets To get the appropriate dependencies and other requirements, see Building the NumPy API and reference docs",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "Development process - details The rest of the story Setting up and using your development environment Recommended development setup Using virtual environments Building from source Testing builds Other build options Running tests Running linting Rebuilding  cleaning the workspace Debugging Understanding the code  getting started Building the NumPy API and reference docs Development environments Prerequisites Instructions Development workflow Basic workflow Additional things you might want to do Advanced debugging tools Finding C errors with additional tooling Using GitHub Codespaces for NumPy development GitHub Codespaces What is a codespace",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "Forking the NumPy repository Starting GitHub Codespaces Quick workspace tour Development workflow with GitHub Codespaces Rendering the NumPy documentation FAQs and troubleshooting Reviewer guidelines Who can be a reviewer",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "Communication guidelines Reviewer checklist Standard replies for reviewing NumPy benchmarks Usage Benchmarking versions Writing benchmarks NumPy C style guide For downstream package authors Understanding NumPys versioning and API/ABI stability Testing against the NumPy main branch or pre-releases Adding a dependency on NumPy Releasing a version How to prepare a release Step-by-step directions Branch walkthrough NumPy governance NumPy project governance and decision-making How to contribute to the NumPy documentation Documentation team meetings Whats needed Contributing fixes Contributing new pages Contributing indirectly Documentation style Documentation reading NumPy-specific workflow is in numpy-development-workflow",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "previous Meson and distutils ways of doing things next Setting up and using your development environment On this page Development process - summary Guidelines Stylistic guidelines Test coverage Building docs Development process - details",
          "url": "https://numpy.org/doc/stable/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_22"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/release.html",
      "title": "Release notes \u2014 NumPy v2.3 Manual",
      "content": "NumPy user guide Release notes Release notes 2.3.0 Highlights New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2.2.6 Contributors Pull requests merged 2.2.5 Contributors Pull requests merged 2.2.4 Contributors Pull requests merged 2.2.3 Contributors Pull requests merged 2.2.2 Contributors Pull requests merged 2.2.1 Contributors Pull requests merged 2.2.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 2.1.3 Improvements Changes Contributors Pull requests merged 2.1.2 Contributors Pull requests merged 2.1.1 Contributors Pull requests merged 2.1.0 New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2.0.2 Contributors Pull requests merged 2.0.1 Improvements Contributors Pull requests merged 2.0.0 Highlights NumPy 2.0 Python API removals Deprecations Expired deprecations Compatibility notes C API changes NumPy 2.0 C API removals New Features Improvements Changes 1.26.4 Contributors Pull requests merged 1.26.3 Compatibility Improvements Contributors Pull requests merged 1.26.2 Contributors Pull requests merged 1.26.1 Build system changes New features Contributors Pull requests merged 1.26.0 New Features Improvements Build system changes Contributors Pull requests merged 1.25.2 Contributors Pull requests merged 1.25.1 Contributors Pull requests merged 1.25.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1.24.4 Contributors Pull requests merged 1.24.3 Contributors Pull requests merged 1.24.2 Contributors Pull requests merged 1.24.1 Contributors Pull requests merged 1.24.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1.23.5 Contributors Pull requests merged 1.23.4 Contributors Pull requests merged 1.23.3 Contributors Pull requests merged 1.23.2 Contributors Pull requests merged 1.23.1 Contributors Pull requests merged 1.23.0 New functions Deprecations Expired deprecations New Features Compatibility notes Improvements Performance improvements and changes 1.22.4 Contributors Pull requests merged 1.22.3 Contributors Pull requests merged 1.22.2 Contributors Pull requests merged 1.22.1 Contributors Pull requests merged 1.22.0 Expired deprecations Deprecations Compatibility notes C API changes New Features Improvements 1.21.6 1.21.5 Contributors Pull requests merged 1.21.4 Contributors Pull requests merged 1.21.3 Contributors Pull requests merged 1.21.2 Contributors Pull requests merged 1.21.1 Contributors Pull requests merged 1.21.0 New functions Expired deprecations Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements Changes 1.20.3 Contributors Pull requests merged 1.20.2 Contributors Pull requests merged 1.20.1 Highlights Contributors Pull requests merged 1.20.0 New functions Deprecations Future Changes Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements and changes Changes 1.19.5 Contributors Pull requests merged 1.19.4 Contributors Pull requests merged 1.19.3 Contributors Pull requests merged 1.19.2 Improvements Contributors Pull requests merged 1.19.1 Contributors Pull requests merged 1.19.0 Highlights Expired deprecations Compatibility notes Deprecations C API changes New Features Improvements Improve detection of CPU features Changes 1.18.5 Contributors Pull requests merged 1.18.4 Contributors Pull requests merged 1.18.3 Highlights Contributors Pull requests merged 1.18.2 Contributors Pull requests merged 1.18.1 Contributors Pull requests merged 1.18.0 Highlights New functions Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Changes 1.17.5 Contributors Pull requests merged 1.17.4 Highlights Contributors Pull requests merged 1.17.3 Highlights Compatibility notes Contributors Pull requests merged 1.17.2 Contributors Pull requests merged 1.17.1 Contributors Pull requests merged 1.17.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1.16.6 Highlights New functions Compatibility notes Improvements Contributors Pull requests merged 1.16.5 Contributors Pull requests merged 1.16.4 New deprecations Compatibility notes Changes Contributors Pull requests merged 1.16.3 Compatibility notes Improvements Changes 1.16.2 Compatibility notes Contributors Pull requests merged 1.16.1 Contributors Enhancements Compatibility notes New Features Improvements Changes 1.16.0 Highlights New functions New deprecations Expired deprecations Future changes Compatibility notes C API changes New Features Improvements Changes 1.15.4 Compatibility Note Contributors Pull requests merged 1.15.3 Compatibility Note Contributors Pull requests merged 1.15.2 Compatibility Note Contributors Pull requests merged 1.15.1 Compatibility Note Contributors Pull requests merged 1.15.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements 1.14.6 Contributors Pull requests merged 1.14.5 Contributors Pull requests merged 1.14.4 Contributors Pull requests merged 1.14.3 Contributors Pull requests merged 1.14.2 Contributors Pull requests merged 1.14.1 Contributors Pull requests merged 1.14.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1.13.3 Contributors Pull requests merged 1.13.2 Contributors Pull requests merged 1.13.1 Pull requests merged Contributors 1.13.0 Highlights New functions Deprecations Future Changes Build System Changes Compatibility notes C API changes New Features Improvements Changes 1.12.1 Bugs Fixed 1.12.0 Highlights Dropped Support Added Support Build System Changes Deprecations Future Changes Compatibility notes New Features Improvements Changes 1.11.3 Contributors to maintenance/1.11.3 Pull Requests Merged 1.11.2 Pull Requests Merged 1.11.1 Fixes Merged 1.11.0 Highlights Build System Changes Future Changes Compatibility notes New Features Improvements Changes Deprecations FutureWarnings 1.10.4 Compatibility notes Issues Fixed Merged PRs 1.10.3 1.10.2 Compatibility notes Issues Fixed Merged PRs Notes 1.10.1 1.10.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations 1.9.2 Issues fixed 1.9.1 Issues fixed 1.9.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Deprecations 1.8.2 Issues fixed 1.8.1 Issues fixed Changes Deprecations 1.8.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations Authors 1.7.2 Issues fixed 1.7.1 Issues fixed 1.7.0 Highlights Compatibility notes New features Changes Deprecations 1.6.2 Issues fixed Changes 1.6.1 Issues Fixed 1.6.0 Highlights New features Changes Deprecated features Removed features 1.5.0 Highlights New features Changes 1.4.0 Highlights New features Improvements Deprecations Internal changes 1.3.0 Highlights New features Deprecated features Documentation changes New C API Internal changes previous Glossary next NumPy 2.3.0 Release Notes",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy user guide Release notes Release notes 2 3 0 Highlights New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2 2 6 Contributors Pull requests merged 2 2 5 Contributors Pull requests merged 2 2 4 Contributors Pull requests merged 2 2 3 Contributors Pull requests merged 2 2 2 Contributors Pull requests merged 2 2 1 Contributors Pull requests merged 2 2",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 2 1 3 Improvements Changes Contributors Pull requests merged 2 1 2 Contributors Pull requests merged 2 1 1 Contributors Pull requests merged 2 1 0 New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2 0 2 Contributors Pull requests merged 2 0 1 Improvements Contributors Pull requests merged 2",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "0 0 Highlights NumPy 2 0 Python API removals Deprecations Expired deprecations Compatibility notes C API changes NumPy 2 0 C API removals New Features Improvements Changes 1 26 4 Contributors Pull requests merged 1 26 3 Compatibility Improvements Contributors Pull requests merged 1 26 2 Contributors Pull requests merged 1 26 1 Build system changes New features Contributors Pull requests merged 1 26 0 New Features Improvements Build system changes Contributors Pull requests merged 1 25",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "2 Contributors Pull requests merged 1 25 1 Contributors Pull requests merged 1 25 0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1 24 4 Contributors Pull requests merged 1 24 3 Contributors Pull requests merged 1 24 2 Contributors Pull requests merged 1 24 1 Contributors Pull requests merged 1 24",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1 23 5 Contributors Pull requests merged 1 23 4 Contributors Pull requests merged 1 23 3 Contributors Pull requests merged 1 23 2 Contributors Pull requests merged 1 23 1 Contributors Pull requests merged 1 23 0 New functions Deprecations Expired deprecations New Features Compatibility notes Improvements Performance improvements and changes 1 22",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "4 Contributors Pull requests merged 1 22 3 Contributors Pull requests merged 1 22 2 Contributors Pull requests merged 1 22 1 Contributors Pull requests merged 1 22 0 Expired deprecations Deprecations Compatibility notes C API changes New Features Improvements 1 21 6 1 21 5 Contributors Pull requests merged 1 21 4 Contributors Pull requests merged 1 21 3 Contributors Pull requests merged 1 21 2 Contributors Pull requests merged 1 21 1 Contributors Pull requests merged 1 21",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "0 New functions Expired deprecations Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements Changes 1 20 3 Contributors Pull requests merged 1 20 2 Contributors Pull requests merged 1 20 1 Highlights Contributors Pull requests merged 1 20 0 New functions Deprecations Future Changes Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements and changes Changes 1 19",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "5 Contributors Pull requests merged 1 19 4 Contributors Pull requests merged 1 19 3 Contributors Pull requests merged 1 19 2 Improvements Contributors Pull requests merged 1 19 1 Contributors Pull requests merged 1 19 0 Highlights Expired deprecations Compatibility notes Deprecations C API changes New Features Improvements Improve detection of CPU features Changes 1 18 5 Contributors Pull requests merged 1 18 4 Contributors Pull requests merged 1 18",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "3 Highlights Contributors Pull requests merged 1 18 2 Contributors Pull requests merged 1 18 1 Contributors Pull requests merged 1 18 0 Highlights New functions Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Changes 1 17 5 Contributors Pull requests merged 1 17 4 Highlights Contributors Pull requests merged 1 17 3 Highlights Compatibility notes Contributors Pull requests merged 1 17 2 Contributors Pull requests merged 1 17",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "1 Contributors Pull requests merged 1 17 0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1 16 6 Highlights New functions Compatibility notes Improvements Contributors Pull requests merged 1 16 5 Contributors Pull requests merged 1 16 4 New deprecations Compatibility notes Changes Contributors Pull requests merged 1 16 3 Compatibility notes Improvements Changes 1 16 2 Compatibility notes Contributors Pull requests merged 1",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "16 1 Contributors Enhancements Compatibility notes New Features Improvements Changes 1 16 0 Highlights New functions New deprecations Expired deprecations Future changes Compatibility notes C API changes New Features Improvements Changes 1 15 4 Compatibility Note Contributors Pull requests merged 1 15 3 Compatibility Note Contributors Pull requests merged 1 15 2 Compatibility Note Contributors Pull requests merged 1 15 1 Compatibility Note Contributors Pull requests merged 1 15",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements 1 14 6 Contributors Pull requests merged 1 14 5 Contributors Pull requests merged 1 14 4 Contributors Pull requests merged 1 14 3 Contributors Pull requests merged 1 14 2 Contributors Pull requests merged 1 14 1 Contributors Pull requests merged 1 14 0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1 13",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "3 Contributors Pull requests merged 1 13 2 Contributors Pull requests merged 1 13 1 Pull requests merged Contributors 1 13 0 Highlights New functions Deprecations Future Changes Build System Changes Compatibility notes C API changes New Features Improvements Changes 1 12 1 Bugs Fixed 1 12 0 Highlights Dropped Support Added Support Build System Changes Deprecations Future Changes Compatibility notes New Features Improvements Changes 1 11 3 Contributors to maintenance/1 11 3 Pull Requests Merged 1",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "11 2 Pull Requests Merged 1 11 1 Fixes Merged 1 11 0 Highlights Build System Changes Future Changes Compatibility notes New Features Improvements Changes Deprecations FutureWarnings 1 10 4 Compatibility notes Issues Fixed Merged PRs 1 10 3 1 10 2 Compatibility notes Issues Fixed Merged PRs Notes 1 10 1 1 10 0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations 1 9 2 Issues fixed 1 9 1 Issues fixed 1 9",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Deprecations 1 8 2 Issues fixed 1 8 1 Issues fixed Changes Deprecations 1 8 0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations Authors 1 7 2 Issues fixed 1 7 1 Issues fixed 1 7 0 Highlights Compatibility notes New features Changes Deprecations 1 6 2 Issues fixed Changes 1 6 1 Issues Fixed 1 6",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "0 Highlights New features Changes Deprecated features Removed features 1 5 0 Highlights New features Changes 1 4 0 Highlights New features Improvements Deprecations Internal changes 1 3 0 Highlights New features Deprecated features Documentation changes New C API Internal changes previous Glossary next NumPy 2 3 0 Release Notes",
          "url": "https://numpy.org/doc/stable/release.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/",
      "title": "NumPy tutorials \u2014 NumPy Tutorials",
      "content": "Repository Open issue .md .pdf NumPy tutorials Contents Content Non-executable articles Useful links and resources NumPy tutorials This set of tutorials and educational materials is being developed in the numpy-tutorials repository, and is not a part of the NumPy source tree. The goal of this repository is to provide high-quality resources by the NumPy project, both for self-learning and for teaching classes with. If youre interested in adding your own content, check the Contributing section. To open a live version of the content, click the launch Binder button above. To open each of the .md files, right click and select Open with - Notebook. You can also launch individual tutorials on Binder by clicking on the rocket icon that appears in the upper-right corner of each tutorial. To download a local copy of the .ipynb files, you can either clone this repository or use the download icon in the upper-right corner of each tutorial. Content NumPy Features Linear algebra on n-dimensional arrays Saving and sharing your NumPy arrays Masked Arrays NumPy Applications Determining Moores Law with real data in NumPy Deep learning on MNIST X-ray image processing Determining Static Equilibrium in NumPy Plotting Fractals Analyzing the impact of the lockdown on air quality in Delhi, India Contributing Why Jupyter Notebooks? Adding your own tutorials Non-executable articles Help improve the tutorials! Want to make a valuable contribution to the tutorials? Consider contributing to these existing articles to help make them fully executable and reproducible! Articles Deep reinforcement learning with Pong from pixels Sentiment Analysis on notable speeches of the last decade Useful links and resources The following links may be useful: NumPy Code of Conduct Main NumPy documentation NumPy documentation team meeting notes NEP 44 - Restructuring the NumPy documentation Blog post - Documentation as a way to build Community Note that regular documentation issues for NumPy can be found in the main NumPy repository (see the Documentation labels there). next NumPy Features Contents Content Non-executable articles Useful links and resources By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "Documentation"
      ],
      "chunks": [
        {
          "content": "Repository Open issue md pdf NumPy tutorials Contents Content Non-executable articles Useful links and resources NumPy tutorials This set of tutorials and educational materials is being developed in the numpy-tutorials repository, and is not a part of the NumPy source tree The goal of this repository is to provide high-quality resources by the NumPy project, both for self-learning and for teaching classes with If youre interested in adding your own content, check the Contributing section",
          "url": "https://numpy.org/numpy-tutorials/",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "To open a live version of the content, click the launch Binder button above To open each of the md files, right click and select Open with - Notebook You can also launch individual tutorials on Binder by clicking on the rocket icon that appears in the upper-right corner of each tutorial To download a local copy of the ipynb files, you can either clone this repository or use the download icon in the upper-right corner of each tutorial",
          "url": "https://numpy.org/numpy-tutorials/",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "Content NumPy Features Linear algebra on n-dimensional arrays Saving and sharing your NumPy arrays Masked Arrays NumPy Applications Determining Moores Law with real data in NumPy Deep learning on MNIST X-ray image processing Determining Static Equilibrium in NumPy Plotting Fractals Analyzing the impact of the lockdown on air quality in Delhi, India Contributing Why Jupyter Notebooks Adding your own tutorials Non-executable articles Help improve the tutorials",
          "url": "https://numpy.org/numpy-tutorials/",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "Want to make a valuable contribution to the tutorials Consider contributing to these existing articles to help make them fully executable and reproducible",
          "url": "https://numpy.org/numpy-tutorials/",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "Articles Deep reinforcement learning with Pong from pixels Sentiment Analysis on notable speeches of the last decade Useful links and resources The following links may be useful: NumPy Code of Conduct Main NumPy documentation NumPy documentation team meeting notes NEP 44 - Restructuring the NumPy documentation Blog post - Documentation as a way to build Community Note that regular documentation issues for NumPy can be found in the main NumPy repository (see the Documentation labels there)",
          "url": "https://numpy.org/numpy-tutorials/",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "next NumPy Features Contents Content Non-executable articles Useful links and resources By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/",
          "library": "numpy",
          "chunk_id": "numpy_5"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/#main-content",
      "title": "NumPy documentation \u2014 NumPy v2.4.dev0 Manual",
      "content": "NumPy documentation Version: 2.4.dev0 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Getting started New to NumPy? Check out the Absolute Beginners Guide. It contains an introduction to NumPys main concepts and links to additional tutorials. To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation. To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts. To the reference guide Contributors guide Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy. To the contributors guide next NumPy user guide",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy documentation Version: 2 4 dev0 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python",
          "url": "https://numpy.org/devdocs/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more Getting started New to NumPy Check out the Absolute Beginners Guide",
          "url": "https://numpy.org/devdocs/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "It contains an introduction to NumPys main concepts and links to additional tutorials To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy The reference describes how the methods work and which parameters can be used",
          "url": "https://numpy.org/devdocs/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "It assumes that you have an understanding of the key concepts To the reference guide Contributors guide Want to add to the codebase Can help add translation or a flowchart to the documentation The contributing guidelines will guide you through the process of improving NumPy To the contributors guide next NumPy user guide",
          "url": "https://numpy.org/devdocs/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/user/index.html",
      "title": "NumPy user guide \u2014 NumPy v2.4.dev0 Manual",
      "content": "NumPy user guide NumPy user guide This guide is an overview and explains the important features; details are found in NumPy reference. Getting started What is NumPy? Installation NumPy quickstart NumPy: the absolute basics for beginners Fundamentals and usage NumPy fundamentals Array creation Indexing on ndarrays I/O with NumPy Data types Broadcasting Copies and views Working with Arrays of Strings And Bytes Structured arrays Universal functions (ufunc) basics NumPy for MATLAB users NumPy tutorials NumPy how-tos Advanced usage and interoperability Using NumPy C-API F2PY user guide and reference manual Under-the-hood documentation for developers Interoperability with NumPy previous NumPy documentation next What is NumPy?",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy user guide NumPy user guide This guide is an overview and explains the important features; details are found in NumPy reference Getting started What is NumPy",
          "url": "https://numpy.org/devdocs/user/index.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Installation NumPy quickstart NumPy: the absolute basics for beginners Fundamentals and usage NumPy fundamentals Array creation Indexing on ndarrays I/O with NumPy Data types Broadcasting Copies and views Working with Arrays of Strings And Bytes Structured arrays Universal functions (ufunc) basics NumPy for MATLAB users NumPy tutorials NumPy how-tos Advanced usage and interoperability Using NumPy C-API F2PY user guide and reference manual Under-the-hood documentation for developers Interoperability with NumPy previous NumPy documentation next What is NumPy",
          "url": "https://numpy.org/devdocs/user/index.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/reference/index.html",
      "title": "NumPy reference \u2014 NumPy v2.4.dev0 Manual",
      "content": "NumPy reference NumPy reference Release: 2.4.dev0 Date: June 26, 2025 This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do. For learning how to use NumPy, see the complete documentation. Python API NumPys module structure Array objects The N-dimensional array (ndarray) Scalars Data type objects (dtype) Data type promotion in NumPy Iterating over arrays Standard array subclasses Masked arrays The array interface protocol Datetimes and timedeltas Universal functions (ufunc) Routines and objects by topic Constants Array creation routines Array manipulation routines Bit-wise operations String functionality Datetime support functions Data type routines Mathematical functions with automatic domain Floating point error handling Exceptions and Warnings Discrete Fourier Transform Functional programming Input and output Indexing routines Linear algebra Logic functions Masked array operations Mathematical functions Miscellaneous routines Polynomials Random sampling Set routines Sorting, searching, and counting Statistics Test support Window functions Typing (numpy.typing) Packaging C API NumPy C-API Python types and C-structures System configuration Data type API Array API Array iterator API ufunc API Generalized universal function API NpyString API NumPy core math library Datetime API C API deprecations Memory management in NumPy Other topics Array API standard compatibility CPU/SIMD optimizations Thread Safety Global Configuration Options NumPy security Status of numpy.distutils and migration advice numpy.distutils user guide NumPy and SWIG Acknowledgements Large parts of this manual originate from Travis E. Oliphants book Guide to NumPy (which generously entered Public Domain in August 2008). The reference documentation for many of the functions are written by numerous contributors and developers of NumPy. previous NumPy license next NumPys module structure On this page Python API C API Other topics Acknowledgements",
      "code_blocks": [
        "numpy.typing",
        "numpy.distutils",
        "numpy.distutils"
      ],
      "chunks": [
        {
          "content": "NumPy reference NumPy reference Release: 2 4 dev0 Date: June 26, 2025 This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do For learning how to use NumPy, see the complete documentation",
          "url": "https://numpy.org/devdocs/reference/index.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Python API NumPys module structure Array objects The N-dimensional array (ndarray) Scalars Data type objects (dtype) Data type promotion in NumPy Iterating over arrays Standard array subclasses Masked arrays The array interface protocol Datetimes and timedeltas Universal functions (ufunc) Routines and objects by topic Constants Array creation routines Array manipulation routines Bit-wise operations String functionality Datetime support functions Data type routines Mathematical functions with automatic domain Floating point error handling Exceptions and Warnings Discrete Fourier Transform Functional programming Input and output Indexing routines Linear algebra Logic functions Masked array operations Mathematical functions Miscellaneous routines Polynomials Random sampling Set routines Sorting, searching, and counting Statistics Test support Window functions Typing (numpy",
          "url": "https://numpy.org/devdocs/reference/index.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "typing) Packaging C API NumPy C-API Python types and C-structures System configuration Data type API Array API Array iterator API ufunc API Generalized universal function API NpyString API NumPy core math library Datetime API C API deprecations Memory management in NumPy Other topics Array API standard compatibility CPU/SIMD optimizations Thread Safety Global Configuration Options NumPy security Status of numpy distutils and migration advice numpy",
          "url": "https://numpy.org/devdocs/reference/index.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "distutils user guide NumPy and SWIG Acknowledgements Large parts of this manual originate from Travis E Oliphants book Guide to NumPy (which generously entered Public Domain in August 2008) The reference documentation for many of the functions are written by numerous contributors and developers of NumPy previous NumPy license next NumPys module structure On this page Python API C API Other topics Acknowledgements",
          "url": "https://numpy.org/devdocs/reference/index.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/building/index.html",
      "title": "Building from source \u2014 NumPy v2.4.dev0 Manual",
      "content": "Building from source Building from source Note If you are only trying to install NumPy, we recommend using binaries - see Installation for details on that. Building NumPy from source requires setting up system-level dependencies (compilers, BLAS/LAPACK libraries, etc.) first, and then invoking a build. The build may be done in order to install NumPy for local usage, develop NumPy itself, or build redistributable binary packages. And it may be desired to customize aspects of how the build is done. This guide will cover all these aspects. In addition, it provides background information on how the NumPy build works, and links to up-to-date guides for generic Python build  packaging documentation that is relevant. System-level dependencies NumPy uses compiled code for speed, which means you need compilers and some other system-level (i.e, non-Python / non-PyPI) dependencies to build it on your system. Note If you are using Conda, you can skip the steps in this section - with the exception of installing compilers for Windows or the Apple Developer Tools for macOS. All other dependencies will be installed automatically by the mamba env create -f environment.yml command. Linux If you want to use the system Python and pip, you will need: C and C++ compilers (typically GCC). Python header files (typically a package named python3-dev or python3-devel) BLAS and LAPACK libraries. OpenBLAS is the NumPy default; other variants include Apple Accelerate, MKL, ATLAS and Netlib (or Reference) BLAS and LAPACK. pkg-config for dependency detection. A Fortran compiler is needed only for running the f2py tests. The instructions below include a Fortran compiler, however you can safely leave it out. Debian/Ubuntu Linux To install NumPy build requirements, you can do: sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev Alternatively, you can do: sudo apt build-dep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. Fedora To install NumPy build requirements, you can do: sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo dnf builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. CentOS/RHEL To install NumPy build requirements, you can do: sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo yum-builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. Arch To install NumPy build requirements, you can do: sudo pacman -S gcc-fortran openblas pkgconf macOS Install Apple Developer Tools. An easy way to do this is to open a terminal window, enter the command: xcode-select --install and follow the prompts. Apple Developer Tools includes Git, the Clang C/C++ compilers, and other development utilities that may be required. Do not use the macOS system Python. Instead, install Python with the python.org installer or with a package manager like Homebrew, MacPorts or Fink. On macOS =13.3, the easiest build option is to use Accelerate, which is already installed and will be automatically used by default. On older macOS versions you need a different BLAS library, most likely OpenBLAS, plus pkg-config to detect OpenBLAS. These are easiest to install with Homebrew: brew install openblas pkg-config gfortran Windows On Windows, the use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together. If you dont need to run the f2py tests, simply using MSVC is easiest. Otherwise, you will need one of these sets of compilers: MSVC + Intel Fortran (ifort) Intel compilers (icc, ifort) Mingw-w64 compilers (gcc, g++, gfortran) Compared to macOS and Linux, building NumPy on Windows is a little more difficult, due to the need to set up these compilers. It is not possible to just call a one-liner on the command prompt as you would on other platforms. First, install Microsoft Visual Studio - the 2019 Community Edition or any newer version will work (see the Visual Studio download site). This is needed even if you use the MinGW-w64 or Intel compilers, in order to ensure you have the Windows Universal C Runtime (the other components of Visual Studio are not needed when using Mingw-w64, and can be deselected if desired, to save disk space). The recommended version of the UCRT is = 10.0.22621.0. MSVC The MSVC installer does not put the compilers on the system path, and the install location may change. To query the install location, MSVC comes with a vswhere.exe command-line utility. And to make the C/C++ compilers available inside the shell you are using, you need to run a .bat file for the correct bitness and architecture (e.g., for 64-bit Intel CPUs, use vcvars64.bat). If using a Conda environment while a version of Visual Studio 2019+ is installed that includes the MSVC v142 package (VS 2019 C++ x86/x64 build tools), activating the conda environment should cause Visual Studio to be found and the appropriate .bat file executed to set these variables. For detailed guidance, see Use the Microsoft C++ toolset from the command line. Intel Similar to MSVC, the Intel compilers are designed to be used with an activation script (Intel\\oneAPI\\setvars.bat) that you run in the shell you are using. This makes the compilers available on the path. For detailed guidance, see Get Started with the Intel oneAPI HPC Toolkit for Windows. MinGW-w64 There are several sources of binaries for MinGW-w64. We recommend the RTools versions, which can be installed with Chocolatey (see Chocolatey install instructions here): choco install rtools -y --no-progress --force --version=4.0.0.20220206 Note Compilers should be on the system path (i.e., the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH. You can use any shell (e.g., Powershell, cmd or Git Bash) to invoke a build. To check that this is the case, try invoking a Fortran compiler in the shell you use (e.g., gfortran --version or ifort --version). Warning When using a conda environment it is possible that the environment creation will not work due to an outdated Fortran compiler. If that happens, remove the compilers entry from environment.yml and try again. The Fortran compiler should be installed as described in this section. Windows on ARM64 In Windows on ARM64, the set of a compiler options that are available for building NumPy are limited. Compilers such as GCC and GFortran are not yet supported for Windows on ARM64. Currently, the NumPy build for Windows on ARM64 is supported with MSVC and LLVM toolchains. The use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together. If you dont need to run the f2py tests, simply using MSVC is easiest. Otherwise, you will need the following set of compilers: MSVC + flang (cl, flang) LLVM + flang (clang-cl, flang) First, install Microsoft Visual Studio - the 2022 Community Edition will work (see the Visual Studio download site). Ensure that you have installed necessary Visual Studio components for building NumPy on WoA from here. To use the flang compiler for Windows on ARM64, install Latest LLVM toolchain for WoA from here. MSVC The MSVC installer does not put the compilers on the system path, and the install location may change. To query the install location, MSVC comes with a vswhere.exe command-line utility. And to make the C/C++ compilers available inside the shell you are using, you need to run a .bat file for the correct bitness and architecture (e.g., for ARM64-based CPUs, use vcvarsarm64.bat). For detailed guidance, see Use the Microsoft C++ toolset from the command line. LLVM Similar to MSVC, LLVM does not put the compilers on the system path. To set system path for LLVM compilers, users may need to use set command to put compilers on the system path. To check compilers path for LLVMs clang-cl, try invoking LLVMs clang-cl compiler in the shell you use (clang-cl --version). Note Compilers should be on the system path (i.e., the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH. You can use any shell (e.g., Powershell, cmd or Git Bash) to invoke a build. To check that this is the case, try invoking a Fortran compiler in the shell you use (e.g., flang --version). Warning Currently, Conda environment is not yet supported officially on Windows on ARM64. The present approach uses virtualenv for building NumPy from source on Windows on ARM64. Building NumPy from source If you want to only install NumPy from source once and not do any development work, then the recommended way to build and install is to use pip. Otherwise, conda is recommended. Note If you dont have a conda installation yet, we recommend using Miniforge; any conda flavor will work though. Building from source to use NumPy Conda env If you are using a conda environment, pip is still the tool you use to invoke a from-source build of NumPy. It is important to always use the --no-build-isolation flag to the pip install command, to avoid building against a numpy wheel from PyPI. In order for that to work you must first install the remaining build dependencies into the conda environment:  Either install all NumPy dev dependencies into a fresh conda environment mamba env create -f environment.yml  Or, install only the required build dependencies mamba install python numpy cython compilers openblas meson-python pkg-config  To build the latest stable release: pip install numpy --no-build-isolation --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init pip install . --no-build-isolation Warning On Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the pip install command to fail. These variables are only needed for flang and can be safely unset prior to running pip install. Virtual env or system Python  To build the latest stable release: pip install numpy --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init pip install . Building from source for NumPy development If you want to build from source in order to work on NumPy itself, first clone the NumPy repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init Then you want to do the following: Create a dedicated development environment (virtual environment or conda environment), Install all needed dependencies (build, and also test, doc and optional dependencies), Build NumPy with the spin developer interface. Step (3) is always the same, steps (1) and (2) are different between conda and virtual environments: Conda env To create a numpy-dev development environment with every required and optional dependency installed, run: mamba env create -f environment.yml mamba activate numpy-dev Virtual env or system Python Note There are many tools to manage virtual environments, like venv, virtualenv/virtualenvwrapper, pyenv/pyenv-virtualenv, Poetry, PDM, Hatch, and more. Here we use the basic venv tool that is part of the Python stdlib. You can use any other tool; all we need is an activated Python environment. Create and activate a virtual environment in a new directory named venv ( note that the exact activation command may be different based on your OS and shell - see How venvs work in the venv docs). Linux python -m venv venv source venv/bin/activate macOS python -m venv venv source venv/bin/activate Windows python -m venv venv .\\venv\\Scripts\\activate Windows on ARM64 python -m venv venv .\\venv\\Scripts\\activate Note Building NumPy with BLAS and LAPACK functions requires OpenBLAS library at Runtime. In Windows on ARM64, this can be done by setting up pkg-config for OpenBLAS dependency. The build steps for OpenBLAS for Windows on ARM64 can be found here. Then install the Python-level dependencies from PyPI with: python -m pip install -r requirements/build_requirements.txt To build NumPy in an activated development environment, run: spin build This will install NumPy inside the repository (by default in a build-install directory). You can then run tests (spin test), drop into IPython (spin ipython), or take other development steps like build the html documentation or running benchmarks. The spin interface is self-documenting, so please see spin --help and spin subcommand --help for detailed guidance. Warning In an activated conda environment on Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the build to fail. These variables are only needed for flang and can be safely unset for build. IDE support  editable installs While the spin interface is our recommended way of working on NumPy, it has one limitation: because of the custom install location, NumPy installed using spin will not be recognized automatically within an IDE (e.g., for running a script via a run button, or setting breakpoints visually). This will work better with an in-place build (or editable install). Editable installs are supported. It is important to understand that you may use either an editable install or ``spin`` in a given repository clone, but not both. If you use editable installs, you have to use pytest and other development tools directly instead of using spin. To use an editable install, ensure you start from a clean repository (run git clean -xdf if youve built with spin before) and have all dependencies set up correctly as described higher up on this page. Then do:  Note: the --no-build-isolation is important! pip install -e . --no-build-isolation  To run the tests for, e.g., the `numpy.linalg` module: pytest numpy/linalg When making changes to NumPy code, including to compiled code, there is no need to manually rebuild or reinstall. NumPy is automatically rebuilt each time NumPy is imported by the Python interpreter; see the meson-python documentation on editable installs for more details on how that works under the hood. When you run git clean -xdf, which removes the built extension modules, remember to also uninstall NumPy with pip uninstall numpy. Warning Note that editable installs are fundamentally incomplete installs. Their only guarantee is that import numpy works - so they are suitable for working on NumPy itself, and for working on pure Python packages that depend on NumPy. Headers, entrypoints, and other such things may not be available from an editable install. Customizing builds Compiler selection and customizing a build BLAS and LAPACK Cross compilation Building redistributable binaries Background information Understanding Meson Introspecting build steps Meson and distutils ways of doing things previous Testing the numpy.i typemaps next Compiler selection and customizing a build On this page System-level dependencies Building NumPy from source Building from source to use NumPy Building from source for NumPy development Customizing builds Background information",
      "code_blocks": [
        "mamba env create -f environment.yml",
        "python3-dev",
        "python3-devel",
        "sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev",
        "sudo apt build-dep numpy",
        "sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig",
        "sudo dnf builddep numpy",
        "sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig",
        "sudo yum-builddep numpy",
        "sudo pacman -S gcc-fortran openblas pkgconf",
        "xcode-select --install",
        "brew install openblas pkg-config gfortran",
        "vswhere.exe",
        "vcvars64.bat",
        "Intel\\oneAPI\\setvars.bat",
        "choco install rtools -y --no-progress --force --version=4.0.0.20220206",
        "gfortran\n--version",
        "ifort --version",
        "environment.yml",
        "vswhere.exe",
        "vcvarsarm64.bat",
        "clang-cl --version",
        "flang\n--version",
        "--no-build-isolation",
        "pip install",
        "# Either install all NumPy dev dependencies into a fresh conda environment\nmamba env create -f environment.yml\n\n# Or, install only the required build dependencies\nmamba install python numpy cython compilers openblas meson-python pkg-config\n\n# To build the latest stable release:\npip install numpy --no-build-isolation --no-binary numpy\n\n# To build a development version, you need a local clone of the NumPy git repository:\ngit clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init\npip install . --no-build-isolation",
        "# To build the latest stable release:\npip install numpy --no-binary numpy\n\n# To build a development version, you need a local clone of the NumPy git repository:\ngit clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init\npip install .",
        "git clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init",
        "mamba env create -f environment.yml\nmamba activate numpy-dev",
        "virtualenvwrapper",
        "pyenv-virtualenv",
        "python -m venv venv\nsource venv/bin/activate",
        "python -m venv venv\nsource venv/bin/activate",
        "python -m venv venv\n.\\venv\\Scripts\\activate",
        "python -m venv venv\n.\\venv\\Scripts\\activate",
        "python -m pip install -r requirements/build_requirements.txt",
        "build-install",
        "spin ipython",
        "spin --help",
        "spin <subcommand> --help",
        "git clean -xdf",
        "# Note: the --no-build-isolation is important!\npip install -e . --no-build-isolation\n\n# To run the tests for, e.g., the `numpy.linalg` module:\npytest numpy/linalg",
        "git clean -xdf",
        "pip uninstall numpy",
        "import numpy"
      ],
      "chunks": [
        {
          "content": "Building from source Building from source Note If you are only trying to install NumPy, we recommend using binaries - see Installation for details on that Building NumPy from source requires setting up system-level dependencies (compilers, BLAS/LAPACK libraries, etc ) first, and then invoking a build The build may be done in order to install NumPy for local usage, develop NumPy itself, or build redistributable binary packages And it may be desired to customize aspects of how the build is done",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "This guide will cover all these aspects In addition, it provides background information on how the NumPy build works, and links to up-to-date guides for generic Python build  packaging documentation that is relevant System-level dependencies NumPy uses compiled code for speed, which means you need compilers and some other system-level (i e, non-Python / non-PyPI) dependencies to build it on your system",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "Note If you are using Conda, you can skip the steps in this section - with the exception of installing compilers for Windows or the Apple Developer Tools for macOS All other dependencies will be installed automatically by the mamba env create -f environment yml command Linux If you want to use the system Python and pip, you will need: C and C++ compilers (typically GCC) Python header files (typically a package named python3-dev or python3-devel) BLAS and LAPACK libraries",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "OpenBLAS is the NumPy default; other variants include Apple Accelerate, MKL, ATLAS and Netlib (or Reference) BLAS and LAPACK pkg-config for dependency detection A Fortran compiler is needed only for running the f2py tests The instructions below include a Fortran compiler, however you can safely leave it out",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "Debian/Ubuntu Linux To install NumPy build requirements, you can do: sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev Alternatively, you can do: sudo apt build-dep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "Fedora To install NumPy build requirements, you can do: sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo dnf builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "CentOS/RHEL To install NumPy build requirements, you can do: sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo yum-builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers Arch To install NumPy build requirements, you can do: sudo pacman -S gcc-fortran openblas pkgconf macOS Install Apple Developer Tools",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "An easy way to do this is to open a terminal window, enter the command: xcode-select --install and follow the prompts Apple Developer Tools includes Git, the Clang C/C++ compilers, and other development utilities that may be required Do not use the macOS system Python Instead, install Python with the python org installer or with a package manager like Homebrew, MacPorts or Fink On macOS =13",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "3, the easiest build option is to use Accelerate, which is already installed and will be automatically used by default On older macOS versions you need a different BLAS library, most likely OpenBLAS, plus pkg-config to detect OpenBLAS These are easiest to install with Homebrew: brew install openblas pkg-config gfortran Windows On Windows, the use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "If you dont need to run the f2py tests, simply using MSVC is easiest Otherwise, you will need one of these sets of compilers: MSVC + Intel Fortran (ifort) Intel compilers (icc, ifort) Mingw-w64 compilers (gcc, g++, gfortran) Compared to macOS and Linux, building NumPy on Windows is a little more difficult, due to the need to set up these compilers It is not possible to just call a one-liner on the command prompt as you would on other platforms",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "First, install Microsoft Visual Studio - the 2019 Community Edition or any newer version will work (see the Visual Studio download site) This is needed even if you use the MinGW-w64 or Intel compilers, in order to ensure you have the Windows Universal C Runtime (the other components of Visual Studio are not needed when using Mingw-w64, and can be deselected if desired, to save disk space) The recommended version of the UCRT is = 10 0 22621 0",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "MSVC The MSVC installer does not put the compilers on the system path, and the install location may change To query the install location, MSVC comes with a vswhere exe command-line utility And to make the C/C++ compilers available inside the shell you are using, you need to run a bat file for the correct bitness and architecture (e g , for 64-bit Intel CPUs, use vcvars64 bat)",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "If using a Conda environment while a version of Visual Studio 2019+ is installed that includes the MSVC v142 package (VS 2019 C++ x86/x64 build tools), activating the conda environment should cause Visual Studio to be found and the appropriate bat file executed to set these variables For detailed guidance, see Use the Microsoft C++ toolset from the command line Intel Similar to MSVC, the Intel compilers are designed to be used with an activation script (Intel\\oneAPI\\setvars",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "bat) that you run in the shell you are using This makes the compilers available on the path For detailed guidance, see Get Started with the Intel oneAPI HPC Toolkit for Windows MinGW-w64 There are several sources of binaries for MinGW-w64 We recommend the RTools versions, which can be installed with Chocolatey (see Chocolatey install instructions here): choco install rtools -y --no-progress --force --version=4 0 0 20220206 Note Compilers should be on the system path (i e",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": ", the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH You can use any shell (e g , Powershell, cmd or Git Bash) to invoke a build To check that this is the case, try invoking a Fortran compiler in the shell you use (e g , gfortran --version or ifort --version)",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "Warning When using a conda environment it is possible that the environment creation will not work due to an outdated Fortran compiler If that happens, remove the compilers entry from environment yml and try again The Fortran compiler should be installed as described in this section Windows on ARM64 In Windows on ARM64, the set of a compiler options that are available for building NumPy are limited Compilers such as GCC and GFortran are not yet supported for Windows on ARM64",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "Currently, the NumPy build for Windows on ARM64 is supported with MSVC and LLVM toolchains The use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together If you dont need to run the f2py tests, simply using MSVC is easiest",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Otherwise, you will need the following set of compilers: MSVC + flang (cl, flang) LLVM + flang (clang-cl, flang) First, install Microsoft Visual Studio - the 2022 Community Edition will work (see the Visual Studio download site) Ensure that you have installed necessary Visual Studio components for building NumPy on WoA from here To use the flang compiler for Windows on ARM64, install Latest LLVM toolchain for WoA from here",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "MSVC The MSVC installer does not put the compilers on the system path, and the install location may change To query the install location, MSVC comes with a vswhere exe command-line utility And to make the C/C++ compilers available inside the shell you are using, you need to run a bat file for the correct bitness and architecture (e g , for ARM64-based CPUs, use vcvarsarm64 bat) For detailed guidance, see Use the Microsoft C++ toolset from the command line",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "LLVM Similar to MSVC, LLVM does not put the compilers on the system path To set system path for LLVM compilers, users may need to use set command to put compilers on the system path To check compilers path for LLVMs clang-cl, try invoking LLVMs clang-cl compiler in the shell you use (clang-cl --version) Note Compilers should be on the system path (i e",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": ", the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH You can use any shell (e g , Powershell, cmd or Git Bash) to invoke a build To check that this is the case, try invoking a Fortran compiler in the shell you use (e g , flang --version)",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "Warning Currently, Conda environment is not yet supported officially on Windows on ARM64 The present approach uses virtualenv for building NumPy from source on Windows on ARM64 Building NumPy from source If you want to only install NumPy from source once and not do any development work, then the recommended way to build and install is to use pip Otherwise, conda is recommended Note If you dont have a conda installation yet, we recommend using Miniforge; any conda flavor will work though",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "Building from source to use NumPy Conda env If you are using a conda environment, pip is still the tool you use to invoke a from-source build of NumPy It is important to always use the --no-build-isolation flag to the pip install command, to avoid building against a numpy wheel from PyPI",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "In order for that to work you must first install the remaining build dependencies into the conda environment:  Either install all NumPy dev dependencies into a fresh conda environment mamba env create -f environment",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "yml  Or, install only the required build dependencies mamba install python numpy cython compilers openblas meson-python pkg-config  To build the latest stable release: pip install numpy --no-build-isolation --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github com/numpy/numpy git cd numpy git submodule update --init pip install",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": "--no-build-isolation Warning On Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the pip install command to fail These variables are only needed for flang and can be safely unset prior to running pip install Virtual env or system Python  To build the latest stable release: pip install numpy --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github com/numpy/numpy",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "git cd numpy git submodule update --init pip install Building from source for NumPy development If you want to build from source in order to work on NumPy itself, first clone the NumPy repository: git clone https://github com/numpy/numpy",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "git cd numpy git submodule update --init Then you want to do the following: Create a dedicated development environment (virtual environment or conda environment), Install all needed dependencies (build, and also test, doc and optional dependencies), Build NumPy with the spin developer interface",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "Step (3) is always the same, steps (1) and (2) are different between conda and virtual environments: Conda env To create a numpy-dev development environment with every required and optional dependency installed, run: mamba env create -f environment yml mamba activate numpy-dev Virtual env or system Python Note There are many tools to manage virtual environments, like venv, virtualenv/virtualenvwrapper, pyenv/pyenv-virtualenv, Poetry, PDM, Hatch, and more",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "Here we use the basic venv tool that is part of the Python stdlib You can use any other tool; all we need is an activated Python environment Create and activate a virtual environment in a new directory named venv ( note that the exact activation command may be different based on your OS and shell - see How venvs work in the venv docs) Linux python -m venv venv source venv/bin/activate macOS python -m venv venv source venv/bin/activate Windows python -m venv venv",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "\\venv\\Scripts\\activate Windows on ARM64 python -m venv venv \\venv\\Scripts\\activate Note Building NumPy with BLAS and LAPACK functions requires OpenBLAS library at Runtime In Windows on ARM64, this can be done by setting up pkg-config for OpenBLAS dependency The build steps for OpenBLAS for Windows on ARM64 can be found here Then install the Python-level dependencies from PyPI with: python -m pip install -r requirements/build_requirements",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "txt To build NumPy in an activated development environment, run: spin build This will install NumPy inside the repository (by default in a build-install directory) You can then run tests (spin test), drop into IPython (spin ipython), or take other development steps like build the html documentation or running benchmarks The spin interface is self-documenting, so please see spin --help and spin subcommand --help for detailed guidance",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "Warning In an activated conda environment on Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the build to fail These variables are only needed for flang and can be safely unset for build IDE support  editable installs While the spin interface is our recommended way of working on NumPy, it has one limitation: because of the custom install location, NumPy installed using spin will not be recognized automatically within an IDE (e g",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": ", for running a script via a run button, or setting breakpoints visually) This will work better with an in-place build (or editable install) Editable installs are supported It is important to understand that you may use either an editable install or ``spin`` in a given repository clone, but not both If you use editable installs, you have to use pytest and other development tools directly instead of using spin",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": "To use an editable install, ensure you start from a clean repository (run git clean -xdf if youve built with spin before) and have all dependencies set up correctly as described higher up on this page Then do:  Note: the --no-build-isolation is important pip install -e --no-build-isolation  To run the tests for, e g , the `numpy linalg` module: pytest numpy/linalg When making changes to NumPy code, including to compiled code, there is no need to manually rebuild or reinstall",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "NumPy is automatically rebuilt each time NumPy is imported by the Python interpreter; see the meson-python documentation on editable installs for more details on how that works under the hood When you run git clean -xdf, which removes the built extension modules, remember to also uninstall NumPy with pip uninstall numpy Warning Note that editable installs are fundamentally incomplete installs",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "Their only guarantee is that import numpy works - so they are suitable for working on NumPy itself, and for working on pure Python packages that depend on NumPy Headers, entrypoints, and other such things may not be available from an editable install",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "Customizing builds Compiler selection and customizing a build BLAS and LAPACK Cross compilation Building redistributable binaries Background information Understanding Meson Introspecting build steps Meson and distutils ways of doing things previous Testing the numpy",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_37"
        },
        {
          "content": "i typemaps next Compiler selection and customizing a build On this page System-level dependencies Building NumPy from source Building from source to use NumPy Building from source for NumPy development Customizing builds Background information",
          "url": "https://numpy.org/devdocs/building/index.html",
          "library": "numpy",
          "chunk_id": "numpy_38"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/dev/index.html",
      "title": "Contributing to NumPy \u2014 NumPy v2.4.dev0 Manual",
      "content": "Contributing to NumPy Contributing to NumPy Not a coder? Not a problem! NumPy is multi-faceted, and we can use a lot of help. These are all activities wed like to get help with (theyre all important, so we list them in alphabetical order): Code maintenance and development Community coordination DevOps Developing educational content  narrative documentation Fundraising Marketing Project management Translating content Website design and development Writing technical documentation We understand that everyone has a different level of experience, also NumPy is a pretty well-established project, so its hard to make assumptions about an ideal first-time-contributor. So, thats why we dont mark issues with the good-first-issue label. Instead, youll find issues labeled Sprintable. These issues can either be: Easily fixed when you have guidance from an experienced contributor (perfect for working in a sprint). A learning opportunity for those ready to dive deeper, even if youre not in a sprint. Additionally, depending on your prior experience, some Sprintable issues might be easy, while others could be more challenging for you. The rest of this document discusses working on the NumPy code base and documentation. Were in the process of updating our descriptions of other activities and roles. If you are interested in these other activities, please contact us! You can do this via the numpy-discussion mailing list, or on GitHub (open an issue or comment on a relevant issue). These are our preferred communication channels (open source is open by nature!), however if you prefer to discuss in a more private space first, you can do so on Slack (see numpy.org/contribute for details). Development process - summary Heres the short summary, complete TOC links are below: If you are a first-time contributor: Go to numpy/numpy and click the fork button to create your own copy of the project. Clone the project to your local computer: git clone --recurse-submodules https://github.com/your-username/numpy.git Change the directory: cd numpy Add the upstream repository: git remote add upstream https://github.com/numpy/numpy.git Now, git remote -v will show two remote repositories named: upstream, which refers to the numpy repository origin, which refers to your personal fork Pull the latest changes from upstream, including tags: git checkout main git pull upstream main --tags Initialize numpys submodules: git submodule update --init Develop your contribution: Create a branch for the feature you want to work on. Since the branch name will appear in the merge message, use a sensible name such as linspace-speedups: git checkout -b linspace-speedups Commit locally as you progress (git add and git commit) Use a properly formatted commit message, write tests that fail before your change and pass afterward, run all the tests locally. Be sure to document any changed behavior in docstrings, keeping to the NumPy docstring standard. To submit your contribution: Push your changes back to your fork on GitHub: git push origin linspace-speedups Go to GitHub. The new branch will show up with a green Pull Request button. Make sure the title and message are clear, concise, and self- explanatory. Then click the button to submit it. If your commit introduces a new feature or changes functionality, post on the mailing list to explain your changes. For bug fixes, documentation updates, etc., this is generally not necessary, though if you do not get any reaction, do feel free to ask for review. Review process: Reviewers (the other developers and interested community members) will write inline and/or general comments on your Pull Request (PR) to help you improve its implementation, documentation and style. Every single developer working on the project has their code reviewed, and weve come to see it as friendly conversation from which we all learn and the overall code quality benefits. Therefore, please dont let the review discourage you from contributing: its only aim is to improve the quality of project, not to criticize (we are, after all, very grateful for the time youre donating!). See our Reviewer Guidelines for more information. To update your PR, make your changes on your local repository, commit, run tests, and only if they succeed push to your fork. As soon as those changes are pushed up (to the same branch as before) the PR will update automatically. If you have no idea how to fix the test failures, you may push your changes anyway and ask for help in a PR comment. Various continuous integration (CI) services are triggered after each PR update to build the code, run unit tests, measure code coverage and check coding style of your branch. The CI tests must pass before your PR can be merged. If CI fails, you can find out why by clicking on the failed icon (red cross) and inspecting the build and test log. To avoid overuse and waste of this resource, test your work locally before committing. A PR must be approved by at least one core team member before merging. Approval means the core team member has carefully reviewed the changes, and the PR is ready for merging. Document changes Beyond changes to a functions docstring and possible description in the general documentation, if your change introduces any user-facing modifications they may need to be mentioned in the release notes. To add your change to the release notes, you need to create a short file with a summary and place it in doc/release/upcoming_changes. The file doc/release/upcoming_changes/README.rst details the format and filename conventions. If your change introduces a deprecation, make sure to discuss this first on GitHub or the mailing list first. If agreement on the deprecation is reached, follow NEP 23 deprecation policy to add the deprecation. Cross referencing issues If the PR relates to any issues, you can add the text xref gh-xxxx where xxxx is the number of the issue to github comments. Likewise, if the PR solves an issue, replace the xref with closes, fixes or any of the other flavors github accepts. In the source code, be sure to preface any issue or PR reference with gh-xxxx. For a more detailed discussion, read on and follow the links at the bottom of this page. Guidelines All code should have tests (see test coverage below for more details). All code should be documented. No changes are ever committed without review and approval by a core team member. Please ask politely on the PR or on the mailing list if you get no response to your pull request within a week. Stylistic guidelines Set up your editor to follow PEP 8 (remove trailing white space, no tabs, etc.). Check code with ruff. Use NumPy data types instead of strings (np.uint8 instead of \"uint8\"). Use the following import conventions: import numpy as np For C code, see NEP 45. Test coverage Pull requests (PRs) that modify code should either have new tests, or modify existing tests to fail before the PR and pass afterwards. You should run the tests before pushing a PR. Running NumPys test suite locally requires some additional packages, such as pytest and hypothesis. The additional testing dependencies are listed in requirements/test_requirements.txt in the top-level directory, and can conveniently be installed with:  python -m pip install -r requirements/test_requirements.txt Tests for a module should ideally cover all code in that module, i.e., statement coverage should be at 100. To measure the test coverage, run:  spin test --coverage This will create a report in html format at build/coverage, which can be viewed with your browser, e.g.:  firefox build/coverage/index.html Building docs To build the HTML documentation, use: spin docs You can also run make from the doc directory. make help lists all targets. To get the appropriate dependencies and other requirements, see Building the NumPy API and reference docs. Development process - details The rest of the story Setting up and using your development environment Recommended development setup Using virtual environments Building from source Testing builds Other build options Running tests Running linting Rebuilding  cleaning the workspace Debugging Understanding the code  getting started Building the NumPy API and reference docs Development environments Prerequisites Instructions Development workflow Basic workflow Additional things you might want to do Advanced debugging tools Finding C errors with additional tooling Using GitHub Codespaces for NumPy development GitHub Codespaces What is a codespace? Forking the NumPy repository Starting GitHub Codespaces Quick workspace tour Development workflow with GitHub Codespaces Rendering the NumPy documentation FAQs and troubleshooting Reviewer guidelines Who can be a reviewer? Communication guidelines Reviewer checklist Standard replies for reviewing NumPy benchmarks Usage Benchmarking versions Writing benchmarks NumPy C style guide For downstream package authors Understanding NumPys versioning and API/ABI stability Testing against the NumPy main branch or pre-releases Adding a dependency on NumPy Releasing a version How to prepare a release Step-by-step directions Branch walkthrough NumPy governance NumPy project governance and decision-making How to contribute to the NumPy documentation Documentation team meetings Whats needed Contributing fixes Contributing new pages Contributing indirectly Documentation style Documentation reading NumPy-specific workflow is in numpy-development-workflow. previous Meson and distutils ways of doing things next Setting up and using your development environment On this page Development process - summary Guidelines Stylistic guidelines Test coverage Building docs Development process - details",
      "code_blocks": [
        "git clone --recurse-submodules https://github.com/your-username/numpy.git",
        "git remote add upstream https://github.com/numpy/numpy.git",
        "git remote -v",
        "git checkout main\ngit pull upstream main --tags",
        "git submodule update --init",
        "git checkout -b linspace-speedups",
        "git push origin linspace-speedups",
        "doc/release/upcoming_changes",
        "doc/release/upcoming_changes/README.rst",
        "xref gh-xxxx",
        "import numpy as np",
        "requirements/test_requirements.txt",
        "$ python -m pip install -r requirements/test_requirements.txt",
        "$ spin test --coverage",
        "build/coverage",
        "$ firefox build/coverage/index.html"
      ],
      "chunks": [
        {
          "content": "Contributing to NumPy Contributing to NumPy Not a coder Not a problem NumPy is multi-faceted, and we can use a lot of help",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "These are all activities wed like to get help with (theyre all important, so we list them in alphabetical order): Code maintenance and development Community coordination DevOps Developing educational content  narrative documentation Fundraising Marketing Project management Translating content Website design and development Writing technical documentation We understand that everyone has a different level of experience, also NumPy is a pretty well-established project, so its hard to make assumptions about an ideal first-time-contributor",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "So, thats why we dont mark issues with the good-first-issue label Instead, youll find issues labeled Sprintable These issues can either be: Easily fixed when you have guidance from an experienced contributor (perfect for working in a sprint) A learning opportunity for those ready to dive deeper, even if youre not in a sprint Additionally, depending on your prior experience, some Sprintable issues might be easy, while others could be more challenging for you",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "The rest of this document discusses working on the NumPy code base and documentation Were in the process of updating our descriptions of other activities and roles If you are interested in these other activities, please contact us You can do this via the numpy-discussion mailing list, or on GitHub (open an issue or comment on a relevant issue) These are our preferred communication channels (open source is open by nature",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "), however if you prefer to discuss in a more private space first, you can do so on Slack (see numpy org/contribute for details) Development process - summary Heres the short summary, complete TOC links are below: If you are a first-time contributor: Go to numpy/numpy and click the fork button to create your own copy of the project Clone the project to your local computer: git clone --recurse-submodules https://github com/your-username/numpy",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "git Change the directory: cd numpy Add the upstream repository: git remote add upstream https://github com/numpy/numpy",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "git Now, git remote -v will show two remote repositories named: upstream, which refers to the numpy repository origin, which refers to your personal fork Pull the latest changes from upstream, including tags: git checkout main git pull upstream main --tags Initialize numpys submodules: git submodule update --init Develop your contribution: Create a branch for the feature you want to work on",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "Since the branch name will appear in the merge message, use a sensible name such as linspace-speedups: git checkout -b linspace-speedups Commit locally as you progress (git add and git commit) Use a properly formatted commit message, write tests that fail before your change and pass afterward, run all the tests locally Be sure to document any changed behavior in docstrings, keeping to the NumPy docstring standard",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "To submit your contribution: Push your changes back to your fork on GitHub: git push origin linspace-speedups Go to GitHub The new branch will show up with a green Pull Request button Make sure the title and message are clear, concise, and self- explanatory Then click the button to submit it If your commit introduces a new feature or changes functionality, post on the mailing list to explain your changes For bug fixes, documentation updates, etc",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": ", this is generally not necessary, though if you do not get any reaction, do feel free to ask for review Review process: Reviewers (the other developers and interested community members) will write inline and/or general comments on your Pull Request (PR) to help you improve its implementation, documentation and style Every single developer working on the project has their code reviewed, and weve come to see it as friendly conversation from which we all learn and the overall code quality benefits",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "Therefore, please dont let the review discourage you from contributing: its only aim is to improve the quality of project, not to criticize (we are, after all, very grateful for the time youre donating ) See our Reviewer Guidelines for more information To update your PR, make your changes on your local repository, commit, run tests, and only if they succeed push to your fork As soon as those changes are pushed up (to the same branch as before) the PR will update automatically",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "If you have no idea how to fix the test failures, you may push your changes anyway and ask for help in a PR comment Various continuous integration (CI) services are triggered after each PR update to build the code, run unit tests, measure code coverage and check coding style of your branch The CI tests must pass before your PR can be merged If CI fails, you can find out why by clicking on the failed icon (red cross) and inspecting the build and test log",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "To avoid overuse and waste of this resource, test your work locally before committing A PR must be approved by at least one core team member before merging Approval means the core team member has carefully reviewed the changes, and the PR is ready for merging Document changes Beyond changes to a functions docstring and possible description in the general documentation, if your change introduces any user-facing modifications they may need to be mentioned in the release notes",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "To add your change to the release notes, you need to create a short file with a summary and place it in doc/release/upcoming_changes The file doc/release/upcoming_changes/README rst details the format and filename conventions If your change introduces a deprecation, make sure to discuss this first on GitHub or the mailing list first If agreement on the deprecation is reached, follow NEP 23 deprecation policy to add the deprecation",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "Cross referencing issues If the PR relates to any issues, you can add the text xref gh-xxxx where xxxx is the number of the issue to github comments Likewise, if the PR solves an issue, replace the xref with closes, fixes or any of the other flavors github accepts In the source code, be sure to preface any issue or PR reference with gh-xxxx For a more detailed discussion, read on and follow the links at the bottom of this page",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "Guidelines All code should have tests (see test coverage below for more details) All code should be documented No changes are ever committed without review and approval by a core team member Please ask politely on the PR or on the mailing list if you get no response to your pull request within a week Stylistic guidelines Set up your editor to follow PEP 8 (remove trailing white space, no tabs, etc ) Check code with ruff Use NumPy data types instead of strings (np uint8 instead of \"uint8\")",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "Use the following import conventions: import numpy as np For C code, see NEP 45 Test coverage Pull requests (PRs) that modify code should either have new tests, or modify existing tests to fail before the PR and pass afterwards You should run the tests before pushing a PR Running NumPys test suite locally requires some additional packages, such as pytest and hypothesis The additional testing dependencies are listed in requirements/test_requirements",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "txt in the top-level directory, and can conveniently be installed with:  python -m pip install -r requirements/test_requirements txt Tests for a module should ideally cover all code in that module, i e , statement coverage should be at 100 To measure the test coverage, run:  spin test --coverage This will create a report in html format at build/coverage, which can be viewed with your browser, e g :  firefox build/coverage/index",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "html Building docs To build the HTML documentation, use: spin docs You can also run make from the doc directory make help lists all targets To get the appropriate dependencies and other requirements, see Building the NumPy API and reference docs",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "Development process - details The rest of the story Setting up and using your development environment Recommended development setup Using virtual environments Building from source Testing builds Other build options Running tests Running linting Rebuilding  cleaning the workspace Debugging Understanding the code  getting started Building the NumPy API and reference docs Development environments Prerequisites Instructions Development workflow Basic workflow Additional things you might want to do Advanced debugging tools Finding C errors with additional tooling Using GitHub Codespaces for NumPy development GitHub Codespaces What is a codespace",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "Forking the NumPy repository Starting GitHub Codespaces Quick workspace tour Development workflow with GitHub Codespaces Rendering the NumPy documentation FAQs and troubleshooting Reviewer guidelines Who can be a reviewer",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "Communication guidelines Reviewer checklist Standard replies for reviewing NumPy benchmarks Usage Benchmarking versions Writing benchmarks NumPy C style guide For downstream package authors Understanding NumPys versioning and API/ABI stability Testing against the NumPy main branch or pre-releases Adding a dependency on NumPy Releasing a version How to prepare a release Step-by-step directions Branch walkthrough NumPy governance NumPy project governance and decision-making How to contribute to the NumPy documentation Documentation team meetings Whats needed Contributing fixes Contributing new pages Contributing indirectly Documentation style Documentation reading NumPy-specific workflow is in numpy-development-workflow",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "previous Meson and distutils ways of doing things next Setting up and using your development environment On this page Development process - summary Guidelines Stylistic guidelines Test coverage Building docs Development process - details",
          "url": "https://numpy.org/devdocs/dev/index.html",
          "library": "numpy",
          "chunk_id": "numpy_22"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/release.html",
      "title": "Release notes \u2014 NumPy v2.4.dev0 Manual",
      "content": "NumPy user guide Release notes Release notes 2.4.0 Highlights Deprecations Compatibility notes Performance improvements and changes Changes 2.3.1 Contributors Pull requests merged 2.3.0 Highlights New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2.2.6 Contributors Pull requests merged 2.2.5 Contributors Pull requests merged 2.2.4 Contributors Pull requests merged 2.2.3 Contributors Pull requests merged 2.2.2 Contributors Pull requests merged 2.2.1 Contributors Pull requests merged 2.2.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 2.1.3 Improvements Changes Contributors Pull requests merged 2.1.2 Contributors Pull requests merged 2.1.1 Contributors Pull requests merged 2.1.0 New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2.0.2 Contributors Pull requests merged 2.0.1 Improvements Contributors Pull requests merged 2.0.0 Highlights NumPy 2.0 Python API removals Deprecations Expired deprecations Compatibility notes C API changes NumPy 2.0 C API removals New Features Improvements Changes 1.26.4 Contributors Pull requests merged 1.26.3 Compatibility Improvements Contributors Pull requests merged 1.26.2 Contributors Pull requests merged 1.26.1 Build system changes New features Contributors Pull requests merged 1.26.0 New Features Improvements Build system changes Contributors Pull requests merged 1.25.2 Contributors Pull requests merged 1.25.1 Contributors Pull requests merged 1.25.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1.24.4 Contributors Pull requests merged 1.24.3 Contributors Pull requests merged 1.24.2 Contributors Pull requests merged 1.24.1 Contributors Pull requests merged 1.24.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1.23.5 Contributors Pull requests merged 1.23.4 Contributors Pull requests merged 1.23.3 Contributors Pull requests merged 1.23.2 Contributors Pull requests merged 1.23.1 Contributors Pull requests merged 1.23.0 New functions Deprecations Expired deprecations New Features Compatibility notes Improvements Performance improvements and changes 1.22.4 Contributors Pull requests merged 1.22.3 Contributors Pull requests merged 1.22.2 Contributors Pull requests merged 1.22.1 Contributors Pull requests merged 1.22.0 Expired deprecations Deprecations Compatibility notes C API changes New Features Improvements 1.21.6 1.21.5 Contributors Pull requests merged 1.21.4 Contributors Pull requests merged 1.21.3 Contributors Pull requests merged 1.21.2 Contributors Pull requests merged 1.21.1 Contributors Pull requests merged 1.21.0 New functions Expired deprecations Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements Changes 1.20.3 Contributors Pull requests merged 1.20.2 Contributors Pull requests merged 1.20.1 Highlights Contributors Pull requests merged 1.20.0 New functions Deprecations Future Changes Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements and changes Changes 1.19.5 Contributors Pull requests merged 1.19.4 Contributors Pull requests merged 1.19.3 Contributors Pull requests merged 1.19.2 Improvements Contributors Pull requests merged 1.19.1 Contributors Pull requests merged 1.19.0 Highlights Expired deprecations Compatibility notes Deprecations C API changes New Features Improvements Improve detection of CPU features Changes 1.18.5 Contributors Pull requests merged 1.18.4 Contributors Pull requests merged 1.18.3 Highlights Contributors Pull requests merged 1.18.2 Contributors Pull requests merged 1.18.1 Contributors Pull requests merged 1.18.0 Highlights New functions Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Changes 1.17.5 Contributors Pull requests merged 1.17.4 Highlights Contributors Pull requests merged 1.17.3 Highlights Compatibility notes Contributors Pull requests merged 1.17.2 Contributors Pull requests merged 1.17.1 Contributors Pull requests merged 1.17.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1.16.6 Highlights New functions Compatibility notes Improvements Contributors Pull requests merged 1.16.5 Contributors Pull requests merged 1.16.4 New deprecations Compatibility notes Changes Contributors Pull requests merged 1.16.3 Compatibility notes Improvements Changes 1.16.2 Compatibility notes Contributors Pull requests merged 1.16.1 Contributors Enhancements Compatibility notes New Features Improvements Changes 1.16.0 Highlights New functions New deprecations Expired deprecations Future changes Compatibility notes C API changes New Features Improvements Changes 1.15.4 Compatibility Note Contributors Pull requests merged 1.15.3 Compatibility Note Contributors Pull requests merged 1.15.2 Compatibility Note Contributors Pull requests merged 1.15.1 Compatibility Note Contributors Pull requests merged 1.15.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements 1.14.6 Contributors Pull requests merged 1.14.5 Contributors Pull requests merged 1.14.4 Contributors Pull requests merged 1.14.3 Contributors Pull requests merged 1.14.2 Contributors Pull requests merged 1.14.1 Contributors Pull requests merged 1.14.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1.13.3 Contributors Pull requests merged 1.13.2 Contributors Pull requests merged 1.13.1 Pull requests merged Contributors 1.13.0 Highlights New functions Deprecations Future Changes Build System Changes Compatibility notes C API changes New Features Improvements Changes 1.12.1 Bugs Fixed 1.12.0 Highlights Dropped Support Added Support Build System Changes Deprecations Future Changes Compatibility notes New Features Improvements Changes 1.11.3 Contributors to maintenance/1.11.3 Pull Requests Merged 1.11.2 Pull Requests Merged 1.11.1 Fixes Merged 1.11.0 Highlights Build System Changes Future Changes Compatibility notes New Features Improvements Changes Deprecations FutureWarnings 1.10.4 Compatibility notes Issues Fixed Merged PRs 1.10.3 1.10.2 Compatibility notes Issues Fixed Merged PRs Notes 1.10.1 1.10.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations 1.9.2 Issues fixed 1.9.1 Issues fixed 1.9.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Deprecations 1.8.2 Issues fixed 1.8.1 Issues fixed Changes Deprecations 1.8.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations Authors 1.7.2 Issues fixed 1.7.1 Issues fixed 1.7.0 Highlights Compatibility notes New features Changes Deprecations 1.6.2 Issues fixed Changes 1.6.1 Issues Fixed 1.6.0 Highlights New features Changes Deprecated features Removed features 1.5.0 Highlights New features Changes 1.4.0 Highlights New features Improvements Deprecations Internal changes 1.3.0 Highlights New features Deprecated features Documentation changes New C API Internal changes previous Glossary next NumPy 2.4.0 Release Notes",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy user guide Release notes Release notes 2 4 0 Highlights Deprecations Compatibility notes Performance improvements and changes Changes 2 3 1 Contributors Pull requests merged 2 3 0 Highlights New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2 2 6 Contributors Pull requests merged 2 2 5 Contributors Pull requests merged 2 2 4 Contributors Pull requests merged 2 2 3 Contributors Pull requests merged 2 2",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "2 Contributors Pull requests merged 2 2 1 Contributors Pull requests merged 2 2 0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 2 1 3 Improvements Changes Contributors Pull requests merged 2 1 2 Contributors Pull requests merged 2 1 1 Contributors Pull requests merged 2 1 0 New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2 0",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "2 Contributors Pull requests merged 2 0 1 Improvements Contributors Pull requests merged 2 0 0 Highlights NumPy 2 0 Python API removals Deprecations Expired deprecations Compatibility notes C API changes NumPy 2 0 C API removals New Features Improvements Changes 1 26 4 Contributors Pull requests merged 1 26 3 Compatibility Improvements Contributors Pull requests merged 1 26 2 Contributors Pull requests merged 1 26 1 Build system changes New features Contributors Pull requests merged 1 26",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "0 New Features Improvements Build system changes Contributors Pull requests merged 1 25 2 Contributors Pull requests merged 1 25 1 Contributors Pull requests merged 1 25 0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1 24 4 Contributors Pull requests merged 1 24 3 Contributors Pull requests merged 1 24 2 Contributors Pull requests merged 1 24 1 Contributors Pull requests merged 1 24",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1 23 5 Contributors Pull requests merged 1 23 4 Contributors Pull requests merged 1 23 3 Contributors Pull requests merged 1 23 2 Contributors Pull requests merged 1 23 1 Contributors Pull requests merged 1 23 0 New functions Deprecations Expired deprecations New Features Compatibility notes Improvements Performance improvements and changes 1 22",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "4 Contributors Pull requests merged 1 22 3 Contributors Pull requests merged 1 22 2 Contributors Pull requests merged 1 22 1 Contributors Pull requests merged 1 22 0 Expired deprecations Deprecations Compatibility notes C API changes New Features Improvements 1 21 6 1 21 5 Contributors Pull requests merged 1 21 4 Contributors Pull requests merged 1 21 3 Contributors Pull requests merged 1 21 2 Contributors Pull requests merged 1 21 1 Contributors Pull requests merged 1 21",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "0 New functions Expired deprecations Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements Changes 1 20 3 Contributors Pull requests merged 1 20 2 Contributors Pull requests merged 1 20 1 Highlights Contributors Pull requests merged 1 20 0 New functions Deprecations Future Changes Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements and changes Changes 1 19",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "5 Contributors Pull requests merged 1 19 4 Contributors Pull requests merged 1 19 3 Contributors Pull requests merged 1 19 2 Improvements Contributors Pull requests merged 1 19 1 Contributors Pull requests merged 1 19 0 Highlights Expired deprecations Compatibility notes Deprecations C API changes New Features Improvements Improve detection of CPU features Changes 1 18 5 Contributors Pull requests merged 1 18 4 Contributors Pull requests merged 1 18",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "3 Highlights Contributors Pull requests merged 1 18 2 Contributors Pull requests merged 1 18 1 Contributors Pull requests merged 1 18 0 Highlights New functions Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Changes 1 17 5 Contributors Pull requests merged 1 17 4 Highlights Contributors Pull requests merged 1 17 3 Highlights Compatibility notes Contributors Pull requests merged 1 17 2 Contributors Pull requests merged 1 17",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "1 Contributors Pull requests merged 1 17 0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1 16 6 Highlights New functions Compatibility notes Improvements Contributors Pull requests merged 1 16 5 Contributors Pull requests merged 1 16 4 New deprecations Compatibility notes Changes Contributors Pull requests merged 1 16 3 Compatibility notes Improvements Changes 1 16 2 Compatibility notes Contributors Pull requests merged 1",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "16 1 Contributors Enhancements Compatibility notes New Features Improvements Changes 1 16 0 Highlights New functions New deprecations Expired deprecations Future changes Compatibility notes C API changes New Features Improvements Changes 1 15 4 Compatibility Note Contributors Pull requests merged 1 15 3 Compatibility Note Contributors Pull requests merged 1 15 2 Compatibility Note Contributors Pull requests merged 1 15 1 Compatibility Note Contributors Pull requests merged 1 15",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements 1 14 6 Contributors Pull requests merged 1 14 5 Contributors Pull requests merged 1 14 4 Contributors Pull requests merged 1 14 3 Contributors Pull requests merged 1 14 2 Contributors Pull requests merged 1 14 1 Contributors Pull requests merged 1 14 0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1 13",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "3 Contributors Pull requests merged 1 13 2 Contributors Pull requests merged 1 13 1 Pull requests merged Contributors 1 13 0 Highlights New functions Deprecations Future Changes Build System Changes Compatibility notes C API changes New Features Improvements Changes 1 12 1 Bugs Fixed 1 12 0 Highlights Dropped Support Added Support Build System Changes Deprecations Future Changes Compatibility notes New Features Improvements Changes 1 11 3 Contributors to maintenance/1 11 3 Pull Requests Merged 1",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "11 2 Pull Requests Merged 1 11 1 Fixes Merged 1 11 0 Highlights Build System Changes Future Changes Compatibility notes New Features Improvements Changes Deprecations FutureWarnings 1 10 4 Compatibility notes Issues Fixed Merged PRs 1 10 3 1 10 2 Compatibility notes Issues Fixed Merged PRs Notes 1 10 1 1 10 0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations 1 9 2 Issues fixed 1 9 1 Issues fixed 1 9",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Deprecations 1 8 2 Issues fixed 1 8 1 Issues fixed Changes Deprecations 1 8 0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations Authors 1 7 2 Issues fixed 1 7 1 Issues fixed 1 7 0 Highlights Compatibility notes New features Changes Deprecations 1 6 2 Issues fixed Changes 1 6 1 Issues Fixed 1 6",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "0 Highlights New features Changes Deprecated features Removed features 1 5 0 Highlights New features Changes 1 4 0 Highlights New features Improvements Deprecations Internal changes 1 3 0 Highlights New features Deprecated features Documentation changes New C API Internal changes previous Glossary next NumPy 2 4 0 Release Notes",
          "url": "https://numpy.org/devdocs/release.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/2.3#main-content",
      "title": "NumPy documentation \u2014 NumPy v2.3 Manual",
      "content": "NumPy documentation Version: 2.3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Getting started New to NumPy? Check out the Absolute Beginners Guide. It contains an introduction to NumPys main concepts and links to additional tutorials. To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation. To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts. To the reference guide Contributors guide Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy. To the contributors guide next NumPy user guide",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy documentation Version: 2 3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python",
          "url": "https://numpy.org/doc/2.3#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more Getting started New to NumPy Check out the Absolute Beginners Guide",
          "url": "https://numpy.org/doc/2.3#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "It contains an introduction to NumPys main concepts and links to additional tutorials To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy The reference describes how the methods work and which parameters can be used",
          "url": "https://numpy.org/doc/2.3#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "It assumes that you have an understanding of the key concepts To the reference guide Contributors guide Want to add to the codebase Can help add translation or a flowchart to the documentation The contributing guidelines will guide you through the process of improving NumPy To the contributors guide next NumPy user guide",
          "url": "https://numpy.org/doc/2.3#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/2.2#main-content",
      "title": "NumPy documentation \u2014 NumPy v2.2 Manual",
      "content": "NumPy documentation Version: 2.2 Download documentation: User guide (PDF)  Reference guide (PDF)  All (ZIP)  Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Getting started New to NumPy? Check out the Absolute Beginners Guide. It contains an introduction to NumPys main concepts and links to additional tutorials. To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation. To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts. To the reference guide Contributors guide Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy. To the contributors guide next NumPy user guide",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy documentation Version: 2 2 Download documentation: User guide (PDF)  Reference guide (PDF)  All (ZIP)  Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python",
          "url": "https://numpy.org/doc/2.2#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more Getting started New to NumPy Check out the Absolute Beginners Guide",
          "url": "https://numpy.org/doc/2.2#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "It contains an introduction to NumPys main concepts and links to additional tutorials To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy The reference describes how the methods work and which parameters can be used",
          "url": "https://numpy.org/doc/2.2#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "It assumes that you have an understanding of the key concepts To the reference guide Contributors guide Want to add to the codebase Can help add translation or a flowchart to the documentation The contributing guidelines will guide you through the process of improving NumPy To the contributors guide next NumPy user guide",
          "url": "https://numpy.org/doc/2.2#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/user/index.html#main-content",
      "title": "NumPy user guide \u2014 NumPy v2.3 Manual",
      "content": "NumPy user guide NumPy user guide This guide is an overview and explains the important features; details are found in NumPy reference. Getting started What is NumPy? Installation NumPy quickstart NumPy: the absolute basics for beginners Fundamentals and usage NumPy fundamentals Array creation Indexing on ndarrays I/O with NumPy Data types Broadcasting Copies and views Working with Arrays of Strings And Bytes Structured arrays Universal functions (ufunc) basics NumPy for MATLAB users NumPy tutorials NumPy how-tos Advanced usage and interoperability Using NumPy C-API F2PY user guide and reference manual Under-the-hood documentation for developers Interoperability with NumPy previous NumPy documentation next What is NumPy?",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy user guide NumPy user guide This guide is an overview and explains the important features; details are found in NumPy reference Getting started What is NumPy",
          "url": "https://numpy.org/doc/stable/user/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Installation NumPy quickstart NumPy: the absolute basics for beginners Fundamentals and usage NumPy fundamentals Array creation Indexing on ndarrays I/O with NumPy Data types Broadcasting Copies and views Working with Arrays of Strings And Bytes Structured arrays Universal functions (ufunc) basics NumPy for MATLAB users NumPy tutorials NumPy how-tos Advanced usage and interoperability Using NumPy C-API F2PY user guide and reference manual Under-the-hood documentation for developers Interoperability with NumPy previous NumPy documentation next What is NumPy",
          "url": "https://numpy.org/doc/stable/user/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/index.html",
      "title": "NumPy documentation \u2014 NumPy v2.3 Manual",
      "content": "NumPy documentation Version: 2.3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Getting started New to NumPy? Check out the Absolute Beginners Guide. It contains an introduction to NumPys main concepts and links to additional tutorials. To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation. To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts. To the reference guide Contributors guide Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy. To the contributors guide next NumPy user guide",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy documentation Version: 2 3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python",
          "url": "https://numpy.org/doc/stable/index.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more Getting started New to NumPy Check out the Absolute Beginners Guide",
          "url": "https://numpy.org/doc/stable/index.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "It contains an introduction to NumPys main concepts and links to additional tutorials To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy The reference describes how the methods work and which parameters can be used",
          "url": "https://numpy.org/doc/stable/index.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "It assumes that you have an understanding of the key concepts To the reference guide Contributors guide Want to add to the codebase Can help add translation or a flowchart to the documentation The contributing guidelines will guide you through the process of improving NumPy To the contributors guide next NumPy user guide",
          "url": "https://numpy.org/doc/stable/index.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/reference/index.html#main-content",
      "title": "NumPy reference \u2014 NumPy v2.3 Manual",
      "content": "NumPy reference NumPy reference Release: 2.3 Date: June 09, 2025 This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do. For learning how to use NumPy, see the complete documentation. Python API NumPys module structure Array objects The N-dimensional array (ndarray) Scalars Data type objects (dtype) Data type promotion in NumPy Iterating over arrays Standard array subclasses Masked arrays The array interface protocol Datetimes and timedeltas Universal functions (ufunc) Routines and objects by topic Constants Array creation routines Array manipulation routines Bit-wise operations String functionality Datetime support functions Data type routines Mathematical functions with automatic domain Floating point error handling Exceptions and Warnings Discrete Fourier Transform Functional programming Input and output Indexing routines Linear algebra Logic functions Masked array operations Mathematical functions Miscellaneous routines Polynomials Random sampling Set routines Sorting, searching, and counting Statistics Test support Window functions Typing (numpy.typing) Packaging C API NumPy C-API Python types and C-structures System configuration Data type API Array API Array iterator API ufunc API Generalized universal function API NpyString API NumPy core math library Datetime API C API deprecations Memory management in NumPy Other topics Array API standard compatibility CPU/SIMD optimizations Thread Safety Global Configuration Options NumPy security Status of numpy.distutils and migration advice numpy.distutils user guide NumPy and SWIG Acknowledgements Large parts of this manual originate from Travis E. Oliphants book Guide to NumPy (which generously entered Public Domain in August 2008). The reference documentation for many of the functions are written by numerous contributors and developers of NumPy. previous NumPy license next NumPys module structure On this page Python API C API Other topics Acknowledgements",
      "code_blocks": [
        "numpy.typing",
        "numpy.distutils",
        "numpy.distutils"
      ],
      "chunks": [
        {
          "content": "NumPy reference NumPy reference Release: 2 3 Date: June 09, 2025 This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do For learning how to use NumPy, see the complete documentation",
          "url": "https://numpy.org/doc/stable/reference/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Python API NumPys module structure Array objects The N-dimensional array (ndarray) Scalars Data type objects (dtype) Data type promotion in NumPy Iterating over arrays Standard array subclasses Masked arrays The array interface protocol Datetimes and timedeltas Universal functions (ufunc) Routines and objects by topic Constants Array creation routines Array manipulation routines Bit-wise operations String functionality Datetime support functions Data type routines Mathematical functions with automatic domain Floating point error handling Exceptions and Warnings Discrete Fourier Transform Functional programming Input and output Indexing routines Linear algebra Logic functions Masked array operations Mathematical functions Miscellaneous routines Polynomials Random sampling Set routines Sorting, searching, and counting Statistics Test support Window functions Typing (numpy",
          "url": "https://numpy.org/doc/stable/reference/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "typing) Packaging C API NumPy C-API Python types and C-structures System configuration Data type API Array API Array iterator API ufunc API Generalized universal function API NpyString API NumPy core math library Datetime API C API deprecations Memory management in NumPy Other topics Array API standard compatibility CPU/SIMD optimizations Thread Safety Global Configuration Options NumPy security Status of numpy distutils and migration advice numpy",
          "url": "https://numpy.org/doc/stable/reference/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "distutils user guide NumPy and SWIG Acknowledgements Large parts of this manual originate from Travis E Oliphants book Guide to NumPy (which generously entered Public Domain in August 2008) The reference documentation for many of the functions are written by numerous contributors and developers of NumPy previous NumPy license next NumPys module structure On this page Python API C API Other topics Acknowledgements",
          "url": "https://numpy.org/doc/stable/reference/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/building/index.html#main-content",
      "title": "Building from source \u2014 NumPy v2.3 Manual",
      "content": "Building from source Building from source Note If you are only trying to install NumPy, we recommend using binaries - see Installation for details on that. Building NumPy from source requires setting up system-level dependencies (compilers, BLAS/LAPACK libraries, etc.) first, and then invoking a build. The build may be done in order to install NumPy for local usage, develop NumPy itself, or build redistributable binary packages. And it may be desired to customize aspects of how the build is done. This guide will cover all these aspects. In addition, it provides background information on how the NumPy build works, and links to up-to-date guides for generic Python build  packaging documentation that is relevant. System-level dependencies NumPy uses compiled code for speed, which means you need compilers and some other system-level (i.e, non-Python / non-PyPI) dependencies to build it on your system. Note If you are using Conda, you can skip the steps in this section - with the exception of installing compilers for Windows or the Apple Developer Tools for macOS. All other dependencies will be installed automatically by the mamba env create -f environment.yml command. Linux If you want to use the system Python and pip, you will need: C and C++ compilers (typically GCC). Python header files (typically a package named python3-dev or python3-devel) BLAS and LAPACK libraries. OpenBLAS is the NumPy default; other variants include Apple Accelerate, MKL, ATLAS and Netlib (or Reference) BLAS and LAPACK. pkg-config for dependency detection. A Fortran compiler is needed only for running the f2py tests. The instructions below include a Fortran compiler, however you can safely leave it out. Debian/Ubuntu Linux To install NumPy build requirements, you can do: sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev Alternatively, you can do: sudo apt build-dep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. Fedora To install NumPy build requirements, you can do: sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo dnf builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. CentOS/RHEL To install NumPy build requirements, you can do: sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo yum-builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. Arch To install NumPy build requirements, you can do: sudo pacman -S gcc-fortran openblas pkgconf macOS Install Apple Developer Tools. An easy way to do this is to open a terminal window, enter the command: xcode-select --install and follow the prompts. Apple Developer Tools includes Git, the Clang C/C++ compilers, and other development utilities that may be required. Do not use the macOS system Python. Instead, install Python with the python.org installer or with a package manager like Homebrew, MacPorts or Fink. On macOS =13.3, the easiest build option is to use Accelerate, which is already installed and will be automatically used by default. On older macOS versions you need a different BLAS library, most likely OpenBLAS, plus pkg-config to detect OpenBLAS. These are easiest to install with Homebrew: brew install openblas pkg-config gfortran Windows On Windows, the use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together. If you dont need to run the f2py tests, simply using MSVC is easiest. Otherwise, you will need one of these sets of compilers: MSVC + Intel Fortran (ifort) Intel compilers (icc, ifort) Mingw-w64 compilers (gcc, g++, gfortran) Compared to macOS and Linux, building NumPy on Windows is a little more difficult, due to the need to set up these compilers. It is not possible to just call a one-liner on the command prompt as you would on other platforms. First, install Microsoft Visual Studio - the 2019 Community Edition or any newer version will work (see the Visual Studio download site). This is needed even if you use the MinGW-w64 or Intel compilers, in order to ensure you have the Windows Universal C Runtime (the other components of Visual Studio are not needed when using Mingw-w64, and can be deselected if desired, to save disk space). The recommended version of the UCRT is = 10.0.22621.0. MSVC The MSVC installer does not put the compilers on the system path, and the install location may change. To query the install location, MSVC comes with a vswhere.exe command-line utility. And to make the C/C++ compilers available inside the shell you are using, you need to run a .bat file for the correct bitness and architecture (e.g., for 64-bit Intel CPUs, use vcvars64.bat). If using a Conda environment while a version of Visual Studio 2019+ is installed that includes the MSVC v142 package (VS 2019 C++ x86/x64 build tools), activating the conda environment should cause Visual Studio to be found and the appropriate .bat file executed to set these variables. For detailed guidance, see Use the Microsoft C++ toolset from the command line. Intel Similar to MSVC, the Intel compilers are designed to be used with an activation script (Intel\\oneAPI\\setvars.bat) that you run in the shell you are using. This makes the compilers available on the path. For detailed guidance, see Get Started with the Intel oneAPI HPC Toolkit for Windows. MinGW-w64 There are several sources of binaries for MinGW-w64. We recommend the RTools versions, which can be installed with Chocolatey (see Chocolatey install instructions here): choco install rtools -y --no-progress --force --version=4.0.0.20220206 Note Compilers should be on the system path (i.e., the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH. You can use any shell (e.g., Powershell, cmd or Git Bash) to invoke a build. To check that this is the case, try invoking a Fortran compiler in the shell you use (e.g., gfortran --version or ifort --version). Warning When using a conda environment it is possible that the environment creation will not work due to an outdated Fortran compiler. If that happens, remove the compilers entry from environment.yml and try again. The Fortran compiler should be installed as described in this section. Windows on ARM64 In Windows on ARM64, the set of a compiler options that are available for building NumPy are limited. Compilers such as GCC and GFortran are not yet supported for Windows on ARM64. Currently, the NumPy build for Windows on ARM64 is supported with MSVC and LLVM toolchains. The use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together. If you dont need to run the f2py tests, simply using MSVC is easiest. Otherwise, you will need the following set of compilers: MSVC + flang (cl, flang) LLVM + flang (clang-cl, flang) First, install Microsoft Visual Studio - the 2022 Community Edition will work (see the Visual Studio download site). Ensure that you have installed necessary Visual Studio components for building NumPy on WoA from here. To use the flang compiler for Windows on ARM64, install Latest LLVM toolchain for WoA from here. MSVC The MSVC installer does not put the compilers on the system path, and the install location may change. To query the install location, MSVC comes with a vswhere.exe command-line utility. And to make the C/C++ compilers available inside the shell you are using, you need to run a .bat file for the correct bitness and architecture (e.g., for ARM64-based CPUs, use vcvarsarm64.bat). For detailed guidance, see Use the Microsoft C++ toolset from the command line. LLVM Similar to MSVC, LLVM does not put the compilers on the system path. To set system path for LLVM compilers, users may need to use set command to put compilers on the system path. To check compilers path for LLVMs clang-cl, try invoking LLVMs clang-cl compiler in the shell you use (clang-cl --version). Note Compilers should be on the system path (i.e., the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH. You can use any shell (e.g., Powershell, cmd or Git Bash) to invoke a build. To check that this is the case, try invoking a Fortran compiler in the shell you use (e.g., flang --version). Warning Currently, Conda environment is not yet supported officially on Windows on ARM64. The present approach uses virtualenv for building NumPy from source on Windows on ARM64. Building NumPy from source If you want to only install NumPy from source once and not do any development work, then the recommended way to build and install is to use pip. Otherwise, conda is recommended. Note If you dont have a conda installation yet, we recommend using Miniforge; any conda flavor will work though. Building from source to use NumPy Conda env If you are using a conda environment, pip is still the tool you use to invoke a from-source build of NumPy. It is important to always use the --no-build-isolation flag to the pip install command, to avoid building against a numpy wheel from PyPI. In order for that to work you must first install the remaining build dependencies into the conda environment:  Either install all NumPy dev dependencies into a fresh conda environment mamba env create -f environment.yml  Or, install only the required build dependencies mamba install python numpy cython compilers openblas meson-python pkg-config  To build the latest stable release: pip install numpy --no-build-isolation --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init pip install . --no-build-isolation Warning On Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the pip install command to fail. These variables are only needed for flang and can be safely unset prior to running pip install. Virtual env or system Python  To build the latest stable release: pip install numpy --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init pip install . Building from source for NumPy development If you want to build from source in order to work on NumPy itself, first clone the NumPy repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init Then you want to do the following: Create a dedicated development environment (virtual environment or conda environment), Install all needed dependencies (build, and also test, doc and optional dependencies), Build NumPy with the spin developer interface. Step (3) is always the same, steps (1) and (2) are different between conda and virtual environments: Conda env To create a numpy-dev development environment with every required and optional dependency installed, run: mamba env create -f environment.yml mamba activate numpy-dev Virtual env or system Python Note There are many tools to manage virtual environments, like venv, virtualenv/virtualenvwrapper, pyenv/pyenv-virtualenv, Poetry, PDM, Hatch, and more. Here we use the basic venv tool that is part of the Python stdlib. You can use any other tool; all we need is an activated Python environment. Create and activate a virtual environment in a new directory named venv ( note that the exact activation command may be different based on your OS and shell - see How venvs work in the venv docs). Linux python -m venv venv source venv/bin/activate macOS python -m venv venv source venv/bin/activate Windows python -m venv venv .\\venv\\Scripts\\activate Windows on ARM64 python -m venv venv .\\venv\\Scripts\\activate Note Building NumPy with BLAS and LAPACK functions requires OpenBLAS library at Runtime. In Windows on ARM64, this can be done by setting up pkg-config for OpenBLAS dependency. The build steps for OpenBLAS for Windows on ARM64 can be found here. Then install the Python-level dependencies from PyPI with: python -m pip install -r requirements/build_requirements.txt To build NumPy in an activated development environment, run: spin build This will install NumPy inside the repository (by default in a build-install directory). You can then run tests (spin test), drop into IPython (spin ipython), or take other development steps like build the html documentation or running benchmarks. The spin interface is self-documenting, so please see spin --help and spin subcommand --help for detailed guidance. Warning In an activated conda environment on Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the build to fail. These variables are only needed for flang and can be safely unset for build. IDE support  editable installs While the spin interface is our recommended way of working on NumPy, it has one limitation: because of the custom install location, NumPy installed using spin will not be recognized automatically within an IDE (e.g., for running a script via a run button, or setting breakpoints visually). This will work better with an in-place build (or editable install). Editable installs are supported. It is important to understand that you may use either an editable install or ``spin`` in a given repository clone, but not both. If you use editable installs, you have to use pytest and other development tools directly instead of using spin. To use an editable install, ensure you start from a clean repository (run git clean -xdf if youve built with spin before) and have all dependencies set up correctly as described higher up on this page. Then do:  Note: the --no-build-isolation is important! pip install -e . --no-build-isolation  To run the tests for, e.g., the `numpy.linalg` module: pytest numpy/linalg When making changes to NumPy code, including to compiled code, there is no need to manually rebuild or reinstall. NumPy is automatically rebuilt each time NumPy is imported by the Python interpreter; see the meson-python documentation on editable installs for more details on how that works under the hood. When you run git clean -xdf, which removes the built extension modules, remember to also uninstall NumPy with pip uninstall numpy. Warning Note that editable installs are fundamentally incomplete installs. Their only guarantee is that import numpy works - so they are suitable for working on NumPy itself, and for working on pure Python packages that depend on NumPy. Headers, entrypoints, and other such things may not be available from an editable install. Customizing builds Compiler selection and customizing a build BLAS and LAPACK Cross compilation Building redistributable binaries Background information Understanding Meson Introspecting build steps Meson and distutils ways of doing things previous Testing the numpy.i typemaps next Compiler selection and customizing a build On this page System-level dependencies Building NumPy from source Building from source to use NumPy Building from source for NumPy development Customizing builds Background information",
      "code_blocks": [
        "mamba env create -f environment.yml",
        "python3-dev",
        "python3-devel",
        "sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev",
        "sudo apt build-dep numpy",
        "sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig",
        "sudo dnf builddep numpy",
        "sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig",
        "sudo yum-builddep numpy",
        "sudo pacman -S gcc-fortran openblas pkgconf",
        "xcode-select --install",
        "brew install openblas pkg-config gfortran",
        "vswhere.exe",
        "vcvars64.bat",
        "Intel\\oneAPI\\setvars.bat",
        "choco install rtools -y --no-progress --force --version=4.0.0.20220206",
        "gfortran\n--version",
        "ifort --version",
        "environment.yml",
        "vswhere.exe",
        "vcvarsarm64.bat",
        "clang-cl --version",
        "flang\n--version",
        "--no-build-isolation",
        "pip install",
        "# Either install all NumPy dev dependencies into a fresh conda environment\nmamba env create -f environment.yml\n\n# Or, install only the required build dependencies\nmamba install python numpy cython compilers openblas meson-python pkg-config\n\n# To build the latest stable release:\npip install numpy --no-build-isolation --no-binary numpy\n\n# To build a development version, you need a local clone of the NumPy git repository:\ngit clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init\npip install . --no-build-isolation",
        "# To build the latest stable release:\npip install numpy --no-binary numpy\n\n# To build a development version, you need a local clone of the NumPy git repository:\ngit clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init\npip install .",
        "git clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init",
        "mamba env create -f environment.yml\nmamba activate numpy-dev",
        "virtualenvwrapper",
        "pyenv-virtualenv",
        "python -m venv venv\nsource venv/bin/activate",
        "python -m venv venv\nsource venv/bin/activate",
        "python -m venv venv\n.\\venv\\Scripts\\activate",
        "python -m venv venv\n.\\venv\\Scripts\\activate",
        "python -m pip install -r requirements/build_requirements.txt",
        "build-install",
        "spin ipython",
        "spin --help",
        "spin <subcommand> --help",
        "git clean -xdf",
        "# Note: the --no-build-isolation is important!\npip install -e . --no-build-isolation\n\n# To run the tests for, e.g., the `numpy.linalg` module:\npytest numpy/linalg",
        "git clean -xdf",
        "pip uninstall numpy",
        "import numpy"
      ],
      "chunks": [
        {
          "content": "Building from source Building from source Note If you are only trying to install NumPy, we recommend using binaries - see Installation for details on that Building NumPy from source requires setting up system-level dependencies (compilers, BLAS/LAPACK libraries, etc ) first, and then invoking a build The build may be done in order to install NumPy for local usage, develop NumPy itself, or build redistributable binary packages And it may be desired to customize aspects of how the build is done",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "This guide will cover all these aspects In addition, it provides background information on how the NumPy build works, and links to up-to-date guides for generic Python build  packaging documentation that is relevant System-level dependencies NumPy uses compiled code for speed, which means you need compilers and some other system-level (i e, non-Python / non-PyPI) dependencies to build it on your system",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "Note If you are using Conda, you can skip the steps in this section - with the exception of installing compilers for Windows or the Apple Developer Tools for macOS All other dependencies will be installed automatically by the mamba env create -f environment yml command Linux If you want to use the system Python and pip, you will need: C and C++ compilers (typically GCC) Python header files (typically a package named python3-dev or python3-devel) BLAS and LAPACK libraries",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "OpenBLAS is the NumPy default; other variants include Apple Accelerate, MKL, ATLAS and Netlib (or Reference) BLAS and LAPACK pkg-config for dependency detection A Fortran compiler is needed only for running the f2py tests The instructions below include a Fortran compiler, however you can safely leave it out",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "Debian/Ubuntu Linux To install NumPy build requirements, you can do: sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev Alternatively, you can do: sudo apt build-dep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "Fedora To install NumPy build requirements, you can do: sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo dnf builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "CentOS/RHEL To install NumPy build requirements, you can do: sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo yum-builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers Arch To install NumPy build requirements, you can do: sudo pacman -S gcc-fortran openblas pkgconf macOS Install Apple Developer Tools",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "An easy way to do this is to open a terminal window, enter the command: xcode-select --install and follow the prompts Apple Developer Tools includes Git, the Clang C/C++ compilers, and other development utilities that may be required Do not use the macOS system Python Instead, install Python with the python org installer or with a package manager like Homebrew, MacPorts or Fink On macOS =13",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "3, the easiest build option is to use Accelerate, which is already installed and will be automatically used by default On older macOS versions you need a different BLAS library, most likely OpenBLAS, plus pkg-config to detect OpenBLAS These are easiest to install with Homebrew: brew install openblas pkg-config gfortran Windows On Windows, the use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "If you dont need to run the f2py tests, simply using MSVC is easiest Otherwise, you will need one of these sets of compilers: MSVC + Intel Fortran (ifort) Intel compilers (icc, ifort) Mingw-w64 compilers (gcc, g++, gfortran) Compared to macOS and Linux, building NumPy on Windows is a little more difficult, due to the need to set up these compilers It is not possible to just call a one-liner on the command prompt as you would on other platforms",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "First, install Microsoft Visual Studio - the 2019 Community Edition or any newer version will work (see the Visual Studio download site) This is needed even if you use the MinGW-w64 or Intel compilers, in order to ensure you have the Windows Universal C Runtime (the other components of Visual Studio are not needed when using Mingw-w64, and can be deselected if desired, to save disk space) The recommended version of the UCRT is = 10 0 22621 0",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "MSVC The MSVC installer does not put the compilers on the system path, and the install location may change To query the install location, MSVC comes with a vswhere exe command-line utility And to make the C/C++ compilers available inside the shell you are using, you need to run a bat file for the correct bitness and architecture (e g , for 64-bit Intel CPUs, use vcvars64 bat)",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "If using a Conda environment while a version of Visual Studio 2019+ is installed that includes the MSVC v142 package (VS 2019 C++ x86/x64 build tools), activating the conda environment should cause Visual Studio to be found and the appropriate bat file executed to set these variables For detailed guidance, see Use the Microsoft C++ toolset from the command line Intel Similar to MSVC, the Intel compilers are designed to be used with an activation script (Intel\\oneAPI\\setvars",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "bat) that you run in the shell you are using This makes the compilers available on the path For detailed guidance, see Get Started with the Intel oneAPI HPC Toolkit for Windows MinGW-w64 There are several sources of binaries for MinGW-w64 We recommend the RTools versions, which can be installed with Chocolatey (see Chocolatey install instructions here): choco install rtools -y --no-progress --force --version=4 0 0 20220206 Note Compilers should be on the system path (i e",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": ", the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH You can use any shell (e g , Powershell, cmd or Git Bash) to invoke a build To check that this is the case, try invoking a Fortran compiler in the shell you use (e g , gfortran --version or ifort --version)",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "Warning When using a conda environment it is possible that the environment creation will not work due to an outdated Fortran compiler If that happens, remove the compilers entry from environment yml and try again The Fortran compiler should be installed as described in this section Windows on ARM64 In Windows on ARM64, the set of a compiler options that are available for building NumPy are limited Compilers such as GCC and GFortran are not yet supported for Windows on ARM64",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "Currently, the NumPy build for Windows on ARM64 is supported with MSVC and LLVM toolchains The use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together If you dont need to run the f2py tests, simply using MSVC is easiest",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Otherwise, you will need the following set of compilers: MSVC + flang (cl, flang) LLVM + flang (clang-cl, flang) First, install Microsoft Visual Studio - the 2022 Community Edition will work (see the Visual Studio download site) Ensure that you have installed necessary Visual Studio components for building NumPy on WoA from here To use the flang compiler for Windows on ARM64, install Latest LLVM toolchain for WoA from here",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "MSVC The MSVC installer does not put the compilers on the system path, and the install location may change To query the install location, MSVC comes with a vswhere exe command-line utility And to make the C/C++ compilers available inside the shell you are using, you need to run a bat file for the correct bitness and architecture (e g , for ARM64-based CPUs, use vcvarsarm64 bat) For detailed guidance, see Use the Microsoft C++ toolset from the command line",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "LLVM Similar to MSVC, LLVM does not put the compilers on the system path To set system path for LLVM compilers, users may need to use set command to put compilers on the system path To check compilers path for LLVMs clang-cl, try invoking LLVMs clang-cl compiler in the shell you use (clang-cl --version) Note Compilers should be on the system path (i e",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": ", the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH You can use any shell (e g , Powershell, cmd or Git Bash) to invoke a build To check that this is the case, try invoking a Fortran compiler in the shell you use (e g , flang --version)",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "Warning Currently, Conda environment is not yet supported officially on Windows on ARM64 The present approach uses virtualenv for building NumPy from source on Windows on ARM64 Building NumPy from source If you want to only install NumPy from source once and not do any development work, then the recommended way to build and install is to use pip Otherwise, conda is recommended Note If you dont have a conda installation yet, we recommend using Miniforge; any conda flavor will work though",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "Building from source to use NumPy Conda env If you are using a conda environment, pip is still the tool you use to invoke a from-source build of NumPy It is important to always use the --no-build-isolation flag to the pip install command, to avoid building against a numpy wheel from PyPI",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "In order for that to work you must first install the remaining build dependencies into the conda environment:  Either install all NumPy dev dependencies into a fresh conda environment mamba env create -f environment",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "yml  Or, install only the required build dependencies mamba install python numpy cython compilers openblas meson-python pkg-config  To build the latest stable release: pip install numpy --no-build-isolation --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github com/numpy/numpy git cd numpy git submodule update --init pip install",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": "--no-build-isolation Warning On Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the pip install command to fail These variables are only needed for flang and can be safely unset prior to running pip install Virtual env or system Python  To build the latest stable release: pip install numpy --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github com/numpy/numpy",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "git cd numpy git submodule update --init pip install Building from source for NumPy development If you want to build from source in order to work on NumPy itself, first clone the NumPy repository: git clone https://github com/numpy/numpy",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "git cd numpy git submodule update --init Then you want to do the following: Create a dedicated development environment (virtual environment or conda environment), Install all needed dependencies (build, and also test, doc and optional dependencies), Build NumPy with the spin developer interface",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "Step (3) is always the same, steps (1) and (2) are different between conda and virtual environments: Conda env To create a numpy-dev development environment with every required and optional dependency installed, run: mamba env create -f environment yml mamba activate numpy-dev Virtual env or system Python Note There are many tools to manage virtual environments, like venv, virtualenv/virtualenvwrapper, pyenv/pyenv-virtualenv, Poetry, PDM, Hatch, and more",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "Here we use the basic venv tool that is part of the Python stdlib You can use any other tool; all we need is an activated Python environment Create and activate a virtual environment in a new directory named venv ( note that the exact activation command may be different based on your OS and shell - see How venvs work in the venv docs) Linux python -m venv venv source venv/bin/activate macOS python -m venv venv source venv/bin/activate Windows python -m venv venv",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "\\venv\\Scripts\\activate Windows on ARM64 python -m venv venv \\venv\\Scripts\\activate Note Building NumPy with BLAS and LAPACK functions requires OpenBLAS library at Runtime In Windows on ARM64, this can be done by setting up pkg-config for OpenBLAS dependency The build steps for OpenBLAS for Windows on ARM64 can be found here Then install the Python-level dependencies from PyPI with: python -m pip install -r requirements/build_requirements",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "txt To build NumPy in an activated development environment, run: spin build This will install NumPy inside the repository (by default in a build-install directory) You can then run tests (spin test), drop into IPython (spin ipython), or take other development steps like build the html documentation or running benchmarks The spin interface is self-documenting, so please see spin --help and spin subcommand --help for detailed guidance",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "Warning In an activated conda environment on Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the build to fail These variables are only needed for flang and can be safely unset for build IDE support  editable installs While the spin interface is our recommended way of working on NumPy, it has one limitation: because of the custom install location, NumPy installed using spin will not be recognized automatically within an IDE (e g",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": ", for running a script via a run button, or setting breakpoints visually) This will work better with an in-place build (or editable install) Editable installs are supported It is important to understand that you may use either an editable install or ``spin`` in a given repository clone, but not both If you use editable installs, you have to use pytest and other development tools directly instead of using spin",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": "To use an editable install, ensure you start from a clean repository (run git clean -xdf if youve built with spin before) and have all dependencies set up correctly as described higher up on this page Then do:  Note: the --no-build-isolation is important pip install -e --no-build-isolation  To run the tests for, e g , the `numpy linalg` module: pytest numpy/linalg When making changes to NumPy code, including to compiled code, there is no need to manually rebuild or reinstall",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "NumPy is automatically rebuilt each time NumPy is imported by the Python interpreter; see the meson-python documentation on editable installs for more details on how that works under the hood When you run git clean -xdf, which removes the built extension modules, remember to also uninstall NumPy with pip uninstall numpy Warning Note that editable installs are fundamentally incomplete installs",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "Their only guarantee is that import numpy works - so they are suitable for working on NumPy itself, and for working on pure Python packages that depend on NumPy Headers, entrypoints, and other such things may not be available from an editable install",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "Customizing builds Compiler selection and customizing a build BLAS and LAPACK Cross compilation Building redistributable binaries Background information Understanding Meson Introspecting build steps Meson and distutils ways of doing things previous Testing the numpy",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_37"
        },
        {
          "content": "i typemaps next Compiler selection and customizing a build On this page System-level dependencies Building NumPy from source Building from source to use NumPy Building from source for NumPy development Customizing builds Background information",
          "url": "https://numpy.org/doc/stable/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_38"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
      "title": "Contributing to NumPy \u2014 NumPy v2.3 Manual",
      "content": "Contributing to NumPy Contributing to NumPy Not a coder? Not a problem! NumPy is multi-faceted, and we can use a lot of help. These are all activities wed like to get help with (theyre all important, so we list them in alphabetical order): Code maintenance and development Community coordination DevOps Developing educational content  narrative documentation Fundraising Marketing Project management Translating content Website design and development Writing technical documentation We understand that everyone has a different level of experience, also NumPy is a pretty well-established project, so its hard to make assumptions about an ideal first-time-contributor. So, thats why we dont mark issues with the good-first-issue label. Instead, youll find issues labeled Sprintable. These issues can either be: Easily fixed when you have guidance from an experienced contributor (perfect for working in a sprint). A learning opportunity for those ready to dive deeper, even if youre not in a sprint. Additionally, depending on your prior experience, some Sprintable issues might be easy, while others could be more challenging for you. The rest of this document discusses working on the NumPy code base and documentation. Were in the process of updating our descriptions of other activities and roles. If you are interested in these other activities, please contact us! You can do this via the numpy-discussion mailing list, or on GitHub (open an issue or comment on a relevant issue). These are our preferred communication channels (open source is open by nature!), however if you prefer to discuss in a more private space first, you can do so on Slack (see numpy.org/contribute for details). Development process - summary Heres the short summary, complete TOC links are below: If you are a first-time contributor: Go to numpy/numpy and click the fork button to create your own copy of the project. Clone the project to your local computer: git clone --recurse-submodules https://github.com/your-username/numpy.git Change the directory: cd numpy Add the upstream repository: git remote add upstream https://github.com/numpy/numpy.git Now, git remote -v will show two remote repositories named: upstream, which refers to the numpy repository origin, which refers to your personal fork Pull the latest changes from upstream, including tags: git checkout main git pull upstream main --tags Initialize numpys submodules: git submodule update --init Develop your contribution: Create a branch for the feature you want to work on. Since the branch name will appear in the merge message, use a sensible name such as linspace-speedups: git checkout -b linspace-speedups Commit locally as you progress (git add and git commit) Use a properly formatted commit message, write tests that fail before your change and pass afterward, run all the tests locally. Be sure to document any changed behavior in docstrings, keeping to the NumPy docstring standard. To submit your contribution: Push your changes back to your fork on GitHub: git push origin linspace-speedups Go to GitHub. The new branch will show up with a green Pull Request button. Make sure the title and message are clear, concise, and self- explanatory. Then click the button to submit it. If your commit introduces a new feature or changes functionality, post on the mailing list to explain your changes. For bug fixes, documentation updates, etc., this is generally not necessary, though if you do not get any reaction, do feel free to ask for review. Review process: Reviewers (the other developers and interested community members) will write inline and/or general comments on your Pull Request (PR) to help you improve its implementation, documentation and style. Every single developer working on the project has their code reviewed, and weve come to see it as friendly conversation from which we all learn and the overall code quality benefits. Therefore, please dont let the review discourage you from contributing: its only aim is to improve the quality of project, not to criticize (we are, after all, very grateful for the time youre donating!). See our Reviewer Guidelines for more information. To update your PR, make your changes on your local repository, commit, run tests, and only if they succeed push to your fork. As soon as those changes are pushed up (to the same branch as before) the PR will update automatically. If you have no idea how to fix the test failures, you may push your changes anyway and ask for help in a PR comment. Various continuous integration (CI) services are triggered after each PR update to build the code, run unit tests, measure code coverage and check coding style of your branch. The CI tests must pass before your PR can be merged. If CI fails, you can find out why by clicking on the failed icon (red cross) and inspecting the build and test log. To avoid overuse and waste of this resource, test your work locally before committing. A PR must be approved by at least one core team member before merging. Approval means the core team member has carefully reviewed the changes, and the PR is ready for merging. Document changes Beyond changes to a functions docstring and possible description in the general documentation, if your change introduces any user-facing modifications they may need to be mentioned in the release notes. To add your change to the release notes, you need to create a short file with a summary and place it in doc/release/upcoming_changes. The file doc/release/upcoming_changes/README.rst details the format and filename conventions. If your change introduces a deprecation, make sure to discuss this first on GitHub or the mailing list first. If agreement on the deprecation is reached, follow NEP 23 deprecation policy to add the deprecation. Cross referencing issues If the PR relates to any issues, you can add the text xref gh-xxxx where xxxx is the number of the issue to github comments. Likewise, if the PR solves an issue, replace the xref with closes, fixes or any of the other flavors github accepts. In the source code, be sure to preface any issue or PR reference with gh-xxxx. For a more detailed discussion, read on and follow the links at the bottom of this page. Guidelines All code should have tests (see test coverage below for more details). All code should be documented. No changes are ever committed without review and approval by a core team member. Please ask politely on the PR or on the mailing list if you get no response to your pull request within a week. Stylistic guidelines Set up your editor to follow PEP 8 (remove trailing white space, no tabs, etc.). Check code with ruff. Use NumPy data types instead of strings (np.uint8 instead of \"uint8\"). Use the following import conventions: import numpy as np For C code, see NEP 45. Test coverage Pull requests (PRs) that modify code should either have new tests, or modify existing tests to fail before the PR and pass afterwards. You should run the tests before pushing a PR. Running NumPys test suite locally requires some additional packages, such as pytest and hypothesis. The additional testing dependencies are listed in requirements/test_requirements.txt in the top-level directory, and can conveniently be installed with:  python -m pip install -r requirements/test_requirements.txt Tests for a module should ideally cover all code in that module, i.e., statement coverage should be at 100. To measure the test coverage, run:  spin test --coverage This will create a report in html format at build/coverage, which can be viewed with your browser, e.g.:  firefox build/coverage/index.html Building docs To build the HTML documentation, use: spin docs You can also run make from the doc directory. make help lists all targets. To get the appropriate dependencies and other requirements, see Building the NumPy API and reference docs. Development process - details The rest of the story Setting up and using your development environment Recommended development setup Using virtual environments Building from source Testing builds Other build options Running tests Running linting Rebuilding  cleaning the workspace Debugging Understanding the code  getting started Building the NumPy API and reference docs Development environments Prerequisites Instructions Development workflow Basic workflow Additional things you might want to do Advanced debugging tools Finding C errors with additional tooling Using GitHub Codespaces for NumPy development GitHub Codespaces What is a codespace? Forking the NumPy repository Starting GitHub Codespaces Quick workspace tour Development workflow with GitHub Codespaces Rendering the NumPy documentation FAQs and troubleshooting Reviewer guidelines Who can be a reviewer? Communication guidelines Reviewer checklist Standard replies for reviewing NumPy benchmarks Usage Benchmarking versions Writing benchmarks NumPy C style guide For downstream package authors Understanding NumPys versioning and API/ABI stability Testing against the NumPy main branch or pre-releases Adding a dependency on NumPy Releasing a version How to prepare a release Step-by-step directions Branch walkthrough NumPy governance NumPy project governance and decision-making How to contribute to the NumPy documentation Documentation team meetings Whats needed Contributing fixes Contributing new pages Contributing indirectly Documentation style Documentation reading NumPy-specific workflow is in numpy-development-workflow. previous Meson and distutils ways of doing things next Setting up and using your development environment On this page Development process - summary Guidelines Stylistic guidelines Test coverage Building docs Development process - details",
      "code_blocks": [
        "git clone --recurse-submodules https://github.com/your-username/numpy.git",
        "git remote add upstream https://github.com/numpy/numpy.git",
        "git remote -v",
        "git checkout main\ngit pull upstream main --tags",
        "git submodule update --init",
        "git checkout -b linspace-speedups",
        "git push origin linspace-speedups",
        "doc/release/upcoming_changes",
        "doc/release/upcoming_changes/README.rst",
        "xref gh-xxxx",
        "import numpy as np",
        "requirements/test_requirements.txt",
        "$ python -m pip install -r requirements/test_requirements.txt",
        "$ spin test --coverage",
        "build/coverage",
        "$ firefox build/coverage/index.html"
      ],
      "chunks": [
        {
          "content": "Contributing to NumPy Contributing to NumPy Not a coder Not a problem NumPy is multi-faceted, and we can use a lot of help",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "These are all activities wed like to get help with (theyre all important, so we list them in alphabetical order): Code maintenance and development Community coordination DevOps Developing educational content  narrative documentation Fundraising Marketing Project management Translating content Website design and development Writing technical documentation We understand that everyone has a different level of experience, also NumPy is a pretty well-established project, so its hard to make assumptions about an ideal first-time-contributor",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "So, thats why we dont mark issues with the good-first-issue label Instead, youll find issues labeled Sprintable These issues can either be: Easily fixed when you have guidance from an experienced contributor (perfect for working in a sprint) A learning opportunity for those ready to dive deeper, even if youre not in a sprint Additionally, depending on your prior experience, some Sprintable issues might be easy, while others could be more challenging for you",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "The rest of this document discusses working on the NumPy code base and documentation Were in the process of updating our descriptions of other activities and roles If you are interested in these other activities, please contact us You can do this via the numpy-discussion mailing list, or on GitHub (open an issue or comment on a relevant issue) These are our preferred communication channels (open source is open by nature",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "), however if you prefer to discuss in a more private space first, you can do so on Slack (see numpy org/contribute for details) Development process - summary Heres the short summary, complete TOC links are below: If you are a first-time contributor: Go to numpy/numpy and click the fork button to create your own copy of the project Clone the project to your local computer: git clone --recurse-submodules https://github com/your-username/numpy",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "git Change the directory: cd numpy Add the upstream repository: git remote add upstream https://github com/numpy/numpy",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "git Now, git remote -v will show two remote repositories named: upstream, which refers to the numpy repository origin, which refers to your personal fork Pull the latest changes from upstream, including tags: git checkout main git pull upstream main --tags Initialize numpys submodules: git submodule update --init Develop your contribution: Create a branch for the feature you want to work on",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "Since the branch name will appear in the merge message, use a sensible name such as linspace-speedups: git checkout -b linspace-speedups Commit locally as you progress (git add and git commit) Use a properly formatted commit message, write tests that fail before your change and pass afterward, run all the tests locally Be sure to document any changed behavior in docstrings, keeping to the NumPy docstring standard",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "To submit your contribution: Push your changes back to your fork on GitHub: git push origin linspace-speedups Go to GitHub The new branch will show up with a green Pull Request button Make sure the title and message are clear, concise, and self- explanatory Then click the button to submit it If your commit introduces a new feature or changes functionality, post on the mailing list to explain your changes For bug fixes, documentation updates, etc",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": ", this is generally not necessary, though if you do not get any reaction, do feel free to ask for review Review process: Reviewers (the other developers and interested community members) will write inline and/or general comments on your Pull Request (PR) to help you improve its implementation, documentation and style Every single developer working on the project has their code reviewed, and weve come to see it as friendly conversation from which we all learn and the overall code quality benefits",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "Therefore, please dont let the review discourage you from contributing: its only aim is to improve the quality of project, not to criticize (we are, after all, very grateful for the time youre donating ) See our Reviewer Guidelines for more information To update your PR, make your changes on your local repository, commit, run tests, and only if they succeed push to your fork As soon as those changes are pushed up (to the same branch as before) the PR will update automatically",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "If you have no idea how to fix the test failures, you may push your changes anyway and ask for help in a PR comment Various continuous integration (CI) services are triggered after each PR update to build the code, run unit tests, measure code coverage and check coding style of your branch The CI tests must pass before your PR can be merged If CI fails, you can find out why by clicking on the failed icon (red cross) and inspecting the build and test log",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "To avoid overuse and waste of this resource, test your work locally before committing A PR must be approved by at least one core team member before merging Approval means the core team member has carefully reviewed the changes, and the PR is ready for merging Document changes Beyond changes to a functions docstring and possible description in the general documentation, if your change introduces any user-facing modifications they may need to be mentioned in the release notes",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "To add your change to the release notes, you need to create a short file with a summary and place it in doc/release/upcoming_changes The file doc/release/upcoming_changes/README rst details the format and filename conventions If your change introduces a deprecation, make sure to discuss this first on GitHub or the mailing list first If agreement on the deprecation is reached, follow NEP 23 deprecation policy to add the deprecation",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "Cross referencing issues If the PR relates to any issues, you can add the text xref gh-xxxx where xxxx is the number of the issue to github comments Likewise, if the PR solves an issue, replace the xref with closes, fixes or any of the other flavors github accepts In the source code, be sure to preface any issue or PR reference with gh-xxxx For a more detailed discussion, read on and follow the links at the bottom of this page",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "Guidelines All code should have tests (see test coverage below for more details) All code should be documented No changes are ever committed without review and approval by a core team member Please ask politely on the PR or on the mailing list if you get no response to your pull request within a week Stylistic guidelines Set up your editor to follow PEP 8 (remove trailing white space, no tabs, etc ) Check code with ruff Use NumPy data types instead of strings (np uint8 instead of \"uint8\")",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "Use the following import conventions: import numpy as np For C code, see NEP 45 Test coverage Pull requests (PRs) that modify code should either have new tests, or modify existing tests to fail before the PR and pass afterwards You should run the tests before pushing a PR Running NumPys test suite locally requires some additional packages, such as pytest and hypothesis The additional testing dependencies are listed in requirements/test_requirements",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "txt in the top-level directory, and can conveniently be installed with:  python -m pip install -r requirements/test_requirements txt Tests for a module should ideally cover all code in that module, i e , statement coverage should be at 100 To measure the test coverage, run:  spin test --coverage This will create a report in html format at build/coverage, which can be viewed with your browser, e g :  firefox build/coverage/index",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "html Building docs To build the HTML documentation, use: spin docs You can also run make from the doc directory make help lists all targets To get the appropriate dependencies and other requirements, see Building the NumPy API and reference docs",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "Development process - details The rest of the story Setting up and using your development environment Recommended development setup Using virtual environments Building from source Testing builds Other build options Running tests Running linting Rebuilding  cleaning the workspace Debugging Understanding the code  getting started Building the NumPy API and reference docs Development environments Prerequisites Instructions Development workflow Basic workflow Additional things you might want to do Advanced debugging tools Finding C errors with additional tooling Using GitHub Codespaces for NumPy development GitHub Codespaces What is a codespace",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "Forking the NumPy repository Starting GitHub Codespaces Quick workspace tour Development workflow with GitHub Codespaces Rendering the NumPy documentation FAQs and troubleshooting Reviewer guidelines Who can be a reviewer",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "Communication guidelines Reviewer checklist Standard replies for reviewing NumPy benchmarks Usage Benchmarking versions Writing benchmarks NumPy C style guide For downstream package authors Understanding NumPys versioning and API/ABI stability Testing against the NumPy main branch or pre-releases Adding a dependency on NumPy Releasing a version How to prepare a release Step-by-step directions Branch walkthrough NumPy governance NumPy project governance and decision-making How to contribute to the NumPy documentation Documentation team meetings Whats needed Contributing fixes Contributing new pages Contributing indirectly Documentation style Documentation reading NumPy-specific workflow is in numpy-development-workflow",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "previous Meson and distutils ways of doing things next Setting up and using your development environment On this page Development process - summary Guidelines Stylistic guidelines Test coverage Building docs Development process - details",
          "url": "https://numpy.org/doc/stable/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_22"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/release.html#main-content",
      "title": "Release notes \u2014 NumPy v2.3 Manual",
      "content": "NumPy user guide Release notes Release notes 2.3.0 Highlights New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2.2.6 Contributors Pull requests merged 2.2.5 Contributors Pull requests merged 2.2.4 Contributors Pull requests merged 2.2.3 Contributors Pull requests merged 2.2.2 Contributors Pull requests merged 2.2.1 Contributors Pull requests merged 2.2.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 2.1.3 Improvements Changes Contributors Pull requests merged 2.1.2 Contributors Pull requests merged 2.1.1 Contributors Pull requests merged 2.1.0 New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2.0.2 Contributors Pull requests merged 2.0.1 Improvements Contributors Pull requests merged 2.0.0 Highlights NumPy 2.0 Python API removals Deprecations Expired deprecations Compatibility notes C API changes NumPy 2.0 C API removals New Features Improvements Changes 1.26.4 Contributors Pull requests merged 1.26.3 Compatibility Improvements Contributors Pull requests merged 1.26.2 Contributors Pull requests merged 1.26.1 Build system changes New features Contributors Pull requests merged 1.26.0 New Features Improvements Build system changes Contributors Pull requests merged 1.25.2 Contributors Pull requests merged 1.25.1 Contributors Pull requests merged 1.25.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1.24.4 Contributors Pull requests merged 1.24.3 Contributors Pull requests merged 1.24.2 Contributors Pull requests merged 1.24.1 Contributors Pull requests merged 1.24.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1.23.5 Contributors Pull requests merged 1.23.4 Contributors Pull requests merged 1.23.3 Contributors Pull requests merged 1.23.2 Contributors Pull requests merged 1.23.1 Contributors Pull requests merged 1.23.0 New functions Deprecations Expired deprecations New Features Compatibility notes Improvements Performance improvements and changes 1.22.4 Contributors Pull requests merged 1.22.3 Contributors Pull requests merged 1.22.2 Contributors Pull requests merged 1.22.1 Contributors Pull requests merged 1.22.0 Expired deprecations Deprecations Compatibility notes C API changes New Features Improvements 1.21.6 1.21.5 Contributors Pull requests merged 1.21.4 Contributors Pull requests merged 1.21.3 Contributors Pull requests merged 1.21.2 Contributors Pull requests merged 1.21.1 Contributors Pull requests merged 1.21.0 New functions Expired deprecations Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements Changes 1.20.3 Contributors Pull requests merged 1.20.2 Contributors Pull requests merged 1.20.1 Highlights Contributors Pull requests merged 1.20.0 New functions Deprecations Future Changes Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements and changes Changes 1.19.5 Contributors Pull requests merged 1.19.4 Contributors Pull requests merged 1.19.3 Contributors Pull requests merged 1.19.2 Improvements Contributors Pull requests merged 1.19.1 Contributors Pull requests merged 1.19.0 Highlights Expired deprecations Compatibility notes Deprecations C API changes New Features Improvements Improve detection of CPU features Changes 1.18.5 Contributors Pull requests merged 1.18.4 Contributors Pull requests merged 1.18.3 Highlights Contributors Pull requests merged 1.18.2 Contributors Pull requests merged 1.18.1 Contributors Pull requests merged 1.18.0 Highlights New functions Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Changes 1.17.5 Contributors Pull requests merged 1.17.4 Highlights Contributors Pull requests merged 1.17.3 Highlights Compatibility notes Contributors Pull requests merged 1.17.2 Contributors Pull requests merged 1.17.1 Contributors Pull requests merged 1.17.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1.16.6 Highlights New functions Compatibility notes Improvements Contributors Pull requests merged 1.16.5 Contributors Pull requests merged 1.16.4 New deprecations Compatibility notes Changes Contributors Pull requests merged 1.16.3 Compatibility notes Improvements Changes 1.16.2 Compatibility notes Contributors Pull requests merged 1.16.1 Contributors Enhancements Compatibility notes New Features Improvements Changes 1.16.0 Highlights New functions New deprecations Expired deprecations Future changes Compatibility notes C API changes New Features Improvements Changes 1.15.4 Compatibility Note Contributors Pull requests merged 1.15.3 Compatibility Note Contributors Pull requests merged 1.15.2 Compatibility Note Contributors Pull requests merged 1.15.1 Compatibility Note Contributors Pull requests merged 1.15.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements 1.14.6 Contributors Pull requests merged 1.14.5 Contributors Pull requests merged 1.14.4 Contributors Pull requests merged 1.14.3 Contributors Pull requests merged 1.14.2 Contributors Pull requests merged 1.14.1 Contributors Pull requests merged 1.14.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1.13.3 Contributors Pull requests merged 1.13.2 Contributors Pull requests merged 1.13.1 Pull requests merged Contributors 1.13.0 Highlights New functions Deprecations Future Changes Build System Changes Compatibility notes C API changes New Features Improvements Changes 1.12.1 Bugs Fixed 1.12.0 Highlights Dropped Support Added Support Build System Changes Deprecations Future Changes Compatibility notes New Features Improvements Changes 1.11.3 Contributors to maintenance/1.11.3 Pull Requests Merged 1.11.2 Pull Requests Merged 1.11.1 Fixes Merged 1.11.0 Highlights Build System Changes Future Changes Compatibility notes New Features Improvements Changes Deprecations FutureWarnings 1.10.4 Compatibility notes Issues Fixed Merged PRs 1.10.3 1.10.2 Compatibility notes Issues Fixed Merged PRs Notes 1.10.1 1.10.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations 1.9.2 Issues fixed 1.9.1 Issues fixed 1.9.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Deprecations 1.8.2 Issues fixed 1.8.1 Issues fixed Changes Deprecations 1.8.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations Authors 1.7.2 Issues fixed 1.7.1 Issues fixed 1.7.0 Highlights Compatibility notes New features Changes Deprecations 1.6.2 Issues fixed Changes 1.6.1 Issues Fixed 1.6.0 Highlights New features Changes Deprecated features Removed features 1.5.0 Highlights New features Changes 1.4.0 Highlights New features Improvements Deprecations Internal changes 1.3.0 Highlights New features Deprecated features Documentation changes New C API Internal changes previous Glossary next NumPy 2.3.0 Release Notes",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy user guide Release notes Release notes 2 3 0 Highlights New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2 2 6 Contributors Pull requests merged 2 2 5 Contributors Pull requests merged 2 2 4 Contributors Pull requests merged 2 2 3 Contributors Pull requests merged 2 2 2 Contributors Pull requests merged 2 2 1 Contributors Pull requests merged 2 2",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 2 1 3 Improvements Changes Contributors Pull requests merged 2 1 2 Contributors Pull requests merged 2 1 1 Contributors Pull requests merged 2 1 0 New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2 0 2 Contributors Pull requests merged 2 0 1 Improvements Contributors Pull requests merged 2",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "0 0 Highlights NumPy 2 0 Python API removals Deprecations Expired deprecations Compatibility notes C API changes NumPy 2 0 C API removals New Features Improvements Changes 1 26 4 Contributors Pull requests merged 1 26 3 Compatibility Improvements Contributors Pull requests merged 1 26 2 Contributors Pull requests merged 1 26 1 Build system changes New features Contributors Pull requests merged 1 26 0 New Features Improvements Build system changes Contributors Pull requests merged 1 25",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "2 Contributors Pull requests merged 1 25 1 Contributors Pull requests merged 1 25 0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1 24 4 Contributors Pull requests merged 1 24 3 Contributors Pull requests merged 1 24 2 Contributors Pull requests merged 1 24 1 Contributors Pull requests merged 1 24",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1 23 5 Contributors Pull requests merged 1 23 4 Contributors Pull requests merged 1 23 3 Contributors Pull requests merged 1 23 2 Contributors Pull requests merged 1 23 1 Contributors Pull requests merged 1 23 0 New functions Deprecations Expired deprecations New Features Compatibility notes Improvements Performance improvements and changes 1 22",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "4 Contributors Pull requests merged 1 22 3 Contributors Pull requests merged 1 22 2 Contributors Pull requests merged 1 22 1 Contributors Pull requests merged 1 22 0 Expired deprecations Deprecations Compatibility notes C API changes New Features Improvements 1 21 6 1 21 5 Contributors Pull requests merged 1 21 4 Contributors Pull requests merged 1 21 3 Contributors Pull requests merged 1 21 2 Contributors Pull requests merged 1 21 1 Contributors Pull requests merged 1 21",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "0 New functions Expired deprecations Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements Changes 1 20 3 Contributors Pull requests merged 1 20 2 Contributors Pull requests merged 1 20 1 Highlights Contributors Pull requests merged 1 20 0 New functions Deprecations Future Changes Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements and changes Changes 1 19",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "5 Contributors Pull requests merged 1 19 4 Contributors Pull requests merged 1 19 3 Contributors Pull requests merged 1 19 2 Improvements Contributors Pull requests merged 1 19 1 Contributors Pull requests merged 1 19 0 Highlights Expired deprecations Compatibility notes Deprecations C API changes New Features Improvements Improve detection of CPU features Changes 1 18 5 Contributors Pull requests merged 1 18 4 Contributors Pull requests merged 1 18",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "3 Highlights Contributors Pull requests merged 1 18 2 Contributors Pull requests merged 1 18 1 Contributors Pull requests merged 1 18 0 Highlights New functions Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Changes 1 17 5 Contributors Pull requests merged 1 17 4 Highlights Contributors Pull requests merged 1 17 3 Highlights Compatibility notes Contributors Pull requests merged 1 17 2 Contributors Pull requests merged 1 17",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "1 Contributors Pull requests merged 1 17 0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1 16 6 Highlights New functions Compatibility notes Improvements Contributors Pull requests merged 1 16 5 Contributors Pull requests merged 1 16 4 New deprecations Compatibility notes Changes Contributors Pull requests merged 1 16 3 Compatibility notes Improvements Changes 1 16 2 Compatibility notes Contributors Pull requests merged 1",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "16 1 Contributors Enhancements Compatibility notes New Features Improvements Changes 1 16 0 Highlights New functions New deprecations Expired deprecations Future changes Compatibility notes C API changes New Features Improvements Changes 1 15 4 Compatibility Note Contributors Pull requests merged 1 15 3 Compatibility Note Contributors Pull requests merged 1 15 2 Compatibility Note Contributors Pull requests merged 1 15 1 Compatibility Note Contributors Pull requests merged 1 15",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements 1 14 6 Contributors Pull requests merged 1 14 5 Contributors Pull requests merged 1 14 4 Contributors Pull requests merged 1 14 3 Contributors Pull requests merged 1 14 2 Contributors Pull requests merged 1 14 1 Contributors Pull requests merged 1 14 0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1 13",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "3 Contributors Pull requests merged 1 13 2 Contributors Pull requests merged 1 13 1 Pull requests merged Contributors 1 13 0 Highlights New functions Deprecations Future Changes Build System Changes Compatibility notes C API changes New Features Improvements Changes 1 12 1 Bugs Fixed 1 12 0 Highlights Dropped Support Added Support Build System Changes Deprecations Future Changes Compatibility notes New Features Improvements Changes 1 11 3 Contributors to maintenance/1 11 3 Pull Requests Merged 1",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "11 2 Pull Requests Merged 1 11 1 Fixes Merged 1 11 0 Highlights Build System Changes Future Changes Compatibility notes New Features Improvements Changes Deprecations FutureWarnings 1 10 4 Compatibility notes Issues Fixed Merged PRs 1 10 3 1 10 2 Compatibility notes Issues Fixed Merged PRs Notes 1 10 1 1 10 0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations 1 9 2 Issues fixed 1 9 1 Issues fixed 1 9",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Deprecations 1 8 2 Issues fixed 1 8 1 Issues fixed Changes Deprecations 1 8 0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations Authors 1 7 2 Issues fixed 1 7 1 Issues fixed 1 7 0 Highlights Compatibility notes New features Changes Deprecations 1 6 2 Issues fixed Changes 1 6 1 Issues Fixed 1 6",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "0 Highlights New features Changes Deprecated features Removed features 1 5 0 Highlights New features Changes 1 4 0 Highlights New features Improvements Deprecations Internal changes 1 3 0 Highlights New features Deprecated features Documentation changes New C API Internal changes previous Glossary next NumPy 2 3 0 Release Notes",
          "url": "https://numpy.org/doc/stable/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_15"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/#main-content",
      "title": "NumPy tutorials \u2014 NumPy Tutorials",
      "content": "Repository Open issue .md .pdf NumPy tutorials Contents Content Non-executable articles Useful links and resources NumPy tutorials This set of tutorials and educational materials is being developed in the numpy-tutorials repository, and is not a part of the NumPy source tree. The goal of this repository is to provide high-quality resources by the NumPy project, both for self-learning and for teaching classes with. If youre interested in adding your own content, check the Contributing section. To open a live version of the content, click the launch Binder button above. To open each of the .md files, right click and select Open with - Notebook. You can also launch individual tutorials on Binder by clicking on the rocket icon that appears in the upper-right corner of each tutorial. To download a local copy of the .ipynb files, you can either clone this repository or use the download icon in the upper-right corner of each tutorial. Content NumPy Features Linear algebra on n-dimensional arrays Saving and sharing your NumPy arrays Masked Arrays NumPy Applications Determining Moores Law with real data in NumPy Deep learning on MNIST X-ray image processing Determining Static Equilibrium in NumPy Plotting Fractals Analyzing the impact of the lockdown on air quality in Delhi, India Contributing Why Jupyter Notebooks? Adding your own tutorials Non-executable articles Help improve the tutorials! Want to make a valuable contribution to the tutorials? Consider contributing to these existing articles to help make them fully executable and reproducible! Articles Deep reinforcement learning with Pong from pixels Sentiment Analysis on notable speeches of the last decade Useful links and resources The following links may be useful: NumPy Code of Conduct Main NumPy documentation NumPy documentation team meeting notes NEP 44 - Restructuring the NumPy documentation Blog post - Documentation as a way to build Community Note that regular documentation issues for NumPy can be found in the main NumPy repository (see the Documentation labels there). next NumPy Features Contents Content Non-executable articles Useful links and resources By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "Documentation"
      ],
      "chunks": [
        {
          "content": "Repository Open issue md pdf NumPy tutorials Contents Content Non-executable articles Useful links and resources NumPy tutorials This set of tutorials and educational materials is being developed in the numpy-tutorials repository, and is not a part of the NumPy source tree The goal of this repository is to provide high-quality resources by the NumPy project, both for self-learning and for teaching classes with If youre interested in adding your own content, check the Contributing section",
          "url": "https://numpy.org/numpy-tutorials/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "To open a live version of the content, click the launch Binder button above To open each of the md files, right click and select Open with - Notebook You can also launch individual tutorials on Binder by clicking on the rocket icon that appears in the upper-right corner of each tutorial To download a local copy of the ipynb files, you can either clone this repository or use the download icon in the upper-right corner of each tutorial",
          "url": "https://numpy.org/numpy-tutorials/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "Content NumPy Features Linear algebra on n-dimensional arrays Saving and sharing your NumPy arrays Masked Arrays NumPy Applications Determining Moores Law with real data in NumPy Deep learning on MNIST X-ray image processing Determining Static Equilibrium in NumPy Plotting Fractals Analyzing the impact of the lockdown on air quality in Delhi, India Contributing Why Jupyter Notebooks Adding your own tutorials Non-executable articles Help improve the tutorials",
          "url": "https://numpy.org/numpy-tutorials/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "Want to make a valuable contribution to the tutorials Consider contributing to these existing articles to help make them fully executable and reproducible",
          "url": "https://numpy.org/numpy-tutorials/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "Articles Deep reinforcement learning with Pong from pixels Sentiment Analysis on notable speeches of the last decade Useful links and resources The following links may be useful: NumPy Code of Conduct Main NumPy documentation NumPy documentation team meeting notes NEP 44 - Restructuring the NumPy documentation Blog post - Documentation as a way to build Community Note that regular documentation issues for NumPy can be found in the main NumPy repository (see the Documentation labels there)",
          "url": "https://numpy.org/numpy-tutorials/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "next NumPy Features Contents Content Non-executable articles Useful links and resources By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/features.html",
      "title": "NumPy Features \u2014 NumPy Tutorials",
      "content": "Repository Open issue .md .pdf NumPy Features NumPy Features A collection of notebooks pertaining to built-in NumPy functionality. Linear algebra on n-dimensional arrays Saving and sharing your NumPy arrays Masked Arrays previous NumPy tutorials next Linear algebra on n-dimensional arrays By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [],
      "chunks": [
        {
          "content": "Repository Open issue md pdf NumPy Features NumPy Features A collection of notebooks pertaining to built-in NumPy functionality Linear algebra on n-dimensional arrays Saving and sharing your NumPy arrays Masked Arrays previous NumPy tutorials next Linear algebra on n-dimensional arrays By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/features.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
      "title": "Linear algebra on n-dimensional arrays \u2014 NumPy Tutorials",
      "content": "Binder Repository Open issue .ipynb .md .pdf Linear algebra on n-dimensional arrays Contents Prerequisites Learner profile Learning Objectives Content Shape, axis and array properties Operations on an axis Approximation Applying to all colors Products with n-dimensional arrays Final words Further reading Linear algebra on n-dimensional arrays Prerequisites Before reading this tutorial, you should know a bit of Python. If you would like to refresh your memory, take a look at the Python tutorial. If you want to be able to run the examples in this tutorial, you should also have matplotlib and SciPy installed on your computer. Learner profile This tutorial is for people who have a basic understanding of linear algebra and arrays in NumPy and want to understand how n-dimensional (\\(n=2\\)) arrays are represented and can be manipulated. In particular, if you dont know how to apply common functions to n-dimensional arrays (without using for-loops), or if you want to understand axis and shape properties for n-dimensional arrays, this tutorial might be of help. Learning Objectives After this tutorial, you should be able to: Understand the difference between one-, two- and n-dimensional arrays in NumPy; Understand how to apply some linear algebra operations to n-dimensional arrays without using for-loops; Understand axis and shape properties for n-dimensional arrays. Content In this tutorial, we will use a matrix decomposition from linear algebra, the Singular Value Decomposition, to generate a compressed approximation of an image. Well use the face image from the scipy.datasets module:  TODO: Rm try-except with scipy 1.10 is the minimum supported version try: from scipy.datasets import face except ImportError:  Data was in scipy.misc prior to scipy v1.10 from scipy.misc import face img = face() Downloading file 'face.dat' from 'https://raw.githubusercontent.com/scipy/dataset-face/main/face.dat' to '/home/circleci/.cache/scipy-data'. Note: If you prefer, you can use your own image as you work through this tutorial. In order to transform your image into a NumPy array that can be manipulated, you can use the imread function from the matplotlib.pyplot submodule. Alternatively, you can use the imageio.imread function from the imageio library. Be aware that if you use your own image, youll likely need to adapt the steps below. For more information on how images are treated when converted to NumPy arrays, see A crash course on NumPy for images from the scikit-image documentation. Now, img is a NumPy array, as we can see when using the type function: type(img) numpy.ndarray We can see the image using the matplotlib.pyplot.imshow function  the special iPython command, matplotlib inline to display plots inline: import matplotlib.pyplot as plt matplotlib inline plt.imshow(img) plt.show() Shape, axis and array properties Note that, in linear algebra, the dimension of a vector refers to the number of entries in an array. In NumPy, it instead defines the number of axes. For example, a 1D array is a vector such as [1, 2, 3], a 2D array is a matrix, and so forth. First, lets check for the shape of the data in our array. Since this image is two-dimensional (the pixels in the image form a rectangle), we might expect a two-dimensional array to represent it (a matrix). However, using the shape property of this NumPy array gives us a different result: img.shape (768, 1024, 3) The output is a tuple with three elements, which means that this is a three-dimensional array. In fact, since this is a color image, and we have used the imread function to read it, the data is organized in three 2D arrays, representing color channels (in this case, red, green and blue - RGB). You can see this by looking at the shape above: it indicates that we have an array of 3 matrices, each having shape 768x1024. Furthermore, using the ndim property of this array, we can see that img.ndim 3 NumPy refers to each dimension as an axis. Because of how imread works, the first index in the 3rd axis is the red pixel data for our image. We can access this by using the syntax img[:, :, 0] array([[121, 138, 153, ..., 119, 131, 139], [ 89, 110, 130, ..., 118, 134, 146], [ 73, 94, 115, ..., 117, 133, 144], ..., [ 87, 94, 107, ..., 120, 119, 119], [ 85, 95, 112, ..., 121, 120, 120], [ 85, 97, 111, ..., 120, 119, 118]], shape=(768, 1024), dtype=uint8) From the output above, we can see that every value in img[:, :, 0] is an integer value between 0 and 255, representing the level of red in each corresponding image pixel (keep in mind that this might be different if you use your own image instead of scipy.datasets.face). As expected, this is a 768x1024 matrix: img[:, :, 0].shape (768, 1024) Since we are going to perform linear algebra operations on this data, it might be more interesting to have real numbers between 0 and 1 in each entry of the matrices to represent the RGB values. We can do that by setting img_array = img / 255 This operation, dividing an array by a scalar, works because of NumPys broadcasting rules. (Note that in real-world applications, it would be better to use, for example, the img_as_float utility function from scikit-image). You can check that the above works by doing some tests; for example, inquiring about maximum and minimum values for this array: img_array.max(), img_array.min() (np.float64(1.0), np.float64(0.0)) or checking the type of data in the array: img_array.dtype dtype('float64') Note that we can assign each color channel to a separate matrix using the slice syntax: red_array = img_array[:, :, 0] green_array = img_array[:, :, 1] blue_array = img_array[:, :, 2] Operations on an axis It is possible to use methods from linear algebra to approximate an existing set of data. Here, we will use the SVD (Singular Value Decomposition) to try to rebuild an image that uses less singular value information than the original one, while still retaining some of its features. Note: We will use NumPys linear algebra module, numpy.linalg, to perform the operations in this tutorial. Most of the linear algebra functions in this module can also be found in scipy.linalg, and users are encouraged to use the scipy module for real-world applications. However, some functions in the scipy.linalg module, such as the SVD function, only support 2D arrays. For more information on this, check the scipy.linalg page. To proceed, import the linear algebra submodule from NumPy: from numpy import linalg In order to extract information from a given matrix, we can use the SVD to obtain 3 arrays which can be multiplied to obtain the original matrix. From the theory of linear algebra, given a matrix \\(A\\), the following product can be computed: \\[U \\Sigma VT = A\\] where \\(U\\) and \\(VT\\) are square and \\(\\Sigma\\) is the same size as \\(A\\). \\(\\Sigma\\) is a diagonal matrix and contains the singular values of \\(A\\), organized from largest to smallest. These values are always non-negative and can be used as an indicator of the importance of some features represented by the matrix \\(A\\). Lets see how this works in practice with just one matrix first. Note that according to colorimetry, it is possible to obtain a fairly reasonable grayscale version of our color image if we apply the formula \\[Y = 0.2126 R + 0.7152 G + 0.0722 B\\] where \\(Y\\) is the array representing the grayscale image, and \\(R\\), \\(G\\) and \\(B\\) are the red, green and blue channel arrays we had originally. Notice we can use the  operator (the matrix multiplication operator for NumPy arrays, see numpy.matmul) for this: img_gray = img_array  [0.2126, 0.7152, 0.0722] Now, img_gray has shape img_gray.shape (768, 1024) To see if this makes sense in our image, we should use a colormap from matplotlib corresponding to the color we wish to see in out image (otherwise, matplotlib will default to a colormap that does not correspond to the real data). In our case, we are approximating the grayscale portion of the image, so we will use the colormap gray: plt.imshow(img_gray, cmap=\"gray\") plt.show() Now, applying the linalg.svd function to this matrix, we obtain the following decomposition: U, s, Vt = linalg.svd(img_gray) Note If you are using your own image, this command might take a while to run, depending on the size of your image and your hardware. Dont worry, this is normal! The SVD can be a pretty intensive computation. Lets check that this is what we expected: U.shape, s.shape, Vt.shape ((768, 768), (768,), (1024, 1024)) Note that s has a particular shape: it has only one dimension. This means that some linear algebra functions that expect 2d arrays might not work. For example, from the theory, one might expect s and Vt to be compatible for multiplication. However, this is not true as s does not have a second axis. Executing s  Vt results in a ValueError. This happens because having a one-dimensional array for s, in this case, is much more economic in practice than building a diagonal matrix with the same data. To reconstruct the original matrix, we can rebuild the diagonal matrix \\(\\Sigma\\) with the elements of s in its diagonal and with the appropriate dimensions for multiplying: in our case, \\(\\Sigma\\) should be 768x1024 since U is 768x768 and Vt is 1024x1024. In order to add the singular values to the diagonal of Sigma, we will use the fill_diagonal function from NumPy: import numpy as np Sigma = np.zeros((U.shape[1], Vt.shape[0])) np.fill_diagonal(Sigma, s) Now, we want to check if the reconstructed U  Sigma  Vt is close to the original img_gray matrix. Approximation The linalg module includes a norm function, which computes the norm of a vector or matrix represented in a NumPy array. For example, from the SVD explanation above, we would expect the norm of the difference between img_gray and the reconstructed SVD product to be small. As expected, you should see something like linalg.norm(img_gray - U  Sigma  Vt) np.float64(1.436745484142884e-12) (The actual result of this operation might be different depending on your architecture and linear algebra setup. Regardless, you should see a small number.) We could also have used the numpy.allclose function to make sure the reconstructed product is, in fact, close to our original matrix (the difference between the two arrays is small): np.allclose(img_gray, U  Sigma  Vt) True To see if an approximation is reasonable, we can check the values in s: plt.plot(s) plt.show() In the graph, we can see that although we have 768 singular values in s, most of those (after the 150th entry or so) are pretty small. So it might make sense to use only the information related to the first (say, 50) singular values to build a more economical approximation to our image. The idea is to consider all but the first k singular values in Sigma (which are the same as in s) as zeros, keeping U and Vt intact, and computing the product of these matrices as the approximation. For example, if we choose k = 10 we can build the approximation by doing approx = U  Sigma[:, :k]  Vt[:k, :] Note that we had to use only the first k rows of Vt, since all other rows would be multiplied by the zeros corresponding to the singular values we eliminated from this approximation. plt.imshow(approx, cmap=\"gray\") plt.show() Now, you can go ahead and repeat this experiment with other values of k, and each of your experiments should give you a slightly better (or worse) image depending on the value you choose. Applying to all colors Now we want to do the same kind of operation, but to all three colors. Our first instinct might be to repeat the same operation we did above to each color matrix individually. However, NumPys broadcasting takes care of this for us. If our array has more than two dimensions, then the SVD can be applied to all axes at once. However, the linear algebra functions in NumPy expect to see an array of the form (n, M, N), where the first axis n represents the number of MxN matrices in the stack. In our case, img_array.shape (768, 1024, 3) so we need to permutate the axis on this array to get a shape like (3, 768, 1024). Fortunately, the numpy.transpose function can do that for us: np.transpose(x, axes=(i, j, k)) indicates that the axis will be reordered such that the final shape of the transposed array will be reordered according to the indices (i, j, k). Lets see how this goes for our array: img_array_transposed = np.transpose(img_array, (2, 0, 1)) img_array_transposed.shape (3, 768, 1024) Now we are ready to apply the SVD: U, s, Vt = linalg.svd(img_array_transposed) Finally, to obtain the full approximated image, we need to reassemble these matrices into the approximation. Now, note that U.shape, s.shape, Vt.shape ((3, 768, 768), (3, 768), (3, 1024, 1024)) To build the final approximation matrix, we must understand how multiplication across different axes works. Products with n-dimensional arrays If you have worked before with only one- or two-dimensional arrays in NumPy, you might use numpy.dot and numpy.matmul (or the  operator) interchangeably. However, for n-dimensional arrays, they work in very different ways. For more details, check the documentation on numpy.matmul. Now, to build our approximation, we first need to make sure that our singular values are ready for multiplication, so we build our Sigma matrix similarly to what we did before. The Sigma array must have dimensions (3, 768, 1024). In order to add the singular values to the diagonal of Sigma, we will again use the fill_diagonal function, using each of the 3 rows in s as the diagonal for each of the 3 matrices in Sigma: Sigma = np.zeros((3, 768, 1024)) for j in range(3): np.fill_diagonal(Sigma[j, :, :], s[j, :]) Now, if we wish to rebuild the full SVD (with no approximation), we can do reconstructed = U  Sigma  Vt Note that reconstructed.shape (3, 768, 1024) The reconstructed image should be indistinguishable from the original one, except for differences due to floating point errors from the reconstruction. Recall that our original image consisted of floating point values in the range [0., 1.]. The accumulation of floating point error from the reconstruction can result in values slightly outside this original range: reconstructed.min(), reconstructed.max() (np.float64(-7.431555371084642e-15), np.float64(1.000000000000005)) Since imshow expects values in the range, we can use clip to excise the floating point error: reconstructed = np.clip(reconstructed, 0, 1) plt.imshow(np.transpose(reconstructed, (1, 2, 0))) plt.show() In fact, imshow peforms this clipping under-the-hood, so if you skip the first line in the previous code cell, you might see a warning message saying \"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\" Now, to do the approximation, we must choose only the first k singular values for each color channel. This can be done using the following syntax: approx_img = U  Sigma[..., :k]  Vt[..., :k, :] You can see that we have selected only the first k components of the last axis for Sigma (this means that we have used only the first k columns of each of the three matrices in the stack), and that we have selected only the first k components in the second-to-last axis of Vt (this means we have selected only the first k rows from every matrix in the stack Vt and all columns). If you are unfamiliar with the ellipsis syntax, it is a placeholder for other axes. For more details, see the documentation on Indexing. Now, approx_img.shape (3, 768, 1024) which is not the right shape for showing the image. Finally, reordering the axes back to our original shape of (768, 1024, 3), we can see our approximation: plt.imshow(np.transpose(approx_img, (1, 2, 0))) plt.show() Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.13567131472460806..1.079453607915587]. Even though the image is not as sharp, using a small number of k singular values (compared to the original set of 768 values), we can recover many of the distinguishing features from this image. Final words Of course, this is not the best method to approximate an image. However, there is, in fact, a result in linear algebra that says that the approximation we built above is the best we can get to the original matrix in terms of the norm of the difference. For more information, see G. H. Golub and C. F. Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985. Further reading Python tutorial NumPy Reference SciPy Tutorial SciPy Lecture Notes A matlab, R, IDL, NumPy/SciPy dictionary previous NumPy Features next Saving and sharing your NumPy arrays Contents Prerequisites Learner profile Learning Objectives Content Shape, axis and array properties Operations on an axis Approximation Applying to all colors Products with n-dimensional arrays Final words Further reading By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "# TODO: Rm try-except with scipy 1.10 is the minimum supported version\ntry:\n    from scipy.datasets import face\nexcept ImportError:  # Data was in scipy.misc prior to scipy v1.10\n    from scipy.misc import face\n\nimg = face()",
        "Downloading file 'face.dat' from 'https://raw.githubusercontent.com/scipy/dataset-face/main/face.dat' to '/home/circleci/.cache/scipy-data'.",
        "scikit-image",
        "numpy.ndarray",
        "%matplotlib inline",
        "import matplotlib.pyplot as plt\n\n%matplotlib inline",
        "plt.imshow(img)\nplt.show()",
        "(768, 1024, 3)",
        "img[:, :, 0]",
        "array([[121, 138, 153, ..., 119, 131, 139],\n       [ 89, 110, 130, ..., 118, 134, 146],\n       [ 73,  94, 115, ..., 117, 133, 144],\n       ...,\n       [ 87,  94, 107, ..., 120, 119, 119],\n       [ 85,  95, 112, ..., 121, 120, 120],\n       [ 85,  97, 111, ..., 120, 119, 118]],\n      shape=(768, 1024), dtype=uint8)",
        "img[:, :, 0]",
        "img[:, :, 0].shape",
        "(768, 1024)",
        "img_array = img / 255",
        "scikit-image",
        "img_array.max(), img_array.min()",
        "(np.float64(1.0), np.float64(0.0))",
        "img_array.dtype",
        "dtype('float64')",
        "red_array = img_array[:, :, 0]\ngreen_array = img_array[:, :, 1]\nblue_array = img_array[:, :, 2]",
        "from numpy import linalg",
        "img_gray = img_array @ [0.2126, 0.7152, 0.0722]",
        "img_gray.shape",
        "(768, 1024)",
        "plt.imshow(img_gray, cmap=\"gray\")\nplt.show()",
        "U, s, Vt = linalg.svd(img_gray)",
        "U.shape, s.shape, Vt.shape",
        "((768, 768), (768,), (1024, 1024))",
        "import numpy as np\n\nSigma = np.zeros((U.shape[1], Vt.shape[0]))\nnp.fill_diagonal(Sigma, s)",
        "U @ Sigma @ Vt",
        "linalg.norm(img_gray - U @ Sigma @ Vt)",
        "np.float64(1.436745484142884e-12)",
        "np.allclose(img_gray, U @ Sigma @ Vt)",
        "plt.plot(s)\nplt.show()",
        "approx = U @ Sigma[:, :k] @ Vt[:k, :]",
        "plt.imshow(approx, cmap=\"gray\")\nplt.show()",
        "img_array.shape",
        "(768, 1024, 3)",
        "(3, 768, 1024)",
        "np.transpose(x, axes=(i, j, k))",
        "img_array_transposed = np.transpose(img_array, (2, 0, 1))\nimg_array_transposed.shape",
        "(3, 768, 1024)",
        "U, s, Vt = linalg.svd(img_array_transposed)",
        "U.shape, s.shape, Vt.shape",
        "((3, 768, 768), (3, 768), (3, 1024, 1024))",
        "(3, 768, 1024)",
        "Sigma = np.zeros((3, 768, 1024))\nfor j in range(3):\n    np.fill_diagonal(Sigma[j, :, :], s[j, :])",
        "reconstructed = U @ Sigma @ Vt",
        "reconstructed.shape",
        "(3, 768, 1024)",
        "reconstructed.min(), reconstructed.max()",
        "(np.float64(-7.431555371084642e-15), np.float64(1.000000000000005))",
        "reconstructed = np.clip(reconstructed, 0, 1)\nplt.imshow(np.transpose(reconstructed, (1, 2, 0)))\nplt.show()",
        "\"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\"",
        "approx_img = U @ Sigma[..., :k] @ Vt[..., :k, :]",
        "approx_img.shape",
        "(3, 768, 1024)",
        "(768, 1024, 3)",
        "plt.imshow(np.transpose(approx_img, (1, 2, 0)))\nplt.show()",
        "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.13567131472460806..1.079453607915587]."
      ],
      "chunks": [
        {
          "content": "Binder Repository Open issue ipynb md pdf Linear algebra on n-dimensional arrays Contents Prerequisites Learner profile Learning Objectives Content Shape, axis and array properties Operations on an axis Approximation Applying to all colors Products with n-dimensional arrays Final words Further reading Linear algebra on n-dimensional arrays Prerequisites Before reading this tutorial, you should know a bit of Python If you would like to refresh your memory, take a look at the Python tutorial",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "If you want to be able to run the examples in this tutorial, you should also have matplotlib and SciPy installed on your computer Learner profile This tutorial is for people who have a basic understanding of linear algebra and arrays in NumPy and want to understand how n-dimensional (\\(n=2\\)) arrays are represented and can be manipulated",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "In particular, if you dont know how to apply common functions to n-dimensional arrays (without using for-loops), or if you want to understand axis and shape properties for n-dimensional arrays, this tutorial might be of help",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "Learning Objectives After this tutorial, you should be able to: Understand the difference between one-, two- and n-dimensional arrays in NumPy; Understand how to apply some linear algebra operations to n-dimensional arrays without using for-loops; Understand axis and shape properties for n-dimensional arrays Content In this tutorial, we will use a matrix decomposition from linear algebra, the Singular Value Decomposition, to generate a compressed approximation of an image",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "Well use the face image from the scipy datasets module:  TODO: Rm try-except with scipy 1 10 is the minimum supported version try: from scipy datasets import face except ImportError:  Data was in scipy misc prior to scipy v1 10 from scipy misc import face img = face() Downloading file 'face dat' from 'https://raw githubusercontent com/scipy/dataset-face/main/face dat' to '/home/circleci/ cache/scipy-data' Note: If you prefer, you can use your own image as you work through this tutorial",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "In order to transform your image into a NumPy array that can be manipulated, you can use the imread function from the matplotlib pyplot submodule Alternatively, you can use the imageio imread function from the imageio library Be aware that if you use your own image, youll likely need to adapt the steps below For more information on how images are treated when converted to NumPy arrays, see A crash course on NumPy for images from the scikit-image documentation",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "Now, img is a NumPy array, as we can see when using the type function: type(img) numpy ndarray We can see the image using the matplotlib pyplot imshow function  the special iPython command, matplotlib inline to display plots inline: import matplotlib pyplot as plt matplotlib inline plt imshow(img) plt show() Shape, axis and array properties Note that, in linear algebra, the dimension of a vector refers to the number of entries in an array In NumPy, it instead defines the number of axes",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "For example, a 1D array is a vector such as [1, 2, 3], a 2D array is a matrix, and so forth First, lets check for the shape of the data in our array Since this image is two-dimensional (the pixels in the image form a rectangle), we might expect a two-dimensional array to represent it (a matrix) However, using the shape property of this NumPy array gives us a different result: img shape (768, 1024, 3) The output is a tuple with three elements, which means that this is a three-dimensional array",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "In fact, since this is a color image, and we have used the imread function to read it, the data is organized in three 2D arrays, representing color channels (in this case, red, green and blue - RGB) You can see this by looking at the shape above: it indicates that we have an array of 3 matrices, each having shape 768x1024 Furthermore, using the ndim property of this array, we can see that img ndim 3 NumPy refers to each dimension as an axis",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "Because of how imread works, the first index in the 3rd axis is the red pixel data for our image We can access this by using the syntax img[:, :, 0] array([[121, 138, 153, , 119, 131, 139], [ 89, 110, 130, , 118, 134, 146], [ 73, 94, 115, , 117, 133, 144], , [ 87, 94, 107, , 120, 119, 119], [ 85, 95, 112, , 121, 120, 120], [ 85, 97, 111,",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": ", 120, 119, 118]], shape=(768, 1024), dtype=uint8) From the output above, we can see that every value in img[:, :, 0] is an integer value between 0 and 255, representing the level of red in each corresponding image pixel (keep in mind that this might be different if you use your own image instead of scipy datasets face) As expected, this is a 768x1024 matrix: img[:, :, 0]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "shape (768, 1024) Since we are going to perform linear algebra operations on this data, it might be more interesting to have real numbers between 0 and 1 in each entry of the matrices to represent the RGB values We can do that by setting img_array = img / 255 This operation, dividing an array by a scalar, works because of NumPys broadcasting rules (Note that in real-world applications, it would be better to use, for example, the img_as_float utility function from scikit-image)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "You can check that the above works by doing some tests; for example, inquiring about maximum and minimum values for this array: img_array max(), img_array min() (np float64(1 0), np float64(0 0)) or checking the type of data in the array: img_array",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "dtype dtype('float64') Note that we can assign each color channel to a separate matrix using the slice syntax: red_array = img_array[:, :, 0] green_array = img_array[:, :, 1] blue_array = img_array[:, :, 2] Operations on an axis It is possible to use methods from linear algebra to approximate an existing set of data",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "Here, we will use the SVD (Singular Value Decomposition) to try to rebuild an image that uses less singular value information than the original one, while still retaining some of its features Note: We will use NumPys linear algebra module, numpy linalg, to perform the operations in this tutorial Most of the linear algebra functions in this module can also be found in scipy linalg, and users are encouraged to use the scipy module for real-world applications However, some functions in the scipy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "linalg module, such as the SVD function, only support 2D arrays For more information on this, check the scipy linalg page To proceed, import the linear algebra submodule from NumPy: from numpy import linalg In order to extract information from a given matrix, we can use the SVD to obtain 3 arrays which can be multiplied to obtain the original matrix",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "From the theory of linear algebra, given a matrix \\(A\\), the following product can be computed: \\[U \\Sigma VT = A\\] where \\(U\\) and \\(VT\\) are square and \\(\\Sigma\\) is the same size as \\(A\\) \\(\\Sigma\\) is a diagonal matrix and contains the singular values of \\(A\\), organized from largest to smallest These values are always non-negative and can be used as an indicator of the importance of some features represented by the matrix \\(A\\) Lets see how this works in practice with just one matrix first",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Note that according to colorimetry, it is possible to obtain a fairly reasonable grayscale version of our color image if we apply the formula \\[Y = 0 2126 R + 0 7152 G + 0 0722 B\\] where \\(Y\\) is the array representing the grayscale image, and \\(R\\), \\(G\\) and \\(B\\) are the red, green and blue channel arrays we had originally Notice we can use the  operator (the matrix multiplication operator for NumPy arrays, see numpy matmul) for this: img_gray = img_array  [0 2126, 0 7152, 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "0722] Now, img_gray has shape img_gray shape (768, 1024) To see if this makes sense in our image, we should use a colormap from matplotlib corresponding to the color we wish to see in out image (otherwise, matplotlib will default to a colormap that does not correspond to the real data) In our case, we are approximating the grayscale portion of the image, so we will use the colormap gray: plt imshow(img_gray, cmap=\"gray\") plt show() Now, applying the linalg",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "svd function to this matrix, we obtain the following decomposition: U, s, Vt = linalg svd(img_gray) Note If you are using your own image, this command might take a while to run, depending on the size of your image and your hardware Dont worry, this is normal The SVD can be a pretty intensive computation Lets check that this is what we expected: U shape, s shape, Vt shape ((768, 768), (768,), (1024, 1024)) Note that s has a particular shape: it has only one dimension",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "This means that some linear algebra functions that expect 2d arrays might not work For example, from the theory, one might expect s and Vt to be compatible for multiplication However, this is not true as s does not have a second axis Executing s  Vt results in a ValueError This happens because having a one-dimensional array for s, in this case, is much more economic in practice than building a diagonal matrix with the same data",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "To reconstruct the original matrix, we can rebuild the diagonal matrix \\(\\Sigma\\) with the elements of s in its diagonal and with the appropriate dimensions for multiplying: in our case, \\(\\Sigma\\) should be 768x1024 since U is 768x768 and Vt is 1024x1024 In order to add the singular values to the diagonal of Sigma, we will use the fill_diagonal function from NumPy: import numpy as np Sigma = np zeros((U shape[1], Vt shape[0])) np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "fill_diagonal(Sigma, s) Now, we want to check if the reconstructed U  Sigma  Vt is close to the original img_gray matrix Approximation The linalg module includes a norm function, which computes the norm of a vector or matrix represented in a NumPy array For example, from the SVD explanation above, we would expect the norm of the difference between img_gray and the reconstructed SVD product to be small As expected, you should see something like linalg norm(img_gray - U  Sigma  Vt) np float64(1",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "436745484142884e-12) (The actual result of this operation might be different depending on your architecture and linear algebra setup Regardless, you should see a small number ) We could also have used the numpy allclose function to make sure the reconstructed product is, in fact, close to our original matrix (the difference between the two arrays is small): np allclose(img_gray, U  Sigma  Vt) True To see if an approximation is reasonable, we can check the values in s: plt plot(s) plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "show() In the graph, we can see that although we have 768 singular values in s, most of those (after the 150th entry or so) are pretty small So it might make sense to use only the information related to the first (say, 50) singular values to build a more economical approximation to our image The idea is to consider all but the first k singular values in Sigma (which are the same as in s) as zeros, keeping U and Vt intact, and computing the product of these matrices as the approximation",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": "For example, if we choose k = 10 we can build the approximation by doing approx = U  Sigma[:, :k]  Vt[:k, :] Note that we had to use only the first k rows of Vt, since all other rows would be multiplied by the zeros corresponding to the singular values we eliminated from this approximation plt imshow(approx, cmap=\"gray\") plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "show() Now, you can go ahead and repeat this experiment with other values of k, and each of your experiments should give you a slightly better (or worse) image depending on the value you choose Applying to all colors Now we want to do the same kind of operation, but to all three colors Our first instinct might be to repeat the same operation we did above to each color matrix individually However, NumPys broadcasting takes care of this for us",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "If our array has more than two dimensions, then the SVD can be applied to all axes at once However, the linear algebra functions in NumPy expect to see an array of the form (n, M, N), where the first axis n represents the number of MxN matrices in the stack In our case, img_array shape (768, 1024, 3) so we need to permutate the axis on this array to get a shape like (3, 768, 1024) Fortunately, the numpy transpose function can do that for us: np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "transpose(x, axes=(i, j, k)) indicates that the axis will be reordered such that the final shape of the transposed array will be reordered according to the indices (i, j, k) Lets see how this goes for our array: img_array_transposed = np transpose(img_array, (2, 0, 1)) img_array_transposed shape (3, 768, 1024) Now we are ready to apply the SVD: U, s, Vt = linalg svd(img_array_transposed) Finally, to obtain the full approximated image, we need to reassemble these matrices into the approximation",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "Now, note that U shape, s shape, Vt shape ((3, 768, 768), (3, 768), (3, 1024, 1024)) To build the final approximation matrix, we must understand how multiplication across different axes works Products with n-dimensional arrays If you have worked before with only one- or two-dimensional arrays in NumPy, you might use numpy dot and numpy matmul (or the  operator) interchangeably However, for n-dimensional arrays, they work in very different ways For more details, check the documentation on numpy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "matmul Now, to build our approximation, we first need to make sure that our singular values are ready for multiplication, so we build our Sigma matrix similarly to what we did before The Sigma array must have dimensions (3, 768, 1024) In order to add the singular values to the diagonal of Sigma, we will again use the fill_diagonal function, using each of the 3 rows in s as the diagonal for each of the 3 matrices in Sigma: Sigma = np zeros((3, 768, 1024)) for j in range(3): np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "fill_diagonal(Sigma[j, :, :], s[j, :]) Now, if we wish to rebuild the full SVD (with no approximation), we can do reconstructed = U  Sigma  Vt Note that reconstructed shape (3, 768, 1024) The reconstructed image should be indistinguishable from the original one, except for differences due to floating point errors from the reconstruction Recall that our original image consisted of floating point values in the range [0 , 1 ]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "The accumulation of floating point error from the reconstruction can result in values slightly outside this original range: reconstructed min(), reconstructed max() (np float64(-7 431555371084642e-15), np float64(1 000000000000005)) Since imshow expects values in the range, we can use clip to excise the floating point error: reconstructed = np clip(reconstructed, 0, 1) plt imshow(np transpose(reconstructed, (1, 2, 0))) plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": "show() In fact, imshow peforms this clipping under-the-hood, so if you skip the first line in the previous code cell, you might see a warning message saying \"Clipping input data to the valid range for imshow with RGB data ([0 1] for floats or [0 255] for integers) \" Now, to do the approximation, we must choose only the first k singular values for each color channel This can be done using the following syntax: approx_img = U  Sigma[ , :k]  Vt[",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": ", :k, :] You can see that we have selected only the first k components of the last axis for Sigma (this means that we have used only the first k columns of each of the three matrices in the stack), and that we have selected only the first k components in the second-to-last axis of Vt (this means we have selected only the first k rows from every matrix in the stack Vt and all columns) If you are unfamiliar with the ellipsis syntax, it is a placeholder for other axes",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "For more details, see the documentation on Indexing Now, approx_img shape (3, 768, 1024) which is not the right shape for showing the image Finally, reordering the axes back to our original shape of (768, 1024, 3), we can see our approximation: plt imshow(np transpose(approx_img, (1, 2, 0))) plt show() Clipping input data to the valid range for imshow with RGB data ([0 1] for floats or [0 255] for integers) Got range [-0 13567131472460806 1 079453607915587]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "Even though the image is not as sharp, using a small number of k singular values (compared to the original set of 768 values), we can recover many of the distinguishing features from this image Final words Of course, this is not the best method to approximate an image However, there is, in fact, a result in linear algebra that says that the approximation we built above is the best we can get to the original matrix in terms of the norm of the difference For more information, see G H Golub and C F",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_37"
        },
        {
          "content": "Further reading Python tutorial NumPy Reference SciPy Tutorial SciPy Lecture Notes A matlab, R, IDL, NumPy/SciPy dictionary previous NumPy Features next Saving and sharing your NumPy arrays Contents Prerequisites Learner profile Learning Objectives Content Shape, axis and array properties Operations on an axis Approximation Applying to all colors Products with n-dimensional arrays Final words Further reading By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html",
          "library": "numpy",
          "chunk_id": "numpy_38"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
      "title": "Saving and sharing your NumPy arrays \u2014 NumPy Tutorials",
      "content": "Binder Repository Open issue .ipynb .md .pdf Saving and sharing your NumPy arrays Contents What youll learn What youll do What youll need Create your arrays Save your arrays with NumPys savez Remove the saved arrays and load them back with NumPys load Reassign the NpzFile arrays to x and y Success Another option: saving to human-readable csv Rearrange the data into a single 2D array Save the data to csv file using savetxt Our arrays as a csv file Success, but remember your types Wrapping up Saving and sharing your NumPy arrays What youll learn Youll save your NumPy arrays as zipped files and human-readable comma-delimited files i.e. *.csv. You will also learn to load both of these file types back into NumPy workspaces. What youll do Youll learn two ways of saving and reading filesas compressed and as text filesthat will serve most of your storage needs in NumPy. Youll create two 1D arrays and one 2D array Youll save these arrays to files Youll remove variables from your workspace Youll load the variables from your saved file Youll compare zipped binary files to human-readable delimited files Youll finish with the skills of saving, loading, and sharing NumPy arrays What youll need NumPy read-write access to your working directory Load the necessary functions using the following command. import numpy as np In this tutorial, you will use the following Python, IPython magic, and NumPy functions: np.arange np.savez del whos np.load np.block np.newaxis np.savetxt np.loadtxt Create your arrays Now that you have imported the NumPy library, you can make a couple of arrays; lets start with two 1D arrays, x and y, where y = x**2.You will assign x to the integers from 0 to 9 using np.arange. x = np.arange(10) y = x ** 2 print(x) print(y) [0 1 2 3 4 5 6 7 8 9] [ 0 1 4 9 16 25 36 49 64 81] Save your arrays with NumPys savez Now you have two arrays in your workspace, x: [0 1 2 3 4 5 6 7 8 9] y: [ 0 1 4 9 16 25 36 49 64 81] The first thing you will do is save them to a file as zipped arrays using savez. You will use two options to label the arrays in the file, x_axis = x: this option is assigning the name x_axis to the variable x y_axis = y: this option is assigning the name y_axis to the variable y np.savez(\"x_y-squared.npz\", x_axis=x, y_axis=y) Remove the saved arrays and load them back with NumPys load In your current working directory, you should have a new file with the name x_y-squared.npz. This file is a zipped binary of the two arrays, x and y. Lets clear the workspace and load the values back in. This x_y-squared.npz file contains two NPY format files. The NPY format is a native binary format. You cannot read the numbers in a standard text editor or spreadsheet. remove x and y from the workspaec with del load the arrays into the workspace in a dictionary with np.load To see what variables are in the workspace, use the Jupyter/IPython magic command whos. del x, y whos Variable Type Data/Info ------------------------------ np module module 'numpy' from '/ho...kages/numpy/__init__.py' load_xy = np.load(\"x_y-squared.npz\") print(load_xy.files) ['x_axis', 'y_axis'] whos Variable Type Data/Info ------------------------------- load_xy NpzFile NpzFile 'x_y-squared.npz'...with keys: x_axis, y_axis np module module 'numpy' from '/ho...kages/numpy/__init__.py' Reassign the NpzFile arrays to x and y Youve now created the dictionary with an NpzFile-type. The included files are x_axis and y_axis that you defined in your savez command. You can reassign x and y to the load_xy files. x = load_xy[\"x_axis\"] y = load_xy[\"y_axis\"] print(x) print(y) [0 1 2 3 4 5 6 7 8 9] [ 0 1 4 9 16 25 36 49 64 81] Success You have created, saved, deleted, and loaded the variables x and y using savez and load. Nice work. Another option: saving to human-readable csv Lets consider another scenario, you want to share x and y with other people or other programs. You may need human-readable text file that is easier to share. Next, you use the savetxt to save x and y in a comma separated value file, x_y-squared.csv. The resulting csv is composed of ASCII characters. You can load the file back into NumPy or read it with other programs. Rearrange the data into a single 2D array First, you have to create a single 2D array from your two 1D arrays. The csv-filetype is a spreadsheet-style dataset. The csv arranges numbers in rowsseparated by new linesand columnsseparated by commas. If the data is more complex e.g. multiple 2D arrays or higher dimensional arrays, it is better to use savez. Here, you use two NumPy functions to format the data: np.block: this function appends arrays together into a 2D array np.newaxis: this function forces the 1D array into a 2D column vector with 10 rows and 1 column. array_out = np.block([x[:, np.newaxis], y[:, np.newaxis]]) print(\"the output array has shape \", array_out.shape, \" with values:\") print(array_out) the output array has shape (10, 2) with values: [[ 0 0] [ 1 1] [ 2 4] [ 3 9] [ 4 16] [ 5 25] [ 6 36] [ 7 49] [ 8 64] [ 9 81]] Save the data to csv file using savetxt You use savetxt with a three options to make your file easier to read: X = array_out: this option tells savetxt to save your 2D array, array_out, to the file x_y-squared.csv header = 'x, y': this option writes a header before any data that labels the columns of the csv delimiter = ',': this option tells savetxt to place a comma between each column in the file np.savetxt(\"x_y-squared.csv\", X=array_out, header=\"x, y\", delimiter=\",\") Open the file, x_y-squared.csv, and youll see the following: !head x_y-squared.csv  x, y 0.000000000000000000e+00,0.000000000000000000e+00 1.000000000000000000e+00,1.000000000000000000e+00 2.000000000000000000e+00,4.000000000000000000e+00 3.000000000000000000e+00,9.000000000000000000e+00 4.000000000000000000e+00,1.600000000000000000e+01 5.000000000000000000e+00,2.500000000000000000e+01 6.000000000000000000e+00,3.600000000000000000e+01 7.000000000000000000e+00,4.900000000000000000e+01 8.000000000000000000e+00,6.400000000000000000e+01 Our arrays as a csv file There are two features that you shoud notice here: NumPy uses  to ignore headings when using loadtxt. If youre using loadtxt with other csv files, you can skip header rows with skiprows = number_of_header_lines. The integers were written in scientific notation. You can specify the format of the text using the savetxt option, fmt = , but it will still be written with ASCII characters. In general, you cannot preserve the type of ASCII numbers as float or int. Now, delete x and y again and assign them to your columns in x-y_squared.csv. del x, y load_xy = np.loadtxt(\"x_y-squared.csv\", delimiter=\",\") load_xy.shape (10, 2) x = load_xy[:, 0] y = load_xy[:, 1] print(x) print(y) [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.] [ 0. 1. 4. 9. 16. 25. 36. 49. 64. 81.] Success, but remember your types When you saved the arrays to the csv file, you did not preserve the int type. When loading the arrays back into your workspace the default process will be to load the csv file as a 2D floating point array e.g. load_xy.dtype == 'float64' and load_xy.shape == (10, 2). Wrapping up In conclusion, you can create, save, and load arrays in NumPy. Saving arrays makes sharing your work and collaboration much easier. There are other ways Python can save data to files, such as pickle, but savez and savetxt will serve most of your storage needs for future NumPy work and sharing with other people, respectively. Next steps: you can import data with missing values from Importing with genfromtext or learn more about general NumPy IO with Reading and Writing Files. previous Linear algebra on n-dimensional arrays next Masked Arrays Contents What youll learn What youll do What youll need Create your arrays Save your arrays with NumPys savez Remove the saved arrays and load them back with NumPys load Reassign the NpzFile arrays to x and y Success Another option: saving to human-readable csv Rearrange the data into a single 2D array Save the data to csv file using savetxt Our arrays as a csv file Success, but remember your types Wrapping up By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "import numpy as np",
        "x = np.arange(10)\ny = x ** 2\nprint(x)\nprint(y)",
        "[0 1 2 3 4 5 6 7 8 9]\n[ 0  1  4  9 16 25 36 49 64 81]",
        "x: [0 1 2 3 4 5 6 7 8 9]",
        "y: [ 0\u00a0 1\u00a0 4\u00a0 9 16 25 36 49 64 81]",
        "np.savez(\"x_y-squared.npz\", x_axis=x, y_axis=y)",
        "x_y-squared.npz",
        "x_y-squared.npz",
        "Variable   Type      Data/Info\n------------------------------\nnp         module    <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>",
        "load_xy = np.load(\"x_y-squared.npz\")\n\nprint(load_xy.files)",
        "['x_axis', 'y_axis']",
        "Variable   Type       Data/Info\n-------------------------------\nload_xy    NpzFile    NpzFile 'x_y-squared.npz'<...>with keys: x_axis, y_axis\nnp         module     <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>",
        "x = load_xy[\"x_axis\"]\ny = load_xy[\"y_axis\"]\nprint(x)\nprint(y)",
        "[0 1 2 3 4 5 6 7 8 9]\n[ 0  1  4  9 16 25 36 49 64 81]",
        "x_y-squared.csv",
        "array_out = np.block([x[:, np.newaxis], y[:, np.newaxis]])\nprint(\"the output array has shape \", array_out.shape, \" with values:\")\nprint(array_out)",
        "the output array has shape  (10, 2)  with values:\n[[ 0  0]\n [ 1  1]\n [ 2  4]\n [ 3  9]\n [ 4 16]\n [ 5 25]\n [ 6 36]\n [ 7 49]\n [ 8 64]\n [ 9 81]]",
        "X = array_out",
        "x_y-squared.csv",
        "header = 'x, y'",
        "delimiter = ','",
        "np.savetxt(\"x_y-squared.csv\", X=array_out, header=\"x, y\", delimiter=\",\")",
        "x_y-squared.csv",
        "!head x_y-squared.csv",
        "# x, y\n0.000000000000000000e+00,0.000000000000000000e+00\n1.000000000000000000e+00,1.000000000000000000e+00\n2.000000000000000000e+00,4.000000000000000000e+00\n3.000000000000000000e+00,9.000000000000000000e+00\n4.000000000000000000e+00,1.600000000000000000e+01\n5.000000000000000000e+00,2.500000000000000000e+01\n6.000000000000000000e+00,3.600000000000000000e+01\n7.000000000000000000e+00,4.900000000000000000e+01\n8.000000000000000000e+00,6.400000000000000000e+01",
        "skiprows = <number_of_header_lines>",
        "x-y_squared.csv",
        "load_xy = np.loadtxt(\"x_y-squared.csv\", delimiter=\",\")",
        "load_xy.shape",
        "x = load_xy[:, 0]\ny = load_xy[:, 1]\nprint(x)\nprint(y)",
        "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n[ 0.  1.  4.  9. 16. 25. 36. 49. 64. 81.]",
        "load_xy.dtype == 'float64'",
        "load_xy.shape == (10, 2)"
      ],
      "chunks": [
        {
          "content": "Binder Repository Open issue ipynb md",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "pdf Saving and sharing your NumPy arrays Contents What youll learn What youll do What youll need Create your arrays Save your arrays with NumPys savez Remove the saved arrays and load them back with NumPys load Reassign the NpzFile arrays to x and y Success Another option: saving to human-readable csv Rearrange the data into a single 2D array Save the data to csv file using savetxt Our arrays as a csv file Success, but remember your types Wrapping up Saving and sharing your NumPy arrays What youll learn Youll save your NumPy arrays as zipped files and human-readable comma-delimited files i",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "e * csv You will also learn to load both of these file types back into NumPy workspaces What youll do Youll learn two ways of saving and reading filesas compressed and as text filesthat will serve most of your storage needs in NumPy",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "Youll create two 1D arrays and one 2D array Youll save these arrays to files Youll remove variables from your workspace Youll load the variables from your saved file Youll compare zipped binary files to human-readable delimited files Youll finish with the skills of saving, loading, and sharing NumPy arrays What youll need NumPy read-write access to your working directory Load the necessary functions using the following command",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "import numpy as np In this tutorial, you will use the following Python, IPython magic, and NumPy functions: np arange np savez del whos np load np block np newaxis np savetxt np loadtxt Create your arrays Now that you have imported the NumPy library, you can make a couple of arrays; lets start with two 1D arrays, x and y, where y = x**2 You will assign x to the integers from 0 to 9 using np arange x = np",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "arange(10) y = x ** 2 print(x) print(y) [0 1 2 3 4 5 6 7 8 9] [ 0 1 4 9 16 25 36 49 64 81] Save your arrays with NumPys savez Now you have two arrays in your workspace, x: [0 1 2 3 4 5 6 7 8 9] y: [ 0 1 4 9 16 25 36 49 64 81] The first thing you will do is save them to a file as zipped arrays using savez",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "You will use two options to label the arrays in the file, x_axis = x: this option is assigning the name x_axis to the variable x y_axis = y: this option is assigning the name y_axis to the variable y np savez(\"x_y-squared npz\", x_axis=x, y_axis=y) Remove the saved arrays and load them back with NumPys load In your current working directory, you should have a new file with the name x_y-squared npz This file is a zipped binary of the two arrays, x and y",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "Lets clear the workspace and load the values back in This x_y-squared npz file contains two NPY format files The NPY format is a native binary format You cannot read the numbers in a standard text editor or spreadsheet remove x and y from the workspaec with del load the arrays into the workspace in a dictionary with np load To see what variables are in the workspace, use the Jupyter/IPython magic command whos",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "del x, y whos Variable Type Data/Info ------------------------------ np module module 'numpy' from '/ho kages/numpy/__init__ py' load_xy = np load(\"x_y-squared npz\") print(load_xy files) ['x_axis', 'y_axis'] whos Variable Type Data/Info ------------------------------- load_xy NpzFile NpzFile 'x_y-squared npz' with keys: x_axis, y_axis np module module 'numpy' from '/ho kages/numpy/__init__ py' Reassign the NpzFile arrays to x and y Youve now created the dictionary with an NpzFile-type",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "The included files are x_axis and y_axis that you defined in your savez command You can reassign x and y to the load_xy files x = load_xy[\"x_axis\"] y = load_xy[\"y_axis\"] print(x) print(y) [0 1 2 3 4 5 6 7 8 9] [ 0 1 4 9 16 25 36 49 64 81] Success You have created, saved, deleted, and loaded the variables x and y using savez and load Nice work Another option: saving to human-readable csv Lets consider another scenario, you want to share x and y with other people or other programs",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "You may need human-readable text file that is easier to share Next, you use the savetxt to save x and y in a comma separated value file, x_y-squared csv The resulting csv is composed of ASCII characters You can load the file back into NumPy or read it with other programs Rearrange the data into a single 2D array First, you have to create a single 2D array from your two 1D arrays The csv-filetype is a spreadsheet-style dataset",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "The csv arranges numbers in rowsseparated by new linesand columnsseparated by commas If the data is more complex e g multiple 2D arrays or higher dimensional arrays, it is better to use savez Here, you use two NumPy functions to format the data: np block: this function appends arrays together into a 2D array np newaxis: this function forces the 1D array into a 2D column vector with 10 rows and 1 column array_out = np block([x[:, np newaxis], y[:, np",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "newaxis]]) print(\"the output array has shape \", array_out shape, \" with values:\") print(array_out) the output array has shape (10, 2) with values: [[ 0 0] [ 1 1] [ 2 4] [ 3 9] [ 4 16] [ 5 25] [ 6 36] [ 7 49] [ 8 64] [ 9 81]] Save the data to csv file using savetxt You use savetxt with a three options to make your file easier to read: X = array_out: this option tells savetxt to save your 2D array, array_out, to the file x_y-squared",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "csv header = 'x, y': this option writes a header before any data that labels the columns of the csv delimiter = ',': this option tells savetxt to place a comma between each column in the file np savetxt(\"x_y-squared csv\", X=array_out, header=\"x, y\", delimiter=\",\") Open the file, x_y-squared csv, and youll see the following: head x_y-squared csv  x, y 0 000000000000000000e+00,0 000000000000000000e+00 1 000000000000000000e+00,1 000000000000000000e+00 2 000000000000000000e+00,4",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "000000000000000000e+00 3 000000000000000000e+00,9 000000000000000000e+00 4 000000000000000000e+00,1 600000000000000000e+01 5 000000000000000000e+00,2 500000000000000000e+01 6 000000000000000000e+00,3 600000000000000000e+01 7 000000000000000000e+00,4 900000000000000000e+01 8 000000000000000000e+00,6 400000000000000000e+01 Our arrays as a csv file There are two features that you shoud notice here: NumPy uses  to ignore headings when using loadtxt",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "If youre using loadtxt with other csv files, you can skip header rows with skiprows = number_of_header_lines The integers were written in scientific notation You can specify the format of the text using the savetxt option, fmt = , but it will still be written with ASCII characters In general, you cannot preserve the type of ASCII numbers as float or int Now, delete x and y again and assign them to your columns in x-y_squared csv del x, y load_xy = np loadtxt(\"x_y-squared",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "csv\", delimiter=\",\") load_xy shape (10, 2) x = load_xy[:, 0] y = load_xy[:, 1] print(x) print(y) [0 1 2 3 4 5 6 7 8 9 ] [ 0 1 4 9 16 25 36 49 64 81 ] Success, but remember your types When you saved the arrays to the csv file, you did not preserve the int type When loading the arrays back into your workspace the default process will be to load the csv file as a 2D floating point array e g load_xy dtype == 'float64' and load_xy shape == (10, 2)",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Wrapping up In conclusion, you can create, save, and load arrays in NumPy Saving arrays makes sharing your work and collaboration much easier There are other ways Python can save data to files, such as pickle, but savez and savetxt will serve most of your storage needs for future NumPy work and sharing with other people, respectively Next steps: you can import data with missing values from Importing with genfromtext or learn more about general NumPy IO with Reading and Writing Files",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "previous Linear algebra on n-dimensional arrays next Masked Arrays Contents What youll learn What youll do What youll need Create your arrays Save your arrays with NumPys savez Remove the saved arrays and load them back with NumPys load Reassign the NpzFile arrays to x and y Success Another option: saving to human-readable csv Rearrange the data into a single 2D array Save the data to csv file using savetxt Our arrays as a csv file Success, but remember your types Wrapping up By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html",
          "library": "numpy",
          "chunk_id": "numpy_18"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
      "title": "Masked Arrays \u2014 NumPy Tutorials",
      "content": "Binder Repository Open issue .ipynb .md .pdf Masked Arrays Contents What youll do What youll learn What youll need What are masked arrays? When can they be useful? Using masked arrays to see COVID-19 data Exploring the data Missing data Fitting Data In practice Further reading Reference Masked Arrays What youll do Use the masked arrays module from NumPy to analyze COVID-19 data and deal with missing values. What youll learn Youll understand what are masked arrays and how they can be created Youll see how to access and modify data for masked arrays Youll be able to decide when the use of masked arrays is appropriate in some of your applications What youll need Basic familiarity with Python. If you would like to refresh your memory, take a look at the Python tutorial. Basic familiarity with NumPy To run the plots on your computer, you need matplotlib. What are masked arrays? Consider the following problem. You have a dataset with missing or invalid entries. If youre doing any kind of processing on this data, and want to skip or flag these unwanted entries without just deleting them, you may have to use conditionals or filter your data somehow. The numpy.ma module provides some of the same functionality of NumPy ndarrays with added structure to ensure invalid entries are not used in computation. From the Reference Guide: A masked array is the combination of a standard numpy.ndarray and a mask. A mask is either nomask, indicating that no value of the associated array is invalid, or an array of booleans that determines for each element of the associated array whether the value is valid or not. When an element of the mask is False, the corresponding element of the associated array is valid and is said to be unmasked. When an element of the mask is True, the corresponding element of the associated array is said to be masked (invalid). We can think of a MaskedArray as a combination of: Data, as a regular numpy.ndarray of any shape or datatype; A boolean mask with the same shape as the data; A fill_value, a value that may be used to replace the invalid entries in order to return a standard numpy.ndarray. When can they be useful? There are a few situations where masked arrays can be more useful than just eliminating the invalid entries of an array: When you want to preserve the values you masked for later processing, without copying the array; When you have to handle many arrays, each with their own mask. If the mask is part of the array, you avoid bugs and the code is possibly more compact; When you have different flags for missing or invalid values, and wish to preserve these flags without replacing them in the original dataset, but exclude them from computations; If you cant avoid or eliminate missing values, but dont want to deal with NaN (Not a Number) values in your operations. Masked arrays are also a good idea since the numpy.ma module also comes with a specific implementation of most NumPy universal functions (ufuncs), which means that you can still apply fast vectorized functions and operations on masked data. The output is then a masked array. Well see some examples of how this works in practice below. Using masked arrays to see COVID-19 data From Kaggle it is possible to download a dataset with initial data about the COVID-19 outbreak in the beginning of 2020. We are going to look at a small subset of this data, contained in the file who_covid_19_sit_rep_time_series.csv. (Note that this file has been replaced with a version without missing data sometime in late 2020.) import numpy as np import os  The os.getcwd() function returns the current folder; you can change  the filepath variable to point to the folder where you saved the .csv file filepath = os.getcwd() filename = os.path.join(filepath, \"who_covid_19_sit_rep_time_series.csv\") The data file contains data of different types and is organized as follows: The first row is a header line that (mostly) describes the data in each column that follow in the rows below, and beginning in the fourth column, the header is the date of the observation. The second through seventh row contain summary data that is of a different type than that which we are going to examine, so we will need to exclude that from the data with which we will work. The numerical data we wish to work with begins at column 4, row 8, and extends from there to the rightmost column and the lowermost row. Lets explore the data inside this file for the first 14 days of records. To gather data from the .csv file, we will use the numpy.genfromtxt function, making sure we select only the columns with actual numbers instead of the first four columns which contain location data. We also skip the first 6 rows of this file, since they contain other data we are not interested in. Separately, we will extract the information about dates and location for this data.  Note we are using skip_header and usecols to read only portions of the  data file into each variable.  Read just the dates for columns 4-18 from the first row dates = np.genfromtxt( filename, dtype=np.str_, delimiter=\",\", max_rows=1, usecols=range(4, 18), encoding=\"utf-8-sig\", )  Read the names of the geographic locations from the first two  columns, skipping the first six rows locations = np.genfromtxt( filename, dtype=np.str_, delimiter=\",\", skip_header=6, usecols=(0, 1), encoding=\"utf-8-sig\", )  Read the numeric data from just the first 14 days nbcases = np.genfromtxt( filename, dtype=np.int_, delimiter=\",\", skip_header=6, usecols=range(4, 18), encoding=\"utf-8-sig\", ) Included in the numpy.genfromtxt function call, we have selected the numpy.dtype for each subset of the data (either an integer - numpy.int_ - or a string of characters - numpy.str_). We have also used the encoding argument to select utf-8-sig as the encoding for the file (read more about encoding in the official Python documentation. You can read more about the numpy.genfromtxt function from the Reference Documentation or from the Basic IO tutorial. Exploring the data First of all, we can plot the whole set of data we have and see what it looks like. In order to get a readable plot, we select only a few of the dates to show in our x-axis ticks. Note also that in our plot command, we use nbcases.T (the transpose of the nbcases array) since this means we will plot each row of the file as a separate line. We choose to plot a dashed line (using the '--' line style). See the matplotlib documentation for more info on this. import matplotlib.pyplot as plt selected_dates = [0, 3, 11, 13] plt.plot(dates, nbcases.T, \"--\") plt.xticks(selected_dates, dates[selected_dates]) plt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\") Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020') The graph has a strange shape from January 24th to February 1st. It would be interesting to know where this data comes from. If we look at the locations array we extracted from the .csv file, we can see that we have two columns, where the first would contain regions and the second would contain the name of the country. However, only the first few rows contain data for the the first column (province names in China). Following that, we only have country names. So it would make sense to group all the data from China into a single row. For this, well select from the nbcases array only the rows for which the second entry of the locations array corresponds to China. Next, well use the numpy.sum function to sum all the selected rows (axis=0). Note also that row 35 corresponds to the total counts for the whole country for each date. Since we want to calculate the sum ourselves from the provinces data, we have to remove that row first from both locations and nbcases: totals_row = 35 locations = np.delete(locations, (totals_row), axis=0) nbcases = np.delete(nbcases, (totals_row), axis=0) china_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0) china_total array([ 247, 288, 556, 817, -22, -22, -15, -10, -9, -7, -4, 11820, 14410, 17237]) Somethings wrong with this data - we are not supposed to have negative values in a cumulative data set. Whats going on? Missing data Looking at the data, heres what we find: there is a period with missing data: nbcases array([[ 258, 270, 375, ..., 7153, 9074, 11177], [ 14, 17, 26, ..., 520, 604, 683], [ -1, 1, 1, ..., 422, 493, 566], ..., [ -1, -1, -1, ..., -1, -1, -1], [ -1, -1, -1, ..., -1, -1, -1], [ -1, -1, -1, ..., -1, -1, -1]], shape=(263, 14)) All the -1 values we are seeing come from numpy.genfromtxt attempting to read missing data from the original .csv file. Obviously, we dont want to compute missing data as -1 - we just want to skip this value so it doesnt interfere in our analysis. After importing the numpy.ma module, well create a new array, this time masking the invalid values: from numpy import ma nbcases_ma = ma.masked_values(nbcases, -1) If we look at the nbcases_ma masked array, this is what we have: nbcases_ma masked_array( data=[[258, 270, 375, ..., 7153, 9074, 11177], [14, 17, 26, ..., 520, 604, 683], [--, 1, 1, ..., 422, 493, 566], ..., [--, --, --, ..., --, --, --], [--, --, --, ..., --, --, --], [--, --, --, ..., --, --, --]], mask=[[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [ True, False, False, ..., False, False, False], ..., [ True, True, True, ..., True, True, True], [ True, True, True, ..., True, True, True], [ True, True, True, ..., True, True, True]], fill_value=-1) We can see that this is a different kind of array. As mentioned in the introduction, it has three attributes (data, mask and fill_value). Keep in mind that the mask attribute has a True value for elements corresponding to invalid data (represented by two dashes in the data attribute). Lets try and see what the data looks like excluding the first row (data from the Hubei province in China) so we can look at the missing data more closely: plt.plot(dates, nbcases_ma[1:].T, \"--\") plt.xticks(selected_dates, dates[selected_dates]) plt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\") Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020') Now that our data has been masked, lets try summing up all the cases in China: china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0) china_masked masked_array(data=[278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821, 14411, 17238], mask=[False, False, False, False, False, False, False, False, False, False, False, False, False, False], fill_value=999999) Note that china_masked is a masked array, so it has a different data structure than a regular NumPy array. Now, we can access its data directly by using the .data attribute: china_total = china_masked.data china_total array([ 278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821, 14411, 17238]) That is better: no more negative values. However, we can still see that for some days, the cumulative number of cases seems to go down (from 835 to 10, for example), which does not agree with the definition of cumulative data. If we look more closely at the data, we can see that in the period where there was missing data in mainland China, there was valid data for Hong Kong, Taiwan, Macau and Unspecified regions of China. Maybe we can remove those from the total sum of cases in China, to get a better understanding of the data. First, well identify the indices of locations in mainland China: china_mask = ( (locations[:, 1] == \"China\")  (locations[:, 0] != \"Hong Kong\")  (locations[:, 0] != \"Taiwan\")  (locations[:, 0] != \"Macau\")  (locations[:, 0] != \"Unspecified*\") ) Now, china_mask is an array of boolean values (True or False); we can check that the indices are what we wanted with the ma.nonzero method for masked arrays: china_mask.nonzero() (array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 31, 33]),) Now we can correctly sum entries for mainland China: china_total = nbcases_ma[china_mask].sum(axis=0) china_total masked_array(data=[278, 308, 440, 446, --, --, --, --, --, --, --, 11791, 14380, 17205], mask=[False, False, False, False, True, True, True, True, True, True, True, False, False, False], fill_value=999999) We can replace the data with this information and plot a new graph, focusing on Mainland China: plt.plot(dates, china_total.T, \"--\") plt.xticks(selected_dates, dates[selected_dates]) plt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\") Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China') Its clear that masked arrays are the right solution here. We cannot represent the missing data without mischaracterizing the evolution of the curve. Fitting Data One possibility we can think of is to interpolate the missing data to estimate the number of cases in late January. Observe that we can select the masked elements using the .mask attribute: china_total.mask invalid = china_total[china_total.mask] invalid masked_array(data=[--, --, --, --, --, --, --], mask=[ True, True, True, True, True, True, True], fill_value=999999, dtype=int64) We can also access the valid entries by using the logical negation for this mask: valid = china_total[china_total.mask] valid masked_array(data=[278, 308, 440, 446, 11791, 14380, 17205], mask=[False, False, False, False, False, False, False], fill_value=999999) Now, if we want to create a very simple approximation for this data, we should take into account the valid entries around the invalid ones. So first lets select the dates for which the data is valid. Note that we can use the mask from the china_total masked array to index the dates array: dates[china_total.mask] array(['1/21/20', '1/22/20', '1/23/20', '1/24/20', '2/1/20', '2/2/20', '2/3/20'], dtype='U7') Finally, we can use the fitting functionality of the numpy.polynomial package to create a cubic polynomial model that fits the data as best as possible: t = np.arange(len(china_total)) model = np.polynomial.Polynomial.fit(t[china_total.mask], valid, deg=3) plt.plot(t, china_total) plt.plot(t, model(t), \"--\") [matplotlib.lines.Line2D at 0x7f6048898bb0] This plot is not so readable since the lines seem to be over each other, so lets summarize in a more elaborate plot. Well plot the real data when available, and show the cubic fit for unavailable data, using this fit to compute an estimate to the observed number of cases on January 28th 2020, 7 days after the beginning of the records: plt.plot(t, china_total) plt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\") plt.plot(7, model(7), \"r*\") plt.xticks([0, 7, 13], dates[[0, 7, 13]]) plt.yticks([0, model(7), 10000, 17500]) plt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"]) plt.title( \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\" \"Cubic estimate for 7 days after start\" ) Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\nCubic estimate for 7 days after start') In practice Adding -1 to missing data is not a problem with numpy.genfromtxt; in this particular case, substituting the missing value with 0 might have been fine, but well see later that this is far from a general solution. Also, it is possible to call the numpy.genfromtxt function using the usemask parameter. If usemask=True, numpy.genfromtxt automatically returns a masked array. Further reading Topics not covered in this tutorial can be found in the documentation: Hardmasks vs. softmasks The numpy.ma module Reference Ensheng Dong, Hongru Du, Lauren Gardner, An interactive web-based dashboard to track COVID-19 in real time, The Lancet Infectious Diseases, Volume 20, Issue 5, 2020, Pages 533-534, ISSN 1473-3099, https://doi.org/10.1016/S1473-3099(20)30120-1. previous Saving and sharing your NumPy arrays next NumPy Applications Contents What youll do What youll learn What youll need What are masked arrays? When can they be useful? Using masked arrays to see COVID-19 data Exploring the data Missing data Fitting Data In practice Further reading Reference By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "numpy.ndarray",
        "numpy.ndarray",
        "who_covid_19_sit_rep_time_series.csv",
        "import numpy as np\nimport os\n\n# The os.getcwd() function returns the current folder; you can change\n# the filepath variable to point to the folder where you saved the .csv file\nfilepath = os.getcwd()\nfilename = os.path.join(filepath, \"who_covid_19_sit_rep_time_series.csv\")",
        "# Note we are using skip_header and usecols to read only portions of the\n# data file into each variable.\n# Read just the dates for columns 4-18 from the first row\ndates = np.genfromtxt(\n    filename,\n    dtype=np.str_,\n    delimiter=\",\",\n    max_rows=1,\n    usecols=range(4, 18),\n    encoding=\"utf-8-sig\",\n)\n# Read the names of the geographic locations from the first two\n# columns, skipping the first six rows\nlocations = np.genfromtxt(\n    filename,\n    dtype=np.str_,\n    delimiter=\",\",\n    skip_header=6,\n    usecols=(0, 1),\n    encoding=\"utf-8-sig\",\n)\n# Read the numeric data from just the first 14 days\nnbcases = np.genfromtxt(\n    filename,\n    dtype=np.int_,\n    delimiter=\",\",\n    skip_header=6,\n    usecols=range(4, 18),\n    encoding=\"utf-8-sig\",\n)",
        "numpy.genfromtxt",
        "numpy.genfromtxt",
        "import matplotlib.pyplot as plt\n\nselected_dates = [0, 3, 11, 13]\nplt.plot(dates, nbcases.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020')",
        "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
        "array([  247,   288,   556,   817,   -22,   -22,   -15,   -10,    -9,\n          -7,    -4, 11820, 14410, 17237])",
        "array([[  258,   270,   375, ...,  7153,  9074, 11177],\n       [   14,    17,    26, ...,   520,   604,   683],\n       [   -1,     1,     1, ...,   422,   493,   566],\n       ...,\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1],\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1],\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1]], shape=(263, 14))",
        "numpy.genfromtxt",
        "from numpy import ma\n\nnbcases_ma = ma.masked_values(nbcases, -1)",
        "masked_array(\n  data=[[258, 270, 375, ..., 7153, 9074, 11177],\n        [14, 17, 26, ..., 520, 604, 683],\n        [--, 1, 1, ..., 422, 493, 566],\n        ...,\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --]],\n  mask=[[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [ True, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n  fill_value=-1)",
        "plt.plot(dates, nbcases_ma[1:].T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020')",
        "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
        "masked_array(data=[278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821,\n                   14411, 17238],\n             mask=[False, False, False, False, False, False, False, False,\n                   False, False, False, False, False, False],\n       fill_value=999999)",
        "china_masked",
        "china_total = china_masked.data\nchina_total",
        "array([  278,   309,   574,   835,    10,    10,    17,    22,    23,\n          25,    28, 11821, 14411, 17238])",
        "china_mask = (\n    (locations[:, 1] == \"China\")\n    & (locations[:, 0] != \"Hong Kong\")\n    & (locations[:, 0] != \"Taiwan\")\n    & (locations[:, 0] != \"Macau\")\n    & (locations[:, 0] != \"Unspecified*\")\n)",
        "china_mask.nonzero()",
        "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 31, 33]),)",
        "china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
        "masked_array(data=[278, 308, 440, 446, --, --, --, --, --, --, --, 11791,\n                   14380, 17205],\n             mask=[False, False, False, False,  True,  True,  True,  True,\n                    True,  True,  True, False, False, False],\n       fill_value=999999)",
        "plt.plot(dates, china_total.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\")",
        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China')",
        "china_total.mask\ninvalid = china_total[china_total.mask]\ninvalid",
        "masked_array(data=[--, --, --, --, --, --, --],\n             mask=[ True,  True,  True,  True,  True,  True,  True],\n       fill_value=999999,\n            dtype=int64)",
        "valid = china_total[~china_total.mask]\nvalid",
        "masked_array(data=[278, 308, 440, 446, 11791, 14380, 17205],\n             mask=[False, False, False, False, False, False, False],\n       fill_value=999999)",
        "china_total",
        "dates[~china_total.mask]",
        "array(['1/21/20', '1/22/20', '1/23/20', '1/24/20', '2/1/20', '2/2/20',\n       '2/3/20'], dtype='<U7')",
        "t = np.arange(len(china_total))\nmodel = np.polynomial.Polynomial.fit(t[~china_total.mask], valid, deg=3)\nplt.plot(t, china_total)\nplt.plot(t, model(t), \"--\")",
        "[<matplotlib.lines.Line2D at 0x7f6048898bb0>]",
        "plt.plot(t, china_total)\nplt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\")\nplt.plot(7, model(7), \"r*\")\nplt.xticks([0, 7, 13], dates[[0, 7, 13]])\nplt.yticks([0, model(7), 10000, 17500])\nplt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"])\nplt.title(\n    \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\"\n    \"Cubic estimate for 7 days after start\"\n)",
        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\nCubic estimate for 7 days after start')",
        "numpy.genfromtxt",
        "numpy.genfromtxt",
        "usemask=True",
        "numpy.genfromtxt"
      ],
      "chunks": [
        {
          "content": "Binder Repository Open issue ipynb md pdf Masked Arrays Contents What youll do What youll learn What youll need What are masked arrays When can they be useful Using masked arrays to see COVID-19 data Exploring the data Missing data Fitting Data In practice Further reading Reference Masked Arrays What youll do Use the masked arrays module from NumPy to analyze COVID-19 data and deal with missing values",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "What youll learn Youll understand what are masked arrays and how they can be created Youll see how to access and modify data for masked arrays Youll be able to decide when the use of masked arrays is appropriate in some of your applications What youll need Basic familiarity with Python If you would like to refresh your memory, take a look at the Python tutorial Basic familiarity with NumPy To run the plots on your computer, you need matplotlib What are masked arrays Consider the following problem",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "You have a dataset with missing or invalid entries If youre doing any kind of processing on this data, and want to skip or flag these unwanted entries without just deleting them, you may have to use conditionals or filter your data somehow The numpy ma module provides some of the same functionality of NumPy ndarrays with added structure to ensure invalid entries are not used in computation From the Reference Guide: A masked array is the combination of a standard numpy ndarray and a mask",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "A mask is either nomask, indicating that no value of the associated array is invalid, or an array of booleans that determines for each element of the associated array whether the value is valid or not When an element of the mask is False, the corresponding element of the associated array is valid and is said to be unmasked When an element of the mask is True, the corresponding element of the associated array is said to be masked (invalid)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "We can think of a MaskedArray as a combination of: Data, as a regular numpy ndarray of any shape or datatype; A boolean mask with the same shape as the data; A fill_value, a value that may be used to replace the invalid entries in order to return a standard numpy ndarray When can they be useful",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "There are a few situations where masked arrays can be more useful than just eliminating the invalid entries of an array: When you want to preserve the values you masked for later processing, without copying the array; When you have to handle many arrays, each with their own mask",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "If the mask is part of the array, you avoid bugs and the code is possibly more compact; When you have different flags for missing or invalid values, and wish to preserve these flags without replacing them in the original dataset, but exclude them from computations; If you cant avoid or eliminate missing values, but dont want to deal with NaN (Not a Number) values in your operations Masked arrays are also a good idea since the numpy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "ma module also comes with a specific implementation of most NumPy universal functions (ufuncs), which means that you can still apply fast vectorized functions and operations on masked data The output is then a masked array Well see some examples of how this works in practice below Using masked arrays to see COVID-19 data From Kaggle it is possible to download a dataset with initial data about the COVID-19 outbreak in the beginning of 2020",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "We are going to look at a small subset of this data, contained in the file who_covid_19_sit_rep_time_series csv (Note that this file has been replaced with a version without missing data sometime in late 2020 ) import numpy as np import os  The os getcwd() function returns the current folder; you can change  the filepath variable to point to the folder where you saved the csv file filepath = os getcwd() filename = os path join(filepath, \"who_covid_19_sit_rep_time_series",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "csv\") The data file contains data of different types and is organized as follows: The first row is a header line that (mostly) describes the data in each column that follow in the rows below, and beginning in the fourth column, the header is the date of the observation The second through seventh row contain summary data that is of a different type than that which we are going to examine, so we will need to exclude that from the data with which we will work",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "The numerical data we wish to work with begins at column 4, row 8, and extends from there to the rightmost column and the lowermost row Lets explore the data inside this file for the first 14 days of records To gather data from the csv file, we will use the numpy genfromtxt function, making sure we select only the columns with actual numbers instead of the first four columns which contain location data",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "We also skip the first 6 rows of this file, since they contain other data we are not interested in Separately, we will extract the information about dates and location for this data Note we are using skip_header and usecols to read only portions of the  data file into each variable Read just the dates for columns 4-18 from the first row dates = np genfromtxt( filename, dtype=np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "str_, delimiter=\",\", max_rows=1, usecols=range(4, 18), encoding=\"utf-8-sig\", )  Read the names of the geographic locations from the first two  columns, skipping the first six rows locations = np genfromtxt( filename, dtype=np str_, delimiter=\",\", skip_header=6, usecols=(0, 1), encoding=\"utf-8-sig\", )  Read the numeric data from just the first 14 days nbcases = np genfromtxt( filename, dtype=np int_, delimiter=\",\", skip_header=6, usecols=range(4, 18), encoding=\"utf-8-sig\", ) Included in the numpy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "genfromtxt function call, we have selected the numpy dtype for each subset of the data (either an integer - numpy int_ - or a string of characters - numpy str_) We have also used the encoding argument to select utf-8-sig as the encoding for the file (read more about encoding in the official Python documentation You can read more about the numpy genfromtxt function from the Reference Documentation or from the Basic IO tutorial",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "Exploring the data First of all, we can plot the whole set of data we have and see what it looks like In order to get a readable plot, we select only a few of the dates to show in our x-axis ticks Note also that in our plot command, we use nbcases T (the transpose of the nbcases array) since this means we will plot each row of the file as a separate line We choose to plot a dashed line (using the '--' line style) See the matplotlib documentation for more info on this import matplotlib",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "pyplot as plt selected_dates = [0, 3, 11, 13] plt plot(dates, nbcases T, \"--\") plt xticks(selected_dates, dates[selected_dates]) plt title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\") Text(0 5, 1 0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020') The graph has a strange shape from January 24th to February 1st It would be interesting to know where this data comes from If we look at the locations array we extracted from the",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "csv file, we can see that we have two columns, where the first would contain regions and the second would contain the name of the country However, only the first few rows contain data for the the first column (province names in China) Following that, we only have country names So it would make sense to group all the data from China into a single row For this, well select from the nbcases array only the rows for which the second entry of the locations array corresponds to China",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Next, well use the numpy sum function to sum all the selected rows (axis=0) Note also that row 35 corresponds to the total counts for the whole country for each date Since we want to calculate the sum ourselves from the provinces data, we have to remove that row first from both locations and nbcases: totals_row = 35 locations = np delete(locations, (totals_row), axis=0) nbcases = np delete(nbcases, (totals_row), axis=0) china_total = nbcases[locations[:, 1] == \"China\"]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "sum(axis=0) china_total array([ 247, 288, 556, 817, -22, -22, -15, -10, -9, -7, -4, 11820, 14410, 17237]) Somethings wrong with this data - we are not supposed to have negative values in a cumulative data set Whats going on Missing data Looking at the data, heres what we find: there is a period with missing data: nbcases array([[ 258, 270, 375, , 7153, 9074, 11177], [ 14, 17, 26, , 520, 604, 683], [ -1, 1, 1, , 422, 493, 566], , [ -1, -1, -1, , -1, -1, -1], [ -1, -1, -1,",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": ", -1, -1, -1], [ -1, -1, -1, , -1, -1, -1]], shape=(263, 14)) All the -1 values we are seeing come from numpy genfromtxt attempting to read missing data from the original csv file Obviously, we dont want to compute missing data as -1 - we just want to skip this value so it doesnt interfere in our analysis After importing the numpy ma module, well create a new array, this time masking the invalid values: from numpy import ma nbcases_ma = ma",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "masked_values(nbcases, -1) If we look at the nbcases_ma masked array, this is what we have: nbcases_ma masked_array( data=[[258, 270, 375, , 7153, 9074, 11177], [14, 17, 26, , 520, 604, 683], [--, 1, 1, , 422, 493, 566], , [--, --, --, , --, --, --], [--, --, --, , --, --, --], [--, --, --, , --, --, --]], mask=[[False, False, False, , False, False, False], [False, False, False, , False, False, False], [ True, False, False, , False, False, False], , [ True, True, True,",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": ", True, True, True], [ True, True, True, , True, True, True], [ True, True, True, , True, True, True]], fill_value=-1) We can see that this is a different kind of array As mentioned in the introduction, it has three attributes (data, mask and fill_value) Keep in mind that the mask attribute has a True value for elements corresponding to invalid data (represented by two dashes in the data attribute)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "Lets try and see what the data looks like excluding the first row (data from the Hubei province in China) so we can look at the missing data more closely: plt plot(dates, nbcases_ma[1:] T, \"--\") plt xticks(selected_dates, dates[selected_dates]) plt title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\") Text(0 5, 1",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020') Now that our data has been masked, lets try summing up all the cases in China: china_masked = nbcases_ma[locations[:, 1] == \"China\"]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "sum(axis=0) china_masked masked_array(data=[278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821, 14411, 17238], mask=[False, False, False, False, False, False, False, False, False, False, False, False, False, False], fill_value=999999) Note that china_masked is a masked array, so it has a different data structure than a regular NumPy array Now, we can access its data directly by using the data attribute: china_total = china_masked",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": "data china_total array([ 278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821, 14411, 17238]) That is better: no more negative values However, we can still see that for some days, the cumulative number of cases seems to go down (from 835 to 10, for example), which does not agree with the definition of cumulative data",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "If we look more closely at the data, we can see that in the period where there was missing data in mainland China, there was valid data for Hong Kong, Taiwan, Macau and Unspecified regions of China Maybe we can remove those from the total sum of cases in China, to get a better understanding of the data First, well identify the indices of locations in mainland China: china_mask = ( (locations[:, 1] == \"China\")  (locations[:, 0] = \"Hong Kong\")  (locations[:, 0] = \"Taiwan\")  (locations[:, 0]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "= \"Macau\")  (locations[:, 0] = \"Unspecified*\") ) Now, china_mask is an array of boolean values (True or False); we can check that the indices are what we wanted with the ma nonzero method for masked arrays: china_mask nonzero() (array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 31, 33]),) Now we can correctly sum entries for mainland China: china_total = nbcases_ma[china_mask]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "sum(axis=0) china_total masked_array(data=[278, 308, 440, 446, --, --, --, --, --, --, --, 11791, 14380, 17205], mask=[False, False, False, False, True, True, True, True, True, True, True, False, False, False], fill_value=999999) We can replace the data with this information and plot a new graph, focusing on Mainland China: plt plot(dates, china_total T, \"--\") plt xticks(selected_dates, dates[selected_dates]) plt title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\") Text(0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "5, 1 0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China') Its clear that masked arrays are the right solution here We cannot represent the missing data without mischaracterizing the evolution of the curve Fitting Data One possibility we can think of is to interpolate the missing data to estimate the number of cases in late January Observe that we can select the masked elements using the mask attribute: china_total mask invalid = china_total[china_total",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "mask] invalid masked_array(data=[--, --, --, --, --, --, --], mask=[ True, True, True, True, True, True, True], fill_value=999999, dtype=int64) We can also access the valid entries by using the logical negation for this mask: valid = china_total[china_total",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "mask] valid masked_array(data=[278, 308, 440, 446, 11791, 14380, 17205], mask=[False, False, False, False, False, False, False], fill_value=999999) Now, if we want to create a very simple approximation for this data, we should take into account the valid entries around the invalid ones So first lets select the dates for which the data is valid Note that we can use the mask from the china_total masked array to index the dates array: dates[china_total",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "mask] array(['1/21/20', '1/22/20', '1/23/20', '1/24/20', '2/1/20', '2/2/20', '2/3/20'], dtype='U7') Finally, we can use the fitting functionality of the numpy polynomial package to create a cubic polynomial model that fits the data as best as possible: t = np arange(len(china_total)) model = np polynomial Polynomial fit(t[china_total mask], valid, deg=3) plt plot(t, china_total) plt plot(t, model(t), \"--\") [matplotlib lines",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": "Line2D at 0x7f6048898bb0] This plot is not so readable since the lines seem to be over each other, so lets summarize in a more elaborate plot Well plot the real data when available, and show the cubic fit for unavailable data, using this fit to compute an estimate to the observed number of cases on January 28th 2020, 7 days after the beginning of the records: plt plot(t, china_total) plt plot(t[china_total mask], model(t)[china_total mask], \"--\", color=\"orange\") plt plot(7, model(7), \"r*\") plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": "xticks([0, 7, 13], dates[[0, 7, 13]]) plt yticks([0, model(7), 10000, 17500]) plt legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"]) plt title( \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\" \"Cubic estimate for 7 days after start\" ) Text(0 5, 1 0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\nCubic estimate for 7 days after start') In practice Adding -1 to missing data is not a problem with numpy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "genfromtxt; in this particular case, substituting the missing value with 0 might have been fine, but well see later that this is far from a general solution Also, it is possible to call the numpy genfromtxt function using the usemask parameter If usemask=True, numpy genfromtxt automatically returns a masked array Further reading Topics not covered in this tutorial can be found in the documentation: Hardmasks vs softmasks The numpy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "ma module Reference Ensheng Dong, Hongru Du, Lauren Gardner, An interactive web-based dashboard to track COVID-19 in real time, The Lancet Infectious Diseases, Volume 20, Issue 5, 2020, Pages 533-534, ISSN 1473-3099, https://doi org/10 1016/S1473-3099(20)30120-1 previous Saving and sharing your NumPy arrays next NumPy Applications Contents What youll do What youll learn What youll need What are masked arrays When can they be useful",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "Using masked arrays to see COVID-19 data Exploring the data Missing data Fitting Data In practice Further reading Reference By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html",
          "library": "numpy",
          "chunk_id": "numpy_37"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/applications.html",
      "title": "NumPy Applications \u2014 NumPy Tutorials",
      "content": "Repository Open issue .md .pdf NumPy Applications NumPy Applications A collection of highlighting the use of NumPy for applications in science, engineering, and data analysis. Determining Moores Law with real data in NumPy Deep learning on MNIST X-ray image processing Determining Static Equilibrium in NumPy Plotting Fractals Analyzing the impact of the lockdown on air quality in Delhi, India previous Masked Arrays next Determining Moores Law with real data in NumPy By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [],
      "chunks": [
        {
          "content": "Repository Open issue md pdf NumPy Applications NumPy Applications A collection of highlighting the use of NumPy for applications in science, engineering, and data analysis",
          "url": "https://numpy.org/numpy-tutorials/applications.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Determining Moores Law with real data in NumPy Deep learning on MNIST X-ray image processing Determining Static Equilibrium in NumPy Plotting Fractals Analyzing the impact of the lockdown on air quality in Delhi, India previous Masked Arrays next Determining Moores Law with real data in NumPy By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/applications.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
      "title": "Determining Moore\u2019s Law with real data in NumPy \u2014 NumPy Tutorials",
      "content": "Binder Repository Open issue .ipynb .md .pdf Determining Moores Law with real data in NumPy Contents What youll do Skills youll learn What youll need Building Moores law as an exponential function Loading historical manufacturing data to your workspace Calculating the historical growth curve for transistors Sharing your results as zipped arrays and a csv Zipping the arrays into a file Creating your own comma separated value file Wrapping up References Determining Moores Law with real data in NumPy The number of transistors reported per a given chip plotted on a log scale in the y axis with the date of introduction on the linear scale x-axis. The blue data points are from a transistor count table. The red line is an ordinary least squares prediction and the orange line is Moores law. What youll do In 1965, engineer Gordon Moore predicted that transistors on a chip would double every two years in the coming decade [1]. Youll compare Moores prediction against actual transistor counts in the 53 years following his prediction. You will determine the best-fit constants to describe the exponential growth of transistors on semiconductors compared to Moores Law. Skills youll learn Load data from a *.csv file Perform linear regression and predict exponential growth using ordinary least squares Youll compare exponential growth constants between models Share your analysis in a file: as NumPy zipped files *.npz as a *.csv file Assess the amazing progress semiconductor manufacturers have made in the last five decades What youll need 1. These packages: NumPy Matplotlib imported with the following commands import matplotlib.pyplot as plt import numpy as np 2. Since this is an exponential growth law you need a little background in doing math with natural logs and exponentials. Youll use these NumPy and Matplotlib functions: np.loadtxt: this function loads text into a NumPy array np.log: this function takes the natural log of all elements in a NumPy array np.exp: this function takes the exponential of all elements in a NumPy array lambda: this is a minimal function definition for creating a function model plt.semilogy: this function will plot x-y data onto a figure with a linear x-axis and \\(\\log_{10}\\) y-axis plt.plot: this function will plot x-y data on linear axes slicing arrays: view parts of the data loaded into the workspace, slice the arrays e.g. x[:10] for the first 10 values in the array, x boolean array indexing: to view parts of the data that match a given condition use boolean operations to index an array np.block: to combine arrays into 2D arrays np.newaxis: to change a 1D vector to a row or column vector np.savez and np.savetxt: these two functions will save your arrays in zipped array format and text, respectively Building Moores law as an exponential function Your empirical model assumes that the number of transistors per semiconductor follows an exponential growth, \\(\\log(\\text{transistor_count})= f(\\text{year}) = A\\cdot \\text{year}+B,\\) where \\(A\\) and \\(B\\) are fitting constants. You use semiconductor manufacturers data to find the fitting constants. You determine these constants for Moores law by specifying the rate for added transistors, 2, and giving an initial number of transistors for a given year. You state Moores law in an exponential form as follows, \\(\\text{transistor_count}= e{A_M\\cdot \\text{year} +B_M}.\\) Where \\(A_M\\) and \\(B_M\\) are constants that double the number of transistors every two years and start at 2250 transistors in 1971, \\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = 2 = \\dfrac{e{B_M}e{A_M \\text{year} + 2A_M}}{e{B_M}e{A_M \\text{year}}} = e{2A_M} \\rightarrow A_M = \\frac{\\log(2)}{2}\\) \\(\\log(2250) = \\frac{\\log(2)}{2}\\cdot 1971 + B_M \\rightarrow B_M = \\log(2250)-\\frac{\\log(2)}{2}\\cdot 1971\\) so Moores law stated as an exponential function is \\(\\log(\\text{transistor_count})= A_M\\cdot \\text{year}+B_M,\\) where \\(A_M=0.3466\\) \\(B_M=-675.4\\) Since the function represents Moores law, define it as a Python function using lambda A_M = np.log(2) / 2 B_M = np.log(2250) - A_M * 1971 Moores_law = lambda year: np.exp(B_M) * np.exp(A_M * year) In 1971, there were 2250 transistors on the Intel 4004 chip. Use Moores_law to check the number of semiconductors Gordon Moore would expect in 1973. ML_1971 = Moores_law(1971) ML_1973 = Moores_law(1973) print(\"In 1973, G. Moore expects {:.0f} transistors on Intels chips\".format(ML_1973)) print(\"This is x{:.2f} more transistors than 1971\".format(ML_1973 / ML_1971)) In 1973, G. Moore expects 4500 transistors on Intels chips This is x2.00 more transistors than 1971 Loading historical manufacturing data to your workspace Now, make a prediction based upon the historical data for semiconductors per chip. The Transistor Count [3] each year is in the transistor_data.csv file. Before loading a *.csv file into a NumPy array, its a good idea to inspect the structure of the file first. Then, locate the columns of interest and save them to a variable. Save two columns of the file to the array, data. Here, print out the first 10 rows of transistor_data.csv. The columns are Processor MOS transistor count Date of Introduction Designer MOSprocess Area Intel 4004 (4-bit 16-pin) 2250 1971 Intel 10,000 nm 12 mm\u00b2       ! head transistor_data.csv Processor,MOS transistor count,Date of Introduction,Designer,MOSprocess,Area Intel 4004 (4-bit 16-pin),2250,1971,Intel,\"10,000 nm\",12 mm\u00b2 Intel 8008 (8-bit 18-pin),3500,1972,Intel,\"10,000 nm\",14 mm\u00b2 NEC \u03bcCOM-4 (4-bit 42-pin),2500,1973,NEC,\"7,500 nm\",? Intel 4040 (4-bit 16-pin),3000,1974,Intel,\"10,000 nm\",12 mm\u00b2 Motorola 6800 (8-bit 40-pin),4100,1974,Motorola,\"6,000 nm\",16 mm\u00b2 Intel 8080 (8-bit 40-pin),6000,1974,Intel,\"6,000 nm\",20 mm\u00b2 TMS 1000 (4-bit 28-pin),8000,1974,Texas Instruments,\"8,000 nm\",11 mm\u00b2 MOS Technology 6502 (8-bit 40-pin),4528,1975,MOS Technology,\"8,000 nm\",21 mm\u00b2 Intersil IM6100 (12-bit 40-pin; clone of PDP-8),4000,1975,Intersil,, You dont need the columns that specify Processor, Designer, MOSprocess, or Area. That leaves the second and third columns, MOS transistor count and Date of Introduction, respectively. Next, you load these two columns into a NumPy array using np.loadtxt. The extra options below will put the data in the desired format: delimiter = ',': specify delimeter as a comma , (this is the default behavior) usecols = [1,2]: import the second and third columns from the csv skiprows = 1: do not use the first row, because its a header row data = np.loadtxt(\"transistor_data.csv\", delimiter=\",\", usecols=[1, 2], skiprows=1) You loaded the entire history of semiconducting into a NumPy array named data. The first column is the MOS transistor count and the second column is the Date of Introduction in a four-digit year. Next, make the data easier to read and manage by assigning the two columns to variables, year and transistor_count. Print out the first 10 values by slicing the year and transistor_count arrays with [:10]. Print these values out to check that you have the saved the data to the correct variables. year = data[:, 1]  grab the second column and assign transistor_count = data[:, 0]  grab the first column and assign print(\"year:\\t\\t\", year[:10]) print(\"trans. cnt:\\t\", transistor_count[:10]) year: [1971. 1972. 1973. 1974. 1974. 1974. 1974. 1975. 1975. 1975.] trans. cnt: [2250. 3500. 2500. 3000. 4100. 6000. 8000. 4528. 4000. 5000.] You are creating a function that predicts the transistor count given a year. You have an independent variable, year, and a dependent variable, transistor_count. Transform the dependent variable to log-scale, \\(y_i = \\log(\\) transistor_count[i] \\(),\\) resulting in a linear equation, \\(y_i = A\\cdot \\text{year} +B\\). yi = np.log(transistor_count) Calculating the historical growth curve for transistors Your model assume that yi is a function of year. Now, find the best-fit model that minimizes the difference between \\(y_i\\) and \\(A\\cdot \\text{year} +B, \\) as such \\(\\min \\sumy_i - (A\\cdot \\text{year}_i + B)2.\\) This sum of squares error can be succinctly represented as arrays as such \\(\\sum\\mathbf{y}-\\mathbf{Z} [A,B]T2,\\) where \\(\\mathbf{y}\\) are the observations of the log of the number of transistors in a 1D array and \\(\\mathbf{Z}=[\\text{year}_i1,\\text{year}_i0]\\) are the polynomial terms for \\(\\text{year}_i\\) in the first and second columns. By creating this set of regressors in the \\(\\mathbf{Z}-\\)matrix you set up an ordinary least squares statistical model. Z is a linear model with two parameters, i.e. a polynomial with degree 1. Therefore we can represent the model with numpy.polynomial.Polynomial and use the fitting functionality to determine the model parameters: model = np.polynomial.Polynomial.fit(year, yi, deg=1) By default, Polynomial.fit performs the fit in the domain determined by the independent variable (year in this case). The coefficients for the unscaled and unshifted model can be recovered with the convert method: model = model.convert() model \\[x \\mapsto \\text{-666.32640635} + \\text{0.34163208}\\,x\\] The individual parameters \\(A\\) and \\(B\\) are the coefficients of our linear model: B, A = model Did manufacturers double the transistor count every two years? You have the final formula, \\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = xFactor = \\dfrac{e{B}e{A( \\text{year} + 2)}}{e{B}e{A \\text{year}}} = e{2A}\\) where increase in number of transistors is \\(xFactor,\\) number of years is 2, and \\(A\\) is the best fit slope on the semilog function. print(f\"Rate of semiconductors added on a chip every 2 years: {np.exp(2 * A):.2f}\") Rate of semiconductors added on a chip every 2 years: 1.98 Based upon your least-squares regression model, the number of semiconductors per chip increased by a factor of \\(1.98\\) every two years. You have a model that predicts the number of semiconductors each year. Now compare your model to the actual manufacturing reports. Plot the linear regression results and all of the transistor counts. Here, use plt.semilogy to plot the number of transistors on a log-scale and the year on a linear scale. You have defined a three arrays to get to a final model \\(y_i = \\log(\\text{transistor_count}),\\) \\(y_i = A \\cdot \\text{year} + B,\\) and \\(\\log(\\text{transistor_count}) = A\\cdot \\text{year} + B,\\) your variables, transistor_count, year, and yi all have the same dimensions, (179,). NumPy arrays need the same dimensions to make a plot. The predicted number of transistors is now \\(\\text{transistor_count}_{\\text{predicted}} = eBe{A\\cdot \\text{year}}\\). In the next plot, use the fivethirtyeight style sheet. The style sheet replicates https://fivethirtyeight.com elements. Change the matplotlib style with plt.style.use. transistor_count_predicted = np.exp(B) * np.exp(A * year) transistor_Moores_law = Moores_law(year) plt.style.use(\"fivethirtyeight\") plt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\") plt.semilogy(year, transistor_count_predicted, label=\"linear regression\") plt.plot(year, transistor_Moores_law, label=\"Moore's Law\") plt.title( \"MOS transistor count per microprocessor\\n\" + \"every two years \\n\" + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2)) ) plt.xlabel(\"year introduced\") plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5)) plt.ylabel(\" of transistors\\nper microprocessor\") Text(0, 0.5, ' of transistors\\nper microprocessor') A scatter plot of MOS transistor count per microprocessor every two years with a red line for the ordinary least squares prediction and an orange line for Moores law. The linear regression captures the increase in the number of transistors per semiconductors each year. In 2015, semiconductor manufacturers claimed they could not keep up with Moores law anymore. Your analysis shows that since 1971, the average increase in transistor count was x1.98 every 2 years, but Gordon Moore predicted it would be x2 every 2 years. That is an amazing prediction. Consider the year 2017. Compare the data to your linear regression model and Gordon Moores prediction. First, get the transistor counts from the year 2017. You can do this with a Boolean comparator, year == 2017. Then, make a prediction for 2017 with Moores_law defined above and plugging in your best fit constants into your function \\(\\text{transistor_count} = e{B}e{A\\cdot \\text{year}}\\). A great way to compare these measurements is to compare your prediction and Moores prediction to the average transistor count and look at the range of reported values for that year. Use the plt.plot option, alpha=0.2, to increase the transparency of the data. The more opaque the points appear, the more reported values lie on that measurement. The green \\(+\\) is the average reported transistor count for 2017. Plot your predictions for \\pm\\frac{1}{2}years. transistor_count2017 = transistor_count[year == 2017] print( transistor_count2017.max(), transistor_count2017.min(), transistor_count2017.mean() ) y = np.linspace(2016.5, 2017.5) your_model2017 = np.exp(B) * np.exp(A * y) Moore_Model2017 = Moores_law(y) plt.plot( 2017 * np.ones(np.sum(year == 2017)), transistor_count2017, \"ro\", label=\"2017\", alpha=0.2, ) plt.plot(2017, transistor_count2017.mean(), \"g+\", markersize=20, mew=6) plt.plot(y, your_model2017, label=\"Your prediction\") plt.plot(y, Moore_Model2017, label=\"Moores law\") plt.ylabel(\" of transistors\\nper microprocessor\") plt.legend() 19200000000.0 250000000.0 7050000000.0 matplotlib.legend.Legend at 0x7fa70c3043d0 The result is that your model is close to the mean, but Gordon Moores prediction is closer to the maximum number of transistors per microprocessor produced in 2017. Even though semiconductor manufacturers thought that the growth would slow, once in 1975 and now again approaching 2025, manufacturers are still producing semiconductors every 2 years that nearly double the number of transistors. The linear regression model is much better at predicting the average than extreme values because it satisfies the condition to minimize \\(\\sum y_i - A\\cdot \\text{year}[i]+B2\\). Sharing your results as zipped arrays and a csv The last step, is to share your findings. You created new arrays that represent a linear regression model and Gordon Moores prediction. You started this process by importing a csv file into a NumPy array using np.loadtxt, to save your model use two approaches np.savez: save NumPy arrays for other Python sessions np.savetxt: save a csv file with the original data and your predicted data Zipping the arrays into a file Using np.savez, you can save thousands of arrays and give them names. The function np.load will load the arrays back into the workspace as a dictionary. Youll save a five arrays so the next user will have the year, transistor count, predicted transistor count, Gordon Moores predicted count, and fitting constants. Add one more variable that other users can use to understand the model, notes. notes = \"the arrays in this file are the result of a linear regression model\\n\" notes += \"the arrays include\\nyear: year of manufacture\\n\" notes += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\" notes += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format( B, A ) notes += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format( B_M, A_M ) notes += \"regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B\" print(notes) the arrays in this file are the result of a linear regression model the arrays include year: year of manufacture transistor_count: number of transistors reported by manufacturers in a given year transistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year) transistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year) regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B np.savez( \"mooreslaw_regression.npz\", notes=notes, year=year, transistor_count=transistor_count, transistor_count_predicted=transistor_count_predicted, transistor_Moores_law=transistor_Moores_law, regression_csts=(A, B), ) results = np.load(\"mooreslaw_regression.npz\") print(results[\"regression_csts\"][1]) -666.3264063536233 ! ls air-quality-data.csv mooreslaw_regression.npz mooreslaw-tutorial.md pairing.md save-load-arrays.md _static text_preprocessing.py transistor_data.csv tutorial-air-quality-analysis.md tutorial-deep-learning-on-mnist.md tutorial-deep-reinforcement-learning-with-pong-from-pixels.md tutorial-ma.md tutorial-nlp-from-scratch tutorial-nlp-from-scratch.md tutorial-plotting-fractals tutorial-plotting-fractals.md tutorial-static_equilibrium.md tutorial-style-guide.md tutorial-svd.md tutorial-x-ray-image-processing tutorial-x-ray-image-processing.md who_covid_19_sit_rep_time_series.csv The benefit of np.savez is you can save hundreds of arrays with different shapes and types. Here, you saved 4 arrays that are double precision floating point numbers shape = (179,), one array that was text, and one array of double precision floating point numbers shape = (2,). This is the preferred method for saving NumPy arrays for use in another analysis. Creating your own comma separated value file If you want to share data and view the results in a table, then you have to create a text file. Save the data using np.savetxt. This function is more limited than np.savez. Delimited files, like csvs, need 2D arrays. Prepare the data for export by creating a new 2D array whose columns contain the data of interest. Use the header option to describe the data and the columns of the file. Define another variable that contains file information as head. head = \"the columns in this file are the result of a linear regression model\\n\" head += \"the columns include\\nyear: year of manufacture\\n\" head += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\" head += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format( B, A ) head += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format( B_M, A_M ) head += \"year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\" print(head) the columns in this file are the result of a linear regression model the columns include year: year of manufacture transistor_count: number of transistors reported by manufacturers in a given year transistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year) transistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year) year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law: Build a single 2D array to export to csv. Tabular data is inherently two dimensional. You need to organize your data to fit this 2D structure. Use year, transistor_count, transistor_count_predicted, and transistor_Moores_law as the first through fourth columns, respectively. Put the calculated constants in the header since they do not fit the (179,) shape. The np.block function appends arrays together to create a new, larger array. Arrange the 1D vectors as columns using np.newaxis e.g.  year.shape (179,)  year[:,np.newaxis].shape (179,1) output = np.block( [ year[:, np.newaxis], transistor_count[:, np.newaxis], transistor_count_predicted[:, np.newaxis], transistor_Moores_law[:, np.newaxis], ] ) Creating the mooreslaw_regression.csv with np.savetxt, use three options to create the desired file format: X = output : use output block to write the data into the file delimiter = ',' : use commas to separate columns in the file header = head : use the header head defined above np.savetxt(\"mooreslaw_regression.csv\", X=output, delimiter=\",\", header=head) ! head mooreslaw_regression.csv  the columns in this file are the result of a linear regression model  the columns include  year: year of manufacture  transistor_count: number of transistors reported by manufacturers in a given year  transistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)  transistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)  year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law: 1.971000000000000000e+03,2.250000000000000000e+03,1.130514785642591505e+03,2.249999999999916326e+03 1.972000000000000000e+03,3.500000000000000000e+03,1.590908400344571419e+03,3.181980515339620069e+03 1.973000000000000000e+03,2.500000000000000000e+03,2.238793840142739100e+03,4.500000000000097316e+03 Wrapping up In conclusion, you have compared historical data for semiconductor manufacturers to Moores law and created a linear regression model to find the average number of transistors added to each microprocessor every two years. Gordon Moore predicted the number of transistors would double every two years from 1965 through 1975, but the average growth has maintained a consistent increase of \\(\\times 1.98 \\pm 0.01\\) every two years from 1971 through 2019. In 2015, Moore revised his prediction to say Moores law should hold until 2025. [2]. You can share these results as a zipped NumPy array file, mooreslaw_regression.npz, or as another csv, mooreslaw_regression.csv. The amazing progress in semiconductor manufacturing has enabled new industries and computational power. This analysis should give you a small insight into how incredible this growth has been over the last half-century. References Moores Law. Wikipedia article. Accessed Oct. 1, 2020. Courtland, Rachel. Gordon Moore: The Man Whose Name Means Progress. IEEE Spectrum. 30 Mar. 2015.. Transistor Count. Wikipedia article. Accessed Oct. 1, 2020. previous NumPy Applications next Deep learning on MNIST Contents What youll do Skills youll learn What youll need Building Moores law as an exponential function Loading historical manufacturing data to your workspace Calculating the historical growth curve for transistors Sharing your results as zipped arrays and a csv Zipping the arrays into a file Creating your own comma separated value file Wrapping up References By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "import matplotlib.pyplot as plt\nimport numpy as np",
        "plt.semilogy",
        "A_M = np.log(2) / 2\nB_M = np.log(2250) - A_M * 1971\nMoores_law = lambda year: np.exp(B_M) * np.exp(A_M * year)",
        "ML_1971 = Moores_law(1971)\nML_1973 = Moores_law(1973)\nprint(\"In 1973, G. Moore expects {:.0f} transistors on Intels chips\".format(ML_1973))\nprint(\"This is x{:.2f} more transistors than 1971\".format(ML_1973 / ML_1971))",
        "In 1973, G. Moore expects 4500 transistors on Intels chips\nThis is x2.00 more transistors than 1971",
        "transistor_data.csv",
        "transistor_data.csv",
        "! head transistor_data.csv",
        "Processor,MOS transistor count,Date of Introduction,Designer,MOSprocess,Area\nIntel 4004 (4-bit  16-pin),2250,1971,Intel,\"10,000\u00a0nm\",12\u00a0mm\u00b2\nIntel 8008 (8-bit  18-pin),3500,1972,Intel,\"10,000\u00a0nm\",14\u00a0mm\u00b2\nNEC \u03bcCOM-4 (4-bit  42-pin),2500,1973,NEC,\"7,500\u00a0nm\",?\nIntel 4040 (4-bit  16-pin),3000,1974,Intel,\"10,000\u00a0nm\",12\u00a0mm\u00b2\nMotorola 6800 (8-bit  40-pin),4100,1974,Motorola,\"6,000\u00a0nm\",16\u00a0mm\u00b2\nIntel 8080 (8-bit  40-pin),6000,1974,Intel,\"6,000\u00a0nm\",20\u00a0mm\u00b2\nTMS 1000 (4-bit  28-pin),8000,1974,Texas Instruments,\"8,000\u00a0nm\",11\u00a0mm\u00b2\nMOS Technology 6502 (8-bit  40-pin),4528,1975,MOS Technology,\"8,000\u00a0nm\",21\u00a0mm\u00b2\nIntersil IM6100 (12-bit  40-pin; clone of PDP-8),4000,1975,Intersil,,",
        "delimiter = ','",
        "usecols = [1,2]",
        "skiprows = 1",
        "data = np.loadtxt(\"transistor_data.csv\", delimiter=\",\", usecols=[1, 2], skiprows=1)",
        "transistor_count",
        "transistor_count",
        "year = data[:, 1]  # grab the second column and assign\ntransistor_count = data[:, 0]  # grab the first column and assign\n\nprint(\"year:\\t\\t\", year[:10])\nprint(\"trans. cnt:\\t\", transistor_count[:10])",
        "year:\t\t [1971. 1972. 1973. 1974. 1974. 1974. 1974. 1975. 1975. 1975.]\ntrans. cnt:\t [2250. 3500. 2500. 3000. 4100. 6000. 8000. 4528. 4000. 5000.]",
        "transistor_count",
        "transistor_count[i]",
        "yi = np.log(transistor_count)",
        "numpy.polynomial.Polynomial",
        "model = np.polynomial.Polynomial.fit(year, yi, deg=1)",
        "Polynomial.fit",
        "model = model.convert()\nmodel",
        "B, A = model",
        "print(f\"Rate of semiconductors added on a chip every 2 years: {np.exp(2 * A):.2f}\")",
        "Rate of semiconductors added on a chip every 2 years: 1.98",
        "plt.semilogy",
        "transistor_count",
        "fivethirtyeight",
        "plt.style.use",
        "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")",
        "Text(0, 0.5, '# of transistors\\nper microprocessor')",
        "year == 2017",
        "transistor_count2017 = transistor_count[year == 2017]\nprint(\n    transistor_count2017.max(), transistor_count2017.min(), transistor_count2017.mean()\n)\ny = np.linspace(2016.5, 2017.5)\nyour_model2017 = np.exp(B) * np.exp(A * y)\nMoore_Model2017 = Moores_law(y)\n\nplt.plot(\n    2017 * np.ones(np.sum(year == 2017)),\n    transistor_count2017,\n    \"ro\",\n    label=\"2017\",\n    alpha=0.2,\n)\nplt.plot(2017, transistor_count2017.mean(), \"g+\", markersize=20, mew=6)\n\nplt.plot(y, your_model2017, label=\"Your prediction\")\nplt.plot(y, Moore_Model2017, label=\"Moores law\")\nplt.ylabel(\"# of transistors\\nper microprocessor\")\nplt.legend()",
        "19200000000.0 250000000.0 7050000000.0",
        "<matplotlib.legend.Legend at 0x7fa70c3043d0>",
        "notes = \"the arrays in this file are the result of a linear regression model\\n\"\nnotes += \"the arrays include\\nyear: year of manufacture\\n\"\nnotes += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\"\nnotes += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B, A\n)\nnotes += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B_M, A_M\n)\nnotes += \"regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B\"\nprint(notes)",
        "the arrays in this file are the result of a linear regression model\nthe arrays include\nyear: year of manufacture\ntransistor_count: number of transistors reported by manufacturers in a given year\ntransistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\ntransistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\nregression_csts: linear regression constants A and B for log(transistor_count)=A*year+B",
        "np.savez(\n    \"mooreslaw_regression.npz\",\n    notes=notes,\n    year=year,\n    transistor_count=transistor_count,\n    transistor_count_predicted=transistor_count_predicted,\n    transistor_Moores_law=transistor_Moores_law,\n    regression_csts=(A, B),\n)",
        "results = np.load(\"mooreslaw_regression.npz\")",
        "print(results[\"regression_csts\"][1])",
        "-666.3264063536233",
        "air-quality-data.csv\nmooreslaw_regression.npz\nmooreslaw-tutorial.md\npairing.md\nsave-load-arrays.md\n_static\ntext_preprocessing.py\ntransistor_data.csv\ntutorial-air-quality-analysis.md\ntutorial-deep-learning-on-mnist.md\ntutorial-deep-reinforcement-learning-with-pong-from-pixels.md\ntutorial-ma.md\ntutorial-nlp-from-scratch\ntutorial-nlp-from-scratch.md\ntutorial-plotting-fractals\ntutorial-plotting-fractals.md\ntutorial-static_equilibrium.md\ntutorial-style-guide.md\ntutorial-svd.md\ntutorial-x-ray-image-processing\ntutorial-x-ray-image-processing.md\nwho_covid_19_sit_rep_time_series.csv",
        "head = \"the columns in this file are the result of a linear regression model\\n\"\nhead += \"the columns include\\nyear: year of manufacture\\n\"\nhead += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\"\nhead += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B, A\n)\nhead += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B_M, A_M\n)\nhead += \"year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\"\nprint(head)",
        "the columns in this file are the result of a linear regression model\nthe columns include\nyear: year of manufacture\ntransistor_count: number of transistors reported by manufacturers in a given year\ntransistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\ntransistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\nyear:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:",
        "transistor_count",
        "transistor_count_predicted",
        "transistor_Moores_law",
        ">>> year.shape\n(179,)\n>>> year[:,np.newaxis].shape\n(179,1)",
        "output = np.block(\n    [\n        year[:, np.newaxis],\n        transistor_count[:, np.newaxis],\n        transistor_count_predicted[:, np.newaxis],\n        transistor_Moores_law[:, np.newaxis],\n    ]\n)",
        "mooreslaw_regression.csv",
        "delimiter = ','",
        "header = head",
        "np.savetxt(\"mooreslaw_regression.csv\", X=output, delimiter=\",\", header=head)",
        "! head mooreslaw_regression.csv",
        "# the columns in this file are the result of a linear regression model\n# the columns include\n# year: year of manufacture\n# transistor_count: number of transistors reported by manufacturers in a given year\n# transistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\n# transistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\n# year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\n1.971000000000000000e+03,2.250000000000000000e+03,1.130514785642591505e+03,2.249999999999916326e+03\n1.972000000000000000e+03,3.500000000000000000e+03,1.590908400344571419e+03,3.181980515339620069e+03\n1.973000000000000000e+03,2.500000000000000000e+03,2.238793840142739100e+03,4.500000000000097316e+03",
        "mooreslaw_regression.npz",
        "mooreslaw_regression.csv"
      ],
      "chunks": [
        {
          "content": "Binder Repository Open issue ipynb md",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "pdf Determining Moores Law with real data in NumPy Contents What youll do Skills youll learn What youll need Building Moores law as an exponential function Loading historical manufacturing data to your workspace Calculating the historical growth curve for transistors Sharing your results as zipped arrays and a csv Zipping the arrays into a file Creating your own comma separated value file Wrapping up References Determining Moores Law with real data in NumPy The number of transistors reported per a given chip plotted on a log scale in the y axis with the date of introduction on the linear scale x-axis",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "The blue data points are from a transistor count table The red line is an ordinary least squares prediction and the orange line is Moores law What youll do In 1965, engineer Gordon Moore predicted that transistors on a chip would double every two years in the coming decade [1] Youll compare Moores prediction against actual transistor counts in the 53 years following his prediction",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "You will determine the best-fit constants to describe the exponential growth of transistors on semiconductors compared to Moores Law Skills youll learn Load data from a * csv file Perform linear regression and predict exponential growth using ordinary least squares Youll compare exponential growth constants between models Share your analysis in a file: as NumPy zipped files * npz as a *",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "csv file Assess the amazing progress semiconductor manufacturers have made in the last five decades What youll need 1 These packages: NumPy Matplotlib imported with the following commands import matplotlib pyplot as plt import numpy as np 2 Since this is an exponential growth law you need a little background in doing math with natural logs and exponentials Youll use these NumPy and Matplotlib functions: np loadtxt: this function loads text into a NumPy array np",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "log: this function takes the natural log of all elements in a NumPy array np exp: this function takes the exponential of all elements in a NumPy array lambda: this is a minimal function definition for creating a function model plt semilogy: this function will plot x-y data onto a figure with a linear x-axis and \\(\\log_{10}\\) y-axis plt plot: this function will plot x-y data on linear axes slicing arrays: view parts of the data loaded into the workspace, slice the arrays e g",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "x[:10] for the first 10 values in the array, x boolean array indexing: to view parts of the data that match a given condition use boolean operations to index an array np block: to combine arrays into 2D arrays np newaxis: to change a 1D vector to a row or column vector np savez and np",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "savetxt: these two functions will save your arrays in zipped array format and text, respectively Building Moores law as an exponential function Your empirical model assumes that the number of transistors per semiconductor follows an exponential growth, \\(\\log(\\text{transistor_count})= f(\\text{year}) = A\\cdot \\text{year}+B,\\) where \\(A\\) and \\(B\\) are fitting constants You use semiconductor manufacturers data to find the fitting constants",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "You determine these constants for Moores law by specifying the rate for added transistors, 2, and giving an initial number of transistors for a given year You state Moores law in an exponential form as follows, \\(\\text{transistor_count}= e{A_M\\cdot \\text{year} +B_M}",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "\\) Where \\(A_M\\) and \\(B_M\\) are constants that double the number of transistors every two years and start at 2250 transistors in 1971, \\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = 2 = \\dfrac{e{B_M}e{A_M \\text{year} + 2A_M}}{e{B_M}e{A_M \\text{year}}} = e{2A_M} \\rightarrow A_M = \\frac{\\log(2)}{2}\\) \\(\\log(2250) = \\frac{\\log(2)}{2}\\cdot 1971 + B_M \\rightarrow B_M = \\log(2250)-\\frac{\\log(2)}{2}\\cdot 1971\\) so Moores law stated as an exponential function is \\(\\log(\\text{transistor_count})= A_M\\cdot \\text{year}+B_M,\\) where \\(A_M=0",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "3466\\) \\(B_M=-675 4\\) Since the function represents Moores law, define it as a Python function using lambda A_M = np log(2) / 2 B_M = np log(2250) - A_M * 1971 Moores_law = lambda year: np exp(B_M) * np exp(A_M * year) In 1971, there were 2250 transistors on the Intel 4004 chip Use Moores_law to check the number of semiconductors Gordon Moore would expect in 1973 ML_1971 = Moores_law(1971) ML_1973 = Moores_law(1973) print(\"In 1973, G Moore expects {: 0f} transistors on Intels chips\"",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "format(ML_1973)) print(\"This is x{: 2f} more transistors than 1971\" format(ML_1973 / ML_1971)) In 1973, G Moore expects 4500 transistors on Intels chips This is x2 00 more transistors than 1971 Loading historical manufacturing data to your workspace Now, make a prediction based upon the historical data for semiconductors per chip The Transistor Count [3] each year is in the transistor_data csv file Before loading a *",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "csv file into a NumPy array, its a good idea to inspect the structure of the file first Then, locate the columns of interest and save them to a variable Save two columns of the file to the array, data Here, print out the first 10 rows of transistor_data csv The columns are Processor MOS transistor count Date of Introduction Designer MOSprocess Area Intel 4004 (4-bit 16-pin) 2250 1971 Intel 10,000 nm 12 mm\u00b2 head transistor_data",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "csv Processor,MOS transistor count,Date of Introduction,Designer,MOSprocess,Area Intel 4004 (4-bit 16-pin),2250,1971,Intel,\"10,000 nm\",12 mm\u00b2 Intel 8008 (8-bit 18-pin),3500,1972,Intel,\"10,000 nm\",14 mm\u00b2 NEC \u03bcCOM-4 (4-bit 42-pin),2500,1973,NEC,\"7,500 nm\",",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "Intel 4040 (4-bit 16-pin),3000,1974,Intel,\"10,000 nm\",12 mm\u00b2 Motorola 6800 (8-bit 40-pin),4100,1974,Motorola,\"6,000 nm\",16 mm\u00b2 Intel 8080 (8-bit 40-pin),6000,1974,Intel,\"6,000 nm\",20 mm\u00b2 TMS 1000 (4-bit 28-pin),8000,1974,Texas Instruments,\"8,000 nm\",11 mm\u00b2 MOS Technology 6502 (8-bit 40-pin),4528,1975,MOS Technology,\"8,000 nm\",21 mm\u00b2 Intersil IM6100 (12-bit 40-pin; clone of PDP-8),4000,1975,Intersil,, You dont need the columns that specify Processor, Designer, MOSprocess, or Area",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "That leaves the second and third columns, MOS transistor count and Date of Introduction, respectively Next, you load these two columns into a NumPy array using np loadtxt The extra options below will put the data in the desired format: delimiter = ',': specify delimeter as a comma , (this is the default behavior) usecols = [1,2]: import the second and third columns from the csv skiprows = 1: do not use the first row, because its a header row data = np loadtxt(\"transistor_data",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "csv\", delimiter=\",\", usecols=[1, 2], skiprows=1) You loaded the entire history of semiconducting into a NumPy array named data The first column is the MOS transistor count and the second column is the Date of Introduction in a four-digit year Next, make the data easier to read and manage by assigning the two columns to variables, year and transistor_count Print out the first 10 values by slicing the year and transistor_count arrays with [:10]",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Print these values out to check that you have the saved the data to the correct variables year = data[:, 1]  grab the second column and assign transistor_count = data[:, 0]  grab the first column and assign print(\"year:\\t\\t\", year[:10]) print(\"trans cnt:\\t\", transistor_count[:10]) year: [1971 1972 1973 1974 1974 1974 1974 1975 1975 1975 ] trans cnt: [2250 3500 2500 3000 4100 6000 8000 4528 4000 5000 ] You are creating a function that predicts the transistor count given a year",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "You have an independent variable, year, and a dependent variable, transistor_count Transform the dependent variable to log-scale, \\(y_i = \\log(\\) transistor_count[i] \\(),\\) resulting in a linear equation, \\(y_i = A\\cdot \\text{year} +B\\) yi = np log(transistor_count) Calculating the historical growth curve for transistors Your model assume that yi is a function of year",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "Now, find the best-fit model that minimizes the difference between \\(y_i\\) and \\(A\\cdot \\text{year} +B, \\) as such \\(\\min \\sumy_i - (A\\cdot \\text{year}_i + B)2 \\) This sum of squares error can be succinctly represented as arrays as such \\(\\sum\\mathbf{y}-\\mathbf{Z} [A,B]T2,\\) where \\(\\mathbf{y}\\) are the observations of the log of the number of transistors in a 1D array and \\(\\mathbf{Z}=[\\text{year}_i1,\\text{year}_i0]\\) are the polynomial terms for \\(\\text{year}_i\\) in the first and second columns",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "By creating this set of regressors in the \\(\\mathbf{Z}-\\)matrix you set up an ordinary least squares statistical model Z is a linear model with two parameters, i e a polynomial with degree 1 Therefore we can represent the model with numpy polynomial Polynomial and use the fitting functionality to determine the model parameters: model = np polynomial Polynomial fit(year, yi, deg=1) By default, Polynomial fit performs the fit in the domain determined by the independent variable (year in this case)",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "The coefficients for the unscaled and unshifted model can be recovered with the convert method: model = model convert() model \\[x \\mapsto \\text{-666 32640635} + \\text{0 34163208}\\,x\\] The individual parameters \\(A\\) and \\(B\\) are the coefficients of our linear model: B, A = model Did manufacturers double the transistor count every two years",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "You have the final formula, \\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = xFactor = \\dfrac{e{B}e{A( \\text{year} + 2)}}{e{B}e{A \\text{year}}} = e{2A}\\) where increase in number of transistors is \\(xFactor,\\) number of years is 2, and \\(A\\) is the best fit slope on the semilog function print(f\"Rate of semiconductors added on a chip every 2 years: {np exp(2 * A): 2f}\") Rate of semiconductors added on a chip every 2 years: 1",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "98 Based upon your least-squares regression model, the number of semiconductors per chip increased by a factor of \\(1 98\\) every two years You have a model that predicts the number of semiconductors each year Now compare your model to the actual manufacturing reports Plot the linear regression results and all of the transistor counts Here, use plt semilogy to plot the number of transistors on a log-scale and the year on a linear scale",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "You have defined a three arrays to get to a final model \\(y_i = \\log(\\text{transistor_count}),\\) \\(y_i = A \\cdot \\text{year} + B,\\) and \\(\\log(\\text{transistor_count}) = A\\cdot \\text{year} + B,\\) your variables, transistor_count, year, and yi all have the same dimensions, (179,) NumPy arrays need the same dimensions to make a plot The predicted number of transistors is now \\(\\text{transistor_count}_{\\text{predicted}} = eBe{A\\cdot \\text{year}}\\)",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": "In the next plot, use the fivethirtyeight style sheet The style sheet replicates https://fivethirtyeight com elements Change the matplotlib style with plt style use transistor_count_predicted = np exp(B) * np exp(A * year) transistor_Moores_law = Moores_law(year) plt style use(\"fivethirtyeight\") plt semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\") plt semilogy(year, transistor_count_predicted, label=\"linear regression\") plt",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "plot(year, transistor_Moores_law, label=\"Moore's Law\") plt title( \"MOS transistor count per microprocessor\\n\" + \"every two years \\n\" + \"Transistor count was x{: 2f} higher\" format(np exp(A * 2)) ) plt xlabel(\"year introduced\") plt legend(loc=\"center left\", bbox_to_anchor=(1, 0 5)) plt ylabel(\" of transistors\\nper microprocessor\") Text(0, 0",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "5, ' of transistors\\nper microprocessor') A scatter plot of MOS transistor count per microprocessor every two years with a red line for the ordinary least squares prediction and an orange line for Moores law The linear regression captures the increase in the number of transistors per semiconductors each year In 2015, semiconductor manufacturers claimed they could not keep up with Moores law anymore Your analysis shows that since 1971, the average increase in transistor count was x1",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "98 every 2 years, but Gordon Moore predicted it would be x2 every 2 years That is an amazing prediction Consider the year 2017 Compare the data to your linear regression model and Gordon Moores prediction First, get the transistor counts from the year 2017 You can do this with a Boolean comparator, year == 2017 Then, make a prediction for 2017 with Moores_law defined above and plugging in your best fit constants into your function \\(\\text{transistor_count} = e{B}e{A\\cdot \\text{year}}\\)",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "A great way to compare these measurements is to compare your prediction and Moores prediction to the average transistor count and look at the range of reported values for that year Use the plt plot option, alpha=0 2, to increase the transparency of the data The more opaque the points appear, the more reported values lie on that measurement The green \\(+\\) is the average reported transistor count for 2017 Plot your predictions for \\pm\\frac{1}{2}years",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "transistor_count2017 = transistor_count[year == 2017] print( transistor_count2017 max(), transistor_count2017 min(), transistor_count2017 mean() ) y = np linspace(2016 5, 2017 5) your_model2017 = np exp(B) * np exp(A * y) Moore_Model2017 = Moores_law(y) plt plot( 2017 * np ones(np sum(year == 2017)), transistor_count2017, \"ro\", label=\"2017\", alpha=0 2, ) plt plot(2017, transistor_count2017 mean(), \"g+\", markersize=20, mew=6) plt plot(y, your_model2017, label=\"Your prediction\") plt",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "plot(y, Moore_Model2017, label=\"Moores law\") plt ylabel(\" of transistors\\nper microprocessor\") plt legend() 19200000000 0 250000000 0 7050000000 0 matplotlib legend Legend at 0x7fa70c3043d0 The result is that your model is close to the mean, but Gordon Moores prediction is closer to the maximum number of transistors per microprocessor produced in 2017",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "Even though semiconductor manufacturers thought that the growth would slow, once in 1975 and now again approaching 2025, manufacturers are still producing semiconductors every 2 years that nearly double the number of transistors The linear regression model is much better at predicting the average than extreme values because it satisfies the condition to minimize \\(\\sum y_i - A\\cdot \\text{year}[i]+B2\\) Sharing your results as zipped arrays and a csv The last step, is to share your findings",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": "You created new arrays that represent a linear regression model and Gordon Moores prediction You started this process by importing a csv file into a NumPy array using np loadtxt, to save your model use two approaches np savez: save NumPy arrays for other Python sessions np savetxt: save a csv file with the original data and your predicted data Zipping the arrays into a file Using np savez, you can save thousands of arrays and give them names The function np",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": "load will load the arrays back into the workspace as a dictionary Youll save a five arrays so the next user will have the year, transistor count, predicted transistor count, Gordon Moores predicted count, and fitting constants Add one more variable that other users can use to understand the model, notes",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "notes = \"the arrays in this file are the result of a linear regression model\\n\" notes += \"the arrays include\\nyear: year of manufacture\\n\" notes += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\" notes += \"transistor_count_predicted: linear regression model = exp({: 2f})*exp({: 2f}*year)\\n\" format( B, A ) notes += \"transistor_Moores_law: Moores law =exp({: 2f})*exp({: 2f}*year)\\n\"",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "format( B_M, A_M ) notes += \"regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B\" print(notes) the arrays in this file are the result of a linear regression model the arrays include year: year of manufacture transistor_count: number of transistors reported by manufacturers in a given year transistor_count_predicted: linear regression model = exp(-666 33)*exp(0 34*year) transistor_Moores_law: Moores law =exp(-675 38)*exp(0",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "35*year) regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B np savez( \"mooreslaw_regression npz\", notes=notes, year=year, transistor_count=transistor_count, transistor_count_predicted=transistor_count_predicted, transistor_Moores_law=transistor_Moores_law, regression_csts=(A, B), ) results = np load(\"mooreslaw_regression npz\") print(results[\"regression_csts\"][1]) -666 3264063536233 ls air-quality-data csv mooreslaw_regression npz mooreslaw-tutorial md pairing",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_37"
        },
        {
          "content": "md save-load-arrays md _static text_preprocessing py transistor_data csv tutorial-air-quality-analysis md tutorial-deep-learning-on-mnist md tutorial-deep-reinforcement-learning-with-pong-from-pixels md tutorial-ma md tutorial-nlp-from-scratch tutorial-nlp-from-scratch md tutorial-plotting-fractals tutorial-plotting-fractals md tutorial-static_equilibrium md tutorial-style-guide md tutorial-svd md tutorial-x-ray-image-processing tutorial-x-ray-image-processing md who_covid_19_sit_rep_time_series",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_38"
        },
        {
          "content": "csv The benefit of np savez is you can save hundreds of arrays with different shapes and types Here, you saved 4 arrays that are double precision floating point numbers shape = (179,), one array that was text, and one array of double precision floating point numbers shape = (2,) This is the preferred method for saving NumPy arrays for use in another analysis Creating your own comma separated value file If you want to share data and view the results in a table, then you have to create a text file",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_39"
        },
        {
          "content": "Save the data using np savetxt This function is more limited than np savez Delimited files, like csvs, need 2D arrays Prepare the data for export by creating a new 2D array whose columns contain the data of interest Use the header option to describe the data and the columns of the file Define another variable that contains file information as head",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_40"
        },
        {
          "content": "head = \"the columns in this file are the result of a linear regression model\\n\" head += \"the columns include\\nyear: year of manufacture\\n\" head += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\" head += \"transistor_count_predicted: linear regression model = exp({: 2f})*exp({: 2f}*year)\\n\" format( B, A ) head += \"transistor_Moores_law: Moores law =exp({: 2f})*exp({: 2f}*year)\\n\"",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_41"
        },
        {
          "content": "format( B_M, A_M ) head += \"year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\" print(head) the columns in this file are the result of a linear regression model the columns include year: year of manufacture transistor_count: number of transistors reported by manufacturers in a given year transistor_count_predicted: linear regression model = exp(-666 33)*exp(0 34*year) transistor_Moores_law: Moores law =exp(-675 38)*exp(0",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_42"
        },
        {
          "content": "35*year) year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law: Build a single 2D array to export to csv Tabular data is inherently two dimensional You need to organize your data to fit this 2D structure Use year, transistor_count, transistor_count_predicted, and transistor_Moores_law as the first through fourth columns, respectively Put the calculated constants in the header since they do not fit the (179,) shape The np",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_43"
        },
        {
          "content": "block function appends arrays together to create a new, larger array Arrange the 1D vectors as columns using np newaxis e g year shape (179,)  year[:,np newaxis] shape (179,1) output = np block( [ year[:, np newaxis], transistor_count[:, np newaxis], transistor_count_predicted[:, np newaxis], transistor_Moores_law[:, np newaxis], ] ) Creating the mooreslaw_regression csv with np",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_44"
        },
        {
          "content": "savetxt, use three options to create the desired file format: X = output : use output block to write the data into the file delimiter = ',' : use commas to separate columns in the file header = head : use the header head defined above np savetxt(\"mooreslaw_regression csv\", X=output, delimiter=\",\", header=head) head mooreslaw_regression",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_45"
        },
        {
          "content": "csv  the columns in this file are the result of a linear regression model  the columns include  year: year of manufacture  transistor_count: number of transistors reported by manufacturers in a given year  transistor_count_predicted: linear regression model = exp(-666 33)*exp(0 34*year)  transistor_Moores_law: Moores law =exp(-675 38)*exp(0 35*year)  year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law: 1 971000000000000000e+03,2 250000000000000000e+03,1",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_46"
        },
        {
          "content": "130514785642591505e+03,2 249999999999916326e+03 1 972000000000000000e+03,3 500000000000000000e+03,1 590908400344571419e+03,3 181980515339620069e+03 1 973000000000000000e+03,2 500000000000000000e+03,2 238793840142739100e+03,4 500000000000097316e+03 Wrapping up In conclusion, you have compared historical data for semiconductor manufacturers to Moores law and created a linear regression model to find the average number of transistors added to each microprocessor every two years",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_47"
        },
        {
          "content": "Gordon Moore predicted the number of transistors would double every two years from 1965 through 1975, but the average growth has maintained a consistent increase of \\(\\times 1 98 \\pm 0 01\\) every two years from 1971 through 2019 In 2015, Moore revised his prediction to say Moores law should hold until 2025 [2] You can share these results as a zipped NumPy array file, mooreslaw_regression npz, or as another csv, mooreslaw_regression csv",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_48"
        },
        {
          "content": "The amazing progress in semiconductor manufacturing has enabled new industries and computational power This analysis should give you a small insight into how incredible this growth has been over the last half-century References Moores Law Wikipedia article Accessed Oct 1, 2020 Courtland, Rachel Gordon Moore: The Man Whose Name Means Progress IEEE Spectrum 30 Mar 2015 Transistor Count Wikipedia article Accessed Oct 1, 2020",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_49"
        },
        {
          "content": "previous NumPy Applications next Deep learning on MNIST Contents What youll do Skills youll learn What youll need Building Moores law as an exponential function Loading historical manufacturing data to your workspace Calculating the historical growth curve for transistors Sharing your results as zipped arrays and a csv Zipping the arrays into a file Creating your own comma separated value file Wrapping up References By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html",
          "library": "numpy",
          "chunk_id": "numpy_50"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
      "title": "Deep learning on MNIST \u2014 NumPy Tutorials",
      "content": "Binder Repository Open issue .ipynb .md .pdf Deep learning on MNIST Contents Prerequisites Table of contents 1. Load the MNIST dataset 2. Preprocess the data Convert the image data to the floating-point format Convert the labels to floating point through categorical/one-hot encoding 3. Build and train a small neural network from scratch Neural network building blocks with NumPy Model architecture and training summary Compose the model and begin training and testing it Next steps Deep learning on MNIST This tutorial demonstrates how to build a simple feedforward neural network (with one hidden layer) and train it from scratch with NumPy to recognize handwritten digit images. Your deep learning model  one of the most basic artificial neural networks that resembles the original multi-layer perceptron  will learn to classify digits from 0 to 9 from the MNIST dataset. The dataset contains 60,000 training and 10,000 test images and corresponding labels. Each training and test image is of size 784 (or 28x28 pixels)  this will be your input for the neural network. Based on the image inputs and their labels (supervised learning), your neural network will be trained to learn their features using forward propagation and backpropagation (reverse-mode differentiation). The final output of the network is a vector of 10 scores  one for each handwritten digit image. You will also evaluate how good your model is at classifying the images on the test set. This tutorial was adapted from the work by Andrew Trask (with the authors permission). Prerequisites The reader should have some knowledge of Python, NumPy array manipulation, and linear algebra. In addition, you should be familiar with main concepts of deep learning. To refresh the memory, you can take the Python and Linear algebra on n-dimensional arrays tutorials. You are advised to read the Deep learning paper published in 2015 by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, who are regarded as some of the pioneers of the field. You should also consider reading Andrew Trasks Grokking Deep Learning, which teaches deep learning with NumPy. In addition to NumPy, you will be utilizing the following Python standard modules for data loading and processing: urllib for URL handling request for URL opening gzip for gzip file decompression pickle to work with the pickle file format as well as: Matplotlib for data visualization This tutorial can be run locally in an isolated environment, such as Virtualenv or conda. You can use Jupyter Notebook or JupyterLab to run each notebook cell. Dont forget to set up NumPy and Matplotlib. Table of contents Load the MNIST dataset Preprocess the dataset Build and train a small neural network from scratch Next steps 1. Load the MNIST dataset In this section, you will download the zipped MNIST dataset files originally developed by Yann LeCuns research team. (More details of the MNIST dataset are available on Kaggle.) Then, you will transform them into 4 files of NumPy array type using built-in Python modules. Finally, you will split the arrays into training and test sets. 1. Define a variable to store the training/test image/label names of the MNIST dataset in a list: data_sources = { \"training_images\": \"train-images-idx3-ubyte.gz\",  60,000 training images. \"test_images\": \"t10k-images-idx3-ubyte.gz\",  10,000 test images. \"training_labels\": \"train-labels-idx1-ubyte.gz\",  60,000 training labels. \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",  10,000 test labels. } 2. Load the data. First check if the data is stored locally; if not, then download it. import requests import os data_dir = \"../_data\" os.makedirs(data_dir, exist_ok=True) base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\" for fname in data_sources.values(): fpath = os.path.join(data_dir, fname) if not os.path.exists(fpath): print(\"Downloading file: \" + fname) resp = requests.get(base_url + fname, stream=True, **request_opts) resp.raise_for_status()  Ensure download was succesful with open(fpath, \"wb\") as fh: for chunk in resp.iter_content(chunk_size=128): fh.write(chunk) 3. Decompress the 4 files and create 4 ndarrays, saving them into a dictionary. Each original image is of size 28x28 and neural networks normally expect a 1D vector input; therefore, you also need to reshape the images by multiplying 28 by 28 (784). import gzip import numpy as np mnist_dataset = {}  Images for key in (\"training_images\", \"test_images\"): with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file: mnist_dataset[key] = np.frombuffer( mnist_file.read(), np.uint8, offset=16 ).reshape(-1, 28 * 28)  Labels for key in (\"training_labels\", \"test_labels\"): with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file: mnist_dataset[key] = np.frombuffer(mnist_file.read(), np.uint8, offset=8) 4. Split the data into training and test sets using the standard notation of x for data and y for labels, calling the training and test set images x_train and x_test, and the labels y_train and y_test: x_train, y_train, x_test, y_test = ( mnist_dataset[\"training_images\"], mnist_dataset[\"training_labels\"], mnist_dataset[\"test_images\"], mnist_dataset[\"test_labels\"], ) 5. You can confirm that the shape of the image arrays is (60000, 784) and (10000, 784) for training and test sets, respectively, and the labels  (60000,) and (10000,): print( \"The shape of training images: {} and training labels: {}\".format( x_train.shape, y_train.shape ) ) print( \"The shape of test images: {} and test labels: {}\".format( x_test.shape, y_test.shape ) ) The shape of training images: (60000, 784) and training labels: (60000,) The shape of test images: (10000, 784) and test labels: (10000,) 6. And you can inspect some images using Matplotlib: import matplotlib.pyplot as plt  Take the 60,000th image (indexed at 59,999) from the training set,  reshape from (784, ) to (28, 28) to have a valid shape for displaying purposes. mnist_image = x_train[59999, :].reshape(28, 28)  Set the color mapping to grayscale to have a black background. plt.imshow(mnist_image, cmap=\"gray\")  Display the image. plt.show()  Display 5 random images from the training set. num_examples = 5 seed = 147197952744 rng = np.random.default_rng(seed) fig, axes = plt.subplots(1, num_examples) for sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes): ax.imshow(sample.reshape(28, 28), cmap=\"gray\") Above are five images taken from the MNIST training set. Various hand-drawn Arabic numerals are shown, with exact values chosen randomly with each run of the code. Note: You can also visualize a sample image as an array by printing x_train[59999]. Here, 59999 is your 60,000th training image sample (0 would be your first). Your output will be quite long and should contain an array of 8-bit integers: ... 0, 0, 38, 48, 48, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 62, 97, 198, 243, 254, 254, 212, 27, 0, 0, 0, 0, ...  Display the label of the 60,000th image (indexed at 59,999) from the training set. y_train[59999] np.uint8(8) 2. Preprocess the data Neural networks can work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type. When preprocessing the data, you should consider the following processes: vectorization and conversion to a floating-point format. Since the MNIST data is already vectorized and the arrays are of dtype uint8, your next challenge is to convert them to a floating-point format, such as float64 (double-precision): Normalizing the image data: a feature scaling procedure that can speed up the neural network training process by standardizing the distribution of your input data. One-hot/categorical encoding of the image labels. In practice, you can use different types of floating-point precision depending on your goals and you can find more information about that in the Nvidia and Google Cloud blog posts. Convert the image data to the floating-point format The images data contain 8-bit integers encoded in the [0, 255] interval with color values between 0 and 255. You will normalize them into floating-point arrays in the [0, 1] interval by dividing them by 255. 1. Check that the vectorized image data has type uint8: print(\"The data type of training images: {}\".format(x_train.dtype)) print(\"The data type of test images: {}\".format(x_test.dtype)) The data type of training images: uint8 The data type of test images: uint8 2. Normalize the arrays by dividing them by 255 (and thus promoting the data type from uint8 to float64) and then assign the train and test image data variables  x_train and x_test  to training_images and train_labels, respectively. To reduce the model training and evaluation time in this example, only a subset of the training and test images will be used. Both training_images and test_images will contain only 1,000 samples each out of the complete datasets of 60,000 and 10,000 images, respectively. These values can be controlled by changing the training_sample and test_sample below, up to their maximum values of 60,000 and 10,000. training_sample, test_sample = 1000, 1000 training_images = x_train[0:training_sample] / 255 test_images = x_test[0:test_sample] / 255 3. Confirm that the image data has changed to the floating-point format: print(\"The data type of training images: {}\".format(training_images.dtype)) print(\"The data type of test images: {}\".format(test_images.dtype)) The data type of training images: float64 The data type of test images: float64 Note: You can also check that normalization was successful by printing training_images[0] in a notebook cell. Your long output should contain an array of floating-point numbers: ... 0. , 0. , 0.01176471, 0.07058824, 0.07058824, 0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078, 0.65098039, 1. , 0.96862745, 0.49803922, 0. , ... Convert the labels to floating point through categorical/one-hot encoding You will use one-hot encoding to embed each digit label as an all-zero vector with np.zeros() and place 1 for a label index. As a result, your label data will be arrays with 1.0 (or 1.) in the position of each image label. Since there are 10 labels (from 0 to 9) in total, your arrays will look similar to this: array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]) 1. Confirm that the image label data are integers with dtype uint8: print(\"The data type of training labels: {}\".format(y_train.dtype)) print(\"The data type of test labels: {}\".format(y_test.dtype)) The data type of training labels: uint8 The data type of test labels: uint8 2. Define a function that performs one-hot encoding on arrays: def one_hot_encoding(labels, dimension=10):  Define a one-hot variable for an all-zero vector  with 10 dimensions (number labels from 0 to 9). one_hot_labels = labels[..., None] == np.arange(dimension)[None]  Return one-hot encoded labels. return one_hot_labels.astype(np.float64) 3. Encode the labels and assign the values to new variables: training_labels = one_hot_encoding(y_train[:training_sample]) test_labels = one_hot_encoding(y_test[:test_sample]) 4. Check that the data type has changed to floating point: print(\"The data type of training labels: {}\".format(training_labels.dtype)) print(\"The data type of test labels: {}\".format(test_labels.dtype)) The data type of training labels: float64 The data type of test labels: float64 5. Examine a few encoded labels: print(training_labels[0]) print(training_labels[1]) print(training_labels[2]) [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] and compare to the originals: print(y_train[0]) print(y_train[1]) print(y_train[2]) 5 0 4 You have finished preparing the dataset. 3. Build and train a small neural network from scratch In this section you will familiarize yourself with some high-level concepts of the basic building blocks of a deep learning model. You can refer to the original Deep learning research publication for more information. Afterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to identify handwritten digits from the MNIST dataset with a certain level of accuracy. Neural network building blocks with NumPy Layers: These building blocks work as data filters  they process data and learn representations from inputs to better predict the target outputs. You will use 1 hidden layer in your model to pass the inputs forward (forward propagation) and propagate the gradients/error derivatives of a loss function backward (backpropagation). These are input, hidden and output layers. In the hidden (middle) and output (last) layers, the neural network model will compute the weighted sum of inputs. To compute this process, you will use NumPys matrix multiplication function (the dot multiply or np.dot(layer, weights)). Note: For simplicity, the bias term is omitted in this example (there is no np.dot(layer, weights) + bias). Weights: These are important adjustable parameters that the neural network fine-tunes by forward and backward propagating the data. They are optimized through a process called gradient descent. Before the model training starts, the weights are randomly initialized with NumPys Generator.random(). The optimal weights should produce the highest prediction accuracy and the lowest error on the training and test sets. Activation function: Deep learning models are capable of determining non-linear relationships between inputs and outputs and these non-linear functions are usually applied to the output of each layer. You will use a rectified linear unit (ReLU) to the hidden layers output (for example, relu(np.dot(layer, weights)). Regularization: This technique helps prevent the neural network model from overfitting. In this example, you will use a method called dropout  dilution  that randomly sets a number of features in a layer to 0s. You will define it with NumPys Generator.integers() method and apply it to the hidden layer of the network. Loss function: The computation determines the quality of predictions by comparing the image labels (the truth) with the predicted values in the final layers output. For simplicity, you will use a basic total squared error using NumPys np.sum() function (for example, np.sum((final_layer_output - image_labels) ** 2)). Accuracy: This metric measures the accuracy of the networks ability to predict on the data it hasnt seen. Model architecture and training summary Here is a summary of the neural network model architecture and the training process: The input layer: It is the input for the network  the previously preprocessed data that is loaded from training_images into layer_0. The hidden (middle) layer: layer_1 takes the output from the previous layer and performs matrix-multiplication of the input by weights (weights_1) with NumPys np.dot()). Then, this output is passed through the ReLU activation function for non-linearity and then dropout is applied to help with overfitting. The output (last) layer: layer_2 ingests the output from layer_1 and repeats the same dot multiply process with weights_2. The final output returns 10 scores for each of the 0-9 digit labels. The network model ends with a size 10 layer  a 10-dimensional vector. Forward propagation, backpropagation, training loop: In the beginning of model training, your network randomly initializes the weights and feeds the input data forward through the hidden and output layers. This process is the forward pass or forward propagation. Then, the network propagates the signal from the loss function back through the hidden layer and adjusts the weights values with the help of the learning rate parameter (more on that later). Note: In more technical terms, you: Measure the error by comparing the real label of an image (the truth) with the prediction of the model. Differentiate the loss function. Ingest the gradients with the respect to the output, and backpropagate them with the respect to the inputs through the layer(s). Since the network contains tensor operations and weight matrices, backpropagation uses the chain rule. With each iteration (epoch) of the neural network training, this forward and backward propagation cycle adjusts the weights, which is reflected in the accuracy and error metrics. As you train the model, your goal is to minimize the error and maximize the accuracy on the training data, where the model learns from, as well as the test data, where you evaluate the model. Compose the model and begin training and testing it Having covered the main deep learning concepts and the neural network architecture, lets write the code. 1. Well start by creating a new random number generator, providing a seed for reproducibility: seed = 884736743 rng = np.random.default_rng(seed) 2. For the hidden layer, define the ReLU activation function for forward propagation and ReLUs derivative that will be used during backpropagation:  Define ReLU that returns the input if it's positive and 0 otherwise. def relu(x): return (x = 0) * x  Set up a derivative of the ReLU function that returns 1 for a positive input  and 0 otherwise. def relu2deriv(output): return output = 0 3. Set certain default values of hyperparameters, such as: Learning rate: learning_rate  helps limit the magnitude of weight updates to prevent them from overcorrecting. Epochs (iterations): epochs  the number of complete passes  forward and backward propagations  of the data through the network. This parameter can positively or negatively affect the results. The higher the iterations, the longer the learning process may take. Because this is a computationally intensive task, we have chosen a very low number of epochs (20). To get meaningful results, you should choose a much larger number. Size of the hidden (middle) layer in a network: hidden_size  different sizes of the hidden layer can affect the results during training and testing. Size of the input: pixels_per_image  you have established that the image input is 784 (28x28) (in pixels). Number of labels: num_labels  indicates the output number for the output layer where the predictions occur for 10 (0 to 9) handwritten digit labels. learning_rate = 0.005 epochs = 20 hidden_size = 100 pixels_per_image = 784 num_labels = 10 4. Initialize the weight vectors that will be used in the hidden and output layers with random values: weights_1 = 0.2 * rng.random((pixels_per_image, hidden_size)) - 0.1 weights_2 = 0.2 * rng.random((hidden_size, num_labels)) - 0.1 5. Set up the neural networks learning experiment with a training loop and start the training process. Note that the model is evaluated against the test set at each epoch to track its performance over the training epochs. Start the training process:  To store training and test set losses and accurate predictions  for visualization. store_training_loss = [] store_training_accurate_pred = [] store_test_loss = [] store_test_accurate_pred = []  This is a training loop.  Run the learning experiment for a defined number of epochs (iterations). for j in range(epochs):   Training step    Set the initial loss/error and the number of accurate predictions to zero. training_loss = 0.0 training_accurate_predictions = 0  For all images in the training set, perform a forward pass  and backpropagation and adjust the weights accordingly. for i in range(len(training_images)):  Forward propagation/forward pass:  1. The input layer:  Initialize the training image data as inputs. layer_0 = training_images[i]  2. The hidden layer:  Take in the training image data into the middle layer by  matrix-multiplying it by randomly initialized weights. layer_1 = np.dot(layer_0, weights_1)  3. Pass the hidden layer's output through the ReLU activation function. layer_1 = relu(layer_1)  4. Define the dropout function for regularization. dropout_mask = rng.integers(low=0, high=2, size=layer_1.shape)  5. Apply dropout to the hidden layer's output. layer_1 *= dropout_mask * 2  6. The output layer:  Ingest the output of the middle layer into the the final layer  by matrix-multiplying it by randomly initialized weights.  Produce a 10-dimension vector with 10 scores. layer_2 = np.dot(layer_1, weights_2)  Backpropagation/backward pass:  1. Measure the training error (loss function) between the actual  image labels (the truth) and the prediction by the model. training_loss += np.sum((training_labels[i] - layer_2) ** 2)  2. Increment the accurate prediction count. training_accurate_predictions += int( np.argmax(layer_2) == np.argmax(training_labels[i]) )  3. Differentiate the loss function/error. layer_2_delta = training_labels[i] - layer_2  4. Propagate the gradients of the loss function back through the hidden layer. layer_1_delta = np.dot(weights_2, layer_2_delta) * relu2deriv(layer_1)  5. Apply the dropout to the gradients. layer_1_delta *= dropout_mask  6. Update the weights for the middle and input layers  by multiplying them by the learning rate and the gradients. weights_1 += learning_rate * np.outer(layer_0, layer_1_delta) weights_2 += learning_rate * np.outer(layer_1, layer_2_delta)  Store training set losses and accurate predictions. store_training_loss.append(training_loss) store_training_accurate_pred.append(training_accurate_predictions)   Evaluation step    Evaluate model performance on the test set at each epoch.  Unlike the training step, the weights are not modified for each image  (or batch). Therefore the model can be applied to the test images in a  vectorized manner, eliminating the need to loop over each image  individually: results = relu(test_images  weights_1)  weights_2  Measure the error between the actual label (truth) and prediction values. test_loss = np.sum((test_labels - results) ** 2)  Measure prediction accuracy on test set test_accurate_predictions = np.sum( np.argmax(results, axis=1) == np.argmax(test_labels, axis=1) )  Store test set losses and accurate predictions. store_test_loss.append(test_loss) store_test_accurate_pred.append(test_accurate_predictions)  Summarize error and accuracy metrics at each epoch print( ( f\"Epoch: {j}\\n\" f\" Training set error: {training_loss / len(training_images):.3f}\\n\" f\" Training set accuracy: {training_accurate_predictions / len(training_images)}\\n\" f\" Test set error: {test_loss / len(test_images):.3f}\\n\" f\" Test set accuracy: {test_accurate_predictions / len(test_images)}\" ) ) Epoch: 0 Training set error: 0.898 Training set accuracy: 0.397 Test set error: 0.680 Test set accuracy: 0.582 Epoch: 1 Training set error: 0.656 Training set accuracy: 0.633 Test set error: 0.607 Test set accuracy: 0.641 Epoch: 2 Training set error: 0.592 Training set accuracy: 0.68 Test set error: 0.569 Test set accuracy: 0.679 Epoch: 3 Training set error: 0.556 Training set accuracy: 0.7 Test set error: 0.541 Test set accuracy: 0.708 Epoch: 4 Training set error: 0.534 Training set accuracy: 0.732 Test set error: 0.526 Test set accuracy: 0.729 Epoch: 5 Training set error: 0.515 Training set accuracy: 0.715 Test set error: 0.500 Test set accuracy: 0.739 Epoch: 6 Training set error: 0.495 Training set accuracy: 0.748 Test set error: 0.487 Test set accuracy: 0.753 Epoch: 7 Training set error: 0.483 Training set accuracy: 0.769 Test set error: 0.486 Test set accuracy: 0.747 Epoch: 8 Training set error: 0.473 Training set accuracy: 0.776 Test set error: 0.473 Test set accuracy: 0.752 Epoch: 9 Training set error: 0.460 Training set accuracy: 0.788 Test set error: 0.462 Test set accuracy: 0.762 Epoch: 10 Training set error: 0.465 Training set accuracy: 0.769 Test set error: 0.462 Test set accuracy: 0.767 Epoch: 11 Training set error: 0.443 Training set accuracy: 0.801 Test set error: 0.456 Test set accuracy: 0.775 Epoch: 12 Training set error: 0.448 Training set accuracy: 0.795 Test set error: 0.455 Test set accuracy: 0.772 Epoch: 13 Training set error: 0.438 Training set accuracy: 0.787 Test set error: 0.453 Test set accuracy: 0.778 Epoch: 14 Training set error: 0.446 Training set accuracy: 0.791 Test set error: 0.450 Test set accuracy: 0.779 Epoch: 15 Training set error: 0.441 Training set accuracy: 0.788 Test set error: 0.452 Test set accuracy: 0.772 Epoch: 16 Training set error: 0.437 Training set accuracy: 0.786 Test set error: 0.453 Test set accuracy: 0.772 Epoch: 17 Training set error: 0.436 Training set accuracy: 0.794 Test set error: 0.449 Test set accuracy: 0.778 Epoch: 18 Training set error: 0.433 Training set accuracy: 0.801 Test set error: 0.450 Test set accuracy: 0.774 Epoch: 19 Training set error: 0.429 Training set accuracy: 0.785 Test set error: 0.436 Test set accuracy: 0.784 The training process may take many minutes, depending on a number of factors, such as the processing power of the machine you are running the experiment on and the number of epochs. To reduce the waiting time, you can change the epoch (iteration) variable from 100 to a lower number, reset the runtime (which will reset the weights), and run the notebook cells again. After executing the cell above, you can visualize the training and test set errors and accuracy for an instance of this training process. epoch_range = np.arange(epochs) + 1  Starting from 1  The training set metrics. training_metrics = { \"accuracy\": np.asarray(store_training_accurate_pred) / len(training_images), \"error\": np.asarray(store_training_loss) / len(training_images), }  The test set metrics. test_metrics = { \"accuracy\": np.asarray(store_test_accurate_pred) / len(test_images), \"error\": np.asarray(store_test_loss) / len(test_images), }  Display the plots. fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5)) for ax, metrics, title in zip( axes, (training_metrics, test_metrics), (\"Training set\", \"Test set\") ):  Plot the metrics for metric, values in metrics.items(): ax.plot(epoch_range, values, label=metric.capitalize()) ax.set_title(title) ax.set_xlabel(\"Epochs\") ax.legend() plt.show() The training and testing error is shown above in the left and right plots, respectively. As the number of Epochs increases, the total error decreases and the accuracy increases. The accuracy rates that your model reaches during training and testing may be somewhat plausible but you may also find the error rates to be quite high. To reduce the error during training and testing, you can consider changing the simple loss function to, for example, categorical cross-entropy. Other possible solutions are discussed below. Next steps You have learned how to build and train a simple feed-forward neural network from scratch using just NumPy to classify handwritten MNIST digits. To further enhance and optimize your neural network model, you can consider one of a mixture of the following: Increase the training sample size from 1,000 to a higher number (up to 60,000). Use mini-batches and reduce the learning rate. Alter the architecture by introducing more hidden layers to make the network deeper. Combine the cross-entropy loss function with a softmax activation function in the last layer. Introduce convolutional layers: replace the feedforward network with a convolutional neural network architecture. Use a higher epoch size to train longer and add more regularization techniques, such as early stopping, to prevent overfitting. Introduce a validation set for an unbiased valuation of the model fit. Apply batch normalization for faster and more stable training. Tune other parameters, such as the learning rate and hidden layer size. Building a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning. However, for real-world applications you should use specialized frameworks  such as PyTorch, JAX, TensorFlow or MXNet  that provide NumPy-like APIs, have built-in automatic differentiation and GPU support, and are designed for high-performance numerical computing and machine learning. Finally, when developing a machine learning model, you should think about potential ethical issues and apply practices to avoid or mitigate those: Document a trained model with a Model Card - see the Model Cards for Model Reporting paper by Margaret Mitchell et al.. Document a dataset with a Datasheet - see the Datasheets for Datasets paper) by Timnit Gebru et al.. Consider the impact of your model - who is affected by it, who does it benefit - see the article and talk by Pratyusha Kalluri. For more resources, see this blog post by Rachel Thomas and the Radical AI podcast. (Credit to hsjeong5 for demonstrating how to download MNIST without the use of external libraries.) previous Determining Moores Law with real data in NumPy next X-ray image processing Contents Prerequisites Table of contents 1. Load the MNIST dataset 2. Preprocess the data Convert the image data to the floating-point format Convert the labels to floating point through categorical/one-hot encoding 3. Build and train a small neural network from scratch Neural network building blocks with NumPy Model architecture and training summary Compose the model and begin training and testing it Next steps By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "data_sources = {\n    \"training_images\": \"train-images-idx3-ubyte.gz\",  # 60,000 training images.\n    \"test_images\": \"t10k-images-idx3-ubyte.gz\",  # 10,000 test images.\n    \"training_labels\": \"train-labels-idx1-ubyte.gz\",  # 60,000 training labels.\n    \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",  # 10,000 test labels.\n}",
        "import requests\nimport os\n\ndata_dir = \"../_data\"\nos.makedirs(data_dir, exist_ok=True)\n\nbase_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n\nfor fname in data_sources.values():\n    fpath = os.path.join(data_dir, fname)\n    if not os.path.exists(fpath):\n        print(\"Downloading file: \" + fname)\n        resp = requests.get(base_url + fname, stream=True, **request_opts)\n        resp.raise_for_status()  # Ensure download was succesful\n        with open(fpath, \"wb\") as fh:\n            for chunk in resp.iter_content(chunk_size=128):\n                fh.write(chunk)",
        "import gzip\nimport numpy as np\n\nmnist_dataset = {}\n\n# Images\nfor key in (\"training_images\", \"test_images\"):\n    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n        mnist_dataset[key] = np.frombuffer(\n            mnist_file.read(), np.uint8, offset=16\n        ).reshape(-1, 28 * 28)\n# Labels\nfor key in (\"training_labels\", \"test_labels\"):\n    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n        mnist_dataset[key] = np.frombuffer(mnist_file.read(), np.uint8, offset=8)",
        "x_train, y_train, x_test, y_test = (\n    mnist_dataset[\"training_images\"],\n    mnist_dataset[\"training_labels\"],\n    mnist_dataset[\"test_images\"],\n    mnist_dataset[\"test_labels\"],\n)",
        "(60000, 784)",
        "(10000, 784)",
        "print(\n    \"The shape of training images: {} and training labels: {}\".format(\n        x_train.shape, y_train.shape\n    )\n)\nprint(\n    \"The shape of test images: {} and test labels: {}\".format(\n        x_test.shape, y_test.shape\n    )\n)",
        "The shape of training images: (60000, 784) and training labels: (60000,)\nThe shape of test images: (10000, 784) and test labels: (10000,)",
        "import matplotlib.pyplot as plt\n\n# Take the 60,000th image (indexed at 59,999) from the training set,\n# reshape from (784, ) to (28, 28) to have a valid shape for displaying purposes.\nmnist_image = x_train[59999, :].reshape(28, 28)\n# Set the color mapping to grayscale to have a black background.\nplt.imshow(mnist_image, cmap=\"gray\")\n# Display the image.\nplt.show()",
        "# Display 5 random images from the training set.\nnum_examples = 5\nseed = 147197952744\nrng = np.random.default_rng(seed)\n\nfig, axes = plt.subplots(1, num_examples)\nfor sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes):\n    ax.imshow(sample.reshape(28, 28), cmap=\"gray\")",
        "x_train[59999]",
        "...\n         0,   0,  38,  48,  48,  22,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,  62,  97, 198, 243, 254, 254, 212,  27,   0,   0,   0,   0,\n...",
        "# Display the label of the 60,000th image (indexed at 59,999) from the training set.\ny_train[59999]",
        "np.uint8(8)",
        "print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))",
        "The data type of training images: uint8\nThe data type of test images: uint8",
        "training_images",
        "train_labels",
        "training_images",
        "test_images",
        "training_sample",
        "test_sample",
        "training_sample, test_sample = 1000, 1000\ntraining_images = x_train[0:training_sample] / 255\ntest_images = x_test[0:test_sample] / 255",
        "print(\"The data type of training images: {}\".format(training_images.dtype))\nprint(\"The data type of test images: {}\".format(test_images.dtype))",
        "The data type of training images: float64\nThe data type of test images: float64",
        "training_images[0]",
        "...\n       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n...",
        "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])",
        "print(\"The data type of training labels: {}\".format(y_train.dtype))\nprint(\"The data type of test labels: {}\".format(y_test.dtype))",
        "The data type of training labels: uint8\nThe data type of test labels: uint8",
        "def one_hot_encoding(labels, dimension=10):\n    # Define a one-hot variable for an all-zero vector\n    # with 10 dimensions (number labels from 0 to 9).\n    one_hot_labels = labels[..., None] == np.arange(dimension)[None]\n    # Return one-hot encoded labels.\n    return one_hot_labels.astype(np.float64)",
        "training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])",
        "print(\"The data type of training labels: {}\".format(training_labels.dtype))\nprint(\"The data type of test labels: {}\".format(test_labels.dtype))",
        "The data type of training labels: float64\nThe data type of test labels: float64",
        "print(training_labels[0])\nprint(training_labels[1])\nprint(training_labels[2])",
        "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]",
        "print(y_train[0])\nprint(y_train[1])\nprint(y_train[2])",
        "np.dot(layer, weights)",
        "np.dot(layer, weights) + bias",
        "Generator.random()",
        "relu(np.dot(layer, weights))",
        "Generator.integers()",
        "np.sum((final_layer_output - image_labels) ** 2)",
        "training_images",
        "seed = 884736743\nrng = np.random.default_rng(seed)",
        "# Define ReLU that returns the input if it's positive and 0 otherwise.\ndef relu(x):\n    return (x >= 0) * x\n\n\n# Set up a derivative of the ReLU function that returns 1 for a positive input\n# and 0 otherwise.\ndef relu2deriv(output):\n    return output >= 0",
        "learning_rate",
        "hidden_size",
        "pixels_per_image",
        "learning_rate = 0.005\nepochs = 20\nhidden_size = 100\npixels_per_image = 784\nnum_labels = 10",
        "weights_1 = 0.2 * rng.random((pixels_per_image, hidden_size)) - 0.1\nweights_2 = 0.2 * rng.random((hidden_size, num_labels)) - 0.1",
        "# To store training and test set losses and accurate predictions\n# for visualization.\nstore_training_loss = []\nstore_training_accurate_pred = []\nstore_test_loss = []\nstore_test_accurate_pred = []\n\n# This is a training loop.\n# Run the learning experiment for a defined number of epochs (iterations).\nfor j in range(epochs):\n\n    #################\n    # Training step #\n    #################\n\n    # Set the initial loss/error and the number of accurate predictions to zero.\n    training_loss = 0.0\n    training_accurate_predictions = 0\n\n    # For all images in the training set, perform a forward pass\n    # and backpropagation and adjust the weights accordingly.\n    for i in range(len(training_images)):\n        # Forward propagation/forward pass:\n        # 1. The input layer:\n        #    Initialize the training image data as inputs.\n        layer_0 = training_images[i]\n        # 2. The hidden layer:\n        #    Take in the training image data into the middle layer by\n        #    matrix-multiplying it by randomly initialized weights.\n        layer_1 = np.dot(layer_0, weights_1)\n        # 3. Pass the hidden layer's output through the ReLU activation function.\n        layer_1 = relu(layer_1)\n        # 4. Define the dropout function for regularization.\n        dropout_mask = rng.integers(low=0, high=2, size=layer_1.shape)\n        # 5. Apply dropout to the hidden layer's output.\n        layer_1 *= dropout_mask * 2\n        # 6. The output layer:\n        #    Ingest the output of the middle layer into the the final layer\n        #    by matrix-multiplying it by randomly initialized weights.\n        #    Produce a 10-dimension vector with 10 scores.\n        layer_2 = np.dot(layer_1, weights_2)\n\n        # Backpropagation/backward pass:\n        # 1. Measure the training error (loss function) between the actual\n        #    image labels (the truth) and the prediction by the model.\n        training_loss += np.sum((training_labels[i] - layer_2) ** 2)\n        # 2. Increment the accurate prediction count.\n        training_accurate_predictions += int(\n            np.argmax(layer_2) == np.argmax(training_labels[i])\n        )\n        # 3. Differentiate the loss function/error.\n        layer_2_delta = training_labels[i] - layer_2\n        # 4. Propagate the gradients of the loss function back through the hidden layer.\n        layer_1_delta = np.dot(weights_2, layer_2_delta) * relu2deriv(layer_1)\n        # 5. Apply the dropout to the gradients.\n        layer_1_delta *= dropout_mask\n        # 6. Update the weights for the middle and input layers\n        #    by multiplying them by the learning rate and the gradients.\n        weights_1 += learning_rate * np.outer(layer_0, layer_1_delta)\n        weights_2 += learning_rate * np.outer(layer_1, layer_2_delta)\n\n    # Store training set losses and accurate predictions.\n    store_training_loss.append(training_loss)\n    store_training_accurate_pred.append(training_accurate_predictions)\n\n    ###################\n    # Evaluation step #\n    ###################\n\n    # Evaluate model performance on the test set at each epoch.\n\n    # Unlike the training step, the weights are not modified for each image\n    # (or batch). Therefore the model can be applied to the test images in a\n    # vectorized manner, eliminating the need to loop over each image\n    # individually:\n\n    results = relu(test_images @ weights_1) @ weights_2\n\n    # Measure the error between the actual label (truth) and prediction values.\n    test_loss = np.sum((test_labels - results) ** 2)\n\n    # Measure prediction accuracy on test set\n    test_accurate_predictions = np.sum(\n        np.argmax(results, axis=1) == np.argmax(test_labels, axis=1)\n    )\n\n    # Store test set losses and accurate predictions.\n    store_test_loss.append(test_loss)\n    store_test_accurate_pred.append(test_accurate_predictions)\n\n    # Summarize error and accuracy metrics at each epoch\n    print(\n        (\n            f\"Epoch: {j}\\n\"\n            f\"  Training set error: {training_loss / len(training_images):.3f}\\n\"\n            f\"  Training set accuracy: {training_accurate_predictions / len(training_images)}\\n\"\n            f\"  Test set error: {test_loss / len(test_images):.3f}\\n\"\n            f\"  Test set accuracy: {test_accurate_predictions / len(test_images)}\"\n        )\n    )",
        "Epoch: 0\n  Training set error: 0.898\n  Training set accuracy: 0.397\n  Test set error: 0.680\n  Test set accuracy: 0.582",
        "Epoch: 1\n  Training set error: 0.656\n  Training set accuracy: 0.633\n  Test set error: 0.607\n  Test set accuracy: 0.641",
        "Epoch: 2\n  Training set error: 0.592\n  Training set accuracy: 0.68\n  Test set error: 0.569\n  Test set accuracy: 0.679",
        "Epoch: 3\n  Training set error: 0.556\n  Training set accuracy: 0.7\n  Test set error: 0.541\n  Test set accuracy: 0.708",
        "Epoch: 4\n  Training set error: 0.534\n  Training set accuracy: 0.732\n  Test set error: 0.526\n  Test set accuracy: 0.729",
        "Epoch: 5\n  Training set error: 0.515\n  Training set accuracy: 0.715\n  Test set error: 0.500\n  Test set accuracy: 0.739",
        "Epoch: 6\n  Training set error: 0.495\n  Training set accuracy: 0.748\n  Test set error: 0.487\n  Test set accuracy: 0.753",
        "Epoch: 7\n  Training set error: 0.483\n  Training set accuracy: 0.769\n  Test set error: 0.486\n  Test set accuracy: 0.747",
        "Epoch: 8\n  Training set error: 0.473\n  Training set accuracy: 0.776\n  Test set error: 0.473\n  Test set accuracy: 0.752",
        "Epoch: 9\n  Training set error: 0.460\n  Training set accuracy: 0.788\n  Test set error: 0.462\n  Test set accuracy: 0.762",
        "Epoch: 10\n  Training set error: 0.465\n  Training set accuracy: 0.769\n  Test set error: 0.462\n  Test set accuracy: 0.767",
        "Epoch: 11\n  Training set error: 0.443\n  Training set accuracy: 0.801\n  Test set error: 0.456\n  Test set accuracy: 0.775",
        "Epoch: 12\n  Training set error: 0.448\n  Training set accuracy: 0.795\n  Test set error: 0.455\n  Test set accuracy: 0.772",
        "Epoch: 13\n  Training set error: 0.438\n  Training set accuracy: 0.787\n  Test set error: 0.453\n  Test set accuracy: 0.778",
        "Epoch: 14\n  Training set error: 0.446\n  Training set accuracy: 0.791\n  Test set error: 0.450\n  Test set accuracy: 0.779",
        "Epoch: 15\n  Training set error: 0.441\n  Training set accuracy: 0.788\n  Test set error: 0.452\n  Test set accuracy: 0.772",
        "Epoch: 16\n  Training set error: 0.437\n  Training set accuracy: 0.786\n  Test set error: 0.453\n  Test set accuracy: 0.772",
        "Epoch: 17\n  Training set error: 0.436\n  Training set accuracy: 0.794\n  Test set error: 0.449\n  Test set accuracy: 0.778",
        "Epoch: 18\n  Training set error: 0.433\n  Training set accuracy: 0.801\n  Test set error: 0.450\n  Test set accuracy: 0.774",
        "Epoch: 19\n  Training set error: 0.429\n  Training set accuracy: 0.785\n  Test set error: 0.436\n  Test set accuracy: 0.784",
        "epoch_range = np.arange(epochs) + 1  # Starting from 1\n\n# The training set metrics.\ntraining_metrics = {\n    \"accuracy\": np.asarray(store_training_accurate_pred) / len(training_images),\n    \"error\": np.asarray(store_training_loss) / len(training_images),\n}\n\n# The test set metrics.\ntest_metrics = {\n    \"accuracy\": np.asarray(store_test_accurate_pred) / len(test_images),\n    \"error\": np.asarray(store_test_loss) / len(test_images),\n}\n\n# Display the plots.\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\nfor ax, metrics, title in zip(\n    axes, (training_metrics, test_metrics), (\"Training set\", \"Test set\")\n):\n    # Plot the metrics\n    for metric, values in metrics.items():\n        ax.plot(epoch_range, values, label=metric.capitalize())\n    ax.set_title(title)\n    ax.set_xlabel(\"Epochs\")\n    ax.legend()\nplt.show()"
      ],
      "chunks": [
        {
          "content": "Binder Repository Open issue ipynb md pdf Deep learning on MNIST Contents Prerequisites Table of contents 1 Load the MNIST dataset 2 Preprocess the data Convert the image data to the floating-point format Convert the labels to floating point through categorical/one-hot encoding 3",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Build and train a small neural network from scratch Neural network building blocks with NumPy Model architecture and training summary Compose the model and begin training and testing it Next steps Deep learning on MNIST This tutorial demonstrates how to build a simple feedforward neural network (with one hidden layer) and train it from scratch with NumPy to recognize handwritten digit images",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "Your deep learning model  one of the most basic artificial neural networks that resembles the original multi-layer perceptron  will learn to classify digits from 0 to 9 from the MNIST dataset The dataset contains 60,000 training and 10,000 test images and corresponding labels Each training and test image is of size 784 (or 28x28 pixels)  this will be your input for the neural network",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "Based on the image inputs and their labels (supervised learning), your neural network will be trained to learn their features using forward propagation and backpropagation (reverse-mode differentiation) The final output of the network is a vector of 10 scores  one for each handwritten digit image You will also evaluate how good your model is at classifying the images on the test set This tutorial was adapted from the work by Andrew Trask (with the authors permission)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "Prerequisites The reader should have some knowledge of Python, NumPy array manipulation, and linear algebra In addition, you should be familiar with main concepts of deep learning To refresh the memory, you can take the Python and Linear algebra on n-dimensional arrays tutorials You are advised to read the Deep learning paper published in 2015 by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, who are regarded as some of the pioneers of the field",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "You should also consider reading Andrew Trasks Grokking Deep Learning, which teaches deep learning with NumPy In addition to NumPy, you will be utilizing the following Python standard modules for data loading and processing: urllib for URL handling request for URL opening gzip for gzip file decompression pickle to work with the pickle file format as well as: Matplotlib for data visualization This tutorial can be run locally in an isolated environment, such as Virtualenv or conda",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "You can use Jupyter Notebook or JupyterLab to run each notebook cell Dont forget to set up NumPy and Matplotlib Table of contents Load the MNIST dataset Preprocess the dataset Build and train a small neural network from scratch Next steps 1 Load the MNIST dataset In this section, you will download the zipped MNIST dataset files originally developed by Yann LeCuns research team (More details of the MNIST dataset are available on Kaggle",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": ") Then, you will transform them into 4 files of NumPy array type using built-in Python modules Finally, you will split the arrays into training and test sets 1 Define a variable to store the training/test image/label names of the MNIST dataset in a list: data_sources = { \"training_images\": \"train-images-idx3-ubyte gz\",  60,000 training images \"test_images\": \"t10k-images-idx3-ubyte gz\",  10,000 test images \"training_labels\": \"train-labels-idx1-ubyte gz\",  60,000 training labels",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "\"test_labels\": \"t10k-labels-idx1-ubyte gz\",  10,000 test labels } 2 Load the data First check if the data is stored locally; if not, then download it import requests import os data_dir = \" /_data\" os makedirs(data_dir, exist_ok=True) base_url = \"https://ossci-datasets s3 amazonaws com/mnist/\" for fname in data_sources values(): fpath = os path join(data_dir, fname) if not os path exists(fpath): print(\"Downloading file: \" + fname) resp = requests",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "get(base_url + fname, stream=True, **request_opts) resp raise_for_status()  Ensure download was succesful with open(fpath, \"wb\") as fh: for chunk in resp iter_content(chunk_size=128): fh write(chunk) 3 Decompress the 4 files and create 4 ndarrays, saving them into a dictionary Each original image is of size 28x28 and neural networks normally expect a 1D vector input; therefore, you also need to reshape the images by multiplying 28 by 28 (784)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "import gzip import numpy as np mnist_dataset = {}  Images for key in (\"training_images\", \"test_images\"): with gzip open(os path join(data_dir, data_sources[key]), \"rb\") as mnist_file: mnist_dataset[key] = np frombuffer( mnist_file read(), np uint8, offset=16 ) reshape(-1, 28 * 28)  Labels for key in (\"training_labels\", \"test_labels\"): with gzip open(os path join(data_dir, data_sources[key]), \"rb\") as mnist_file: mnist_dataset[key] = np frombuffer(mnist_file read(), np uint8, offset=8) 4",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "Split the data into training and test sets using the standard notation of x for data and y for labels, calling the training and test set images x_train and x_test, and the labels y_train and y_test: x_train, y_train, x_test, y_test = ( mnist_dataset[\"training_images\"], mnist_dataset[\"training_labels\"], mnist_dataset[\"test_images\"], mnist_dataset[\"test_labels\"], ) 5",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "You can confirm that the shape of the image arrays is (60000, 784) and (10000, 784) for training and test sets, respectively, and the labels  (60000,) and (10000,): print( \"The shape of training images: {} and training labels: {}\" format( x_train shape, y_train shape ) ) print( \"The shape of test images: {} and test labels: {}\" format( x_test shape, y_test",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "shape ) ) The shape of training images: (60000, 784) and training labels: (60000,) The shape of test images: (10000, 784) and test labels: (10000,) 6 And you can inspect some images using Matplotlib: import matplotlib pyplot as plt  Take the 60,000th image (indexed at 59,999) from the training set,  reshape from (784, ) to (28, 28) to have a valid shape for displaying purposes mnist_image = x_train[59999, :] reshape(28, 28)  Set the color mapping to grayscale to have a black background plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "imshow(mnist_image, cmap=\"gray\")  Display the image plt show()  Display 5 random images from the training set num_examples = 5 seed = 147197952744 rng = np random default_rng(seed) fig, axes = plt subplots(1, num_examples) for sample, ax in zip(rng choice(x_train, size=num_examples, replace=False), axes): ax imshow(sample reshape(28, 28), cmap=\"gray\") Above are five images taken from the MNIST training set",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "Various hand-drawn Arabic numerals are shown, with exact values chosen randomly with each run of the code Note: You can also visualize a sample image as an array by printing x_train[59999] Here, 59999 is your 60,000th training image sample (0 would be your first) Your output will be quite long and should contain an array of 8-bit integers: 0, 0, 38, 48, 48, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 62, 97, 198, 243, 254, 254, 212, 27, 0, 0, 0, 0,",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "Display the label of the 60,000th image (indexed at 59,999) from the training set y_train[59999] np uint8(8) 2 Preprocess the data Neural networks can work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type When preprocessing the data, you should consider the following processes: vectorization and conversion to a floating-point format",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Since the MNIST data is already vectorized and the arrays are of dtype uint8, your next challenge is to convert them to a floating-point format, such as float64 (double-precision): Normalizing the image data: a feature scaling procedure that can speed up the neural network training process by standardizing the distribution of your input data One-hot/categorical encoding of the image labels",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "In practice, you can use different types of floating-point precision depending on your goals and you can find more information about that in the Nvidia and Google Cloud blog posts Convert the image data to the floating-point format The images data contain 8-bit integers encoded in the [0, 255] interval with color values between 0 and 255 You will normalize them into floating-point arrays in the [0, 1] interval by dividing them by 255 1",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "Check that the vectorized image data has type uint8: print(\"The data type of training images: {}\" format(x_train dtype)) print(\"The data type of test images: {}\" format(x_test dtype)) The data type of training images: uint8 The data type of test images: uint8 2 Normalize the arrays by dividing them by 255 (and thus promoting the data type from uint8 to float64) and then assign the train and test image data variables  x_train and x_test  to training_images and train_labels, respectively",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "To reduce the model training and evaluation time in this example, only a subset of the training and test images will be used Both training_images and test_images will contain only 1,000 samples each out of the complete datasets of 60,000 and 10,000 images, respectively These values can be controlled by changing the training_sample and test_sample below, up to their maximum values of 60,000 and 10,000",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "training_sample, test_sample = 1000, 1000 training_images = x_train[0:training_sample] / 255 test_images = x_test[0:test_sample] / 255 3 Confirm that the image data has changed to the floating-point format: print(\"The data type of training images: {}\" format(training_images dtype)) print(\"The data type of test images: {}\" format(test_images",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "dtype)) The data type of training images: float64 The data type of test images: float64 Note: You can also check that normalization was successful by printing training_images[0] in a notebook cell Your long output should contain an array of floating-point numbers: 0 , 0 , 0 01176471, 0 07058824, 0 07058824, 0 07058824, 0 49411765, 0 53333333, 0 68627451, 0 10196078, 0 65098039, 1 , 0 96862745, 0 49803922, 0 ,",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "Convert the labels to floating point through categorical/one-hot encoding You will use one-hot encoding to embed each digit label as an all-zero vector with np zeros() and place 1 for a label index As a result, your label data will be arrays with 1 0 (or 1 ) in the position of each image label Since there are 10 labels (from 0 to 9) in total, your arrays will look similar to this: array([0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ]) 1",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "Confirm that the image label data are integers with dtype uint8: print(\"The data type of training labels: {}\" format(y_train dtype)) print(\"The data type of test labels: {}\" format(y_test dtype)) The data type of training labels: uint8 The data type of test labels: uint8 2 Define a function that performs one-hot encoding on arrays: def one_hot_encoding(labels, dimension=10):  Define a one-hot variable for an all-zero vector  with 10 dimensions (number labels from 0 to 9) one_hot_labels = labels[",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": ", None] == np arange(dimension)[None]  Return one-hot encoded labels return one_hot_labels astype(np float64) 3 Encode the labels and assign the values to new variables: training_labels = one_hot_encoding(y_train[:training_sample]) test_labels = one_hot_encoding(y_test[:test_sample]) 4 Check that the data type has changed to floating point: print(\"The data type of training labels: {}\" format(training_labels dtype)) print(\"The data type of test labels: {}\" format(test_labels",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "dtype)) The data type of training labels: float64 The data type of test labels: float64 5 Examine a few encoded labels: print(training_labels[0]) print(training_labels[1]) print(training_labels[2]) [0 0 0 0 0 1 0 0 0 0 ] [1 0 0 0 0 0 0 0 0 0 ] [0 0 0 0 1 0 0 0 0 0 ] and compare to the originals: print(y_train[0]) print(y_train[1]) print(y_train[2]) 5 0 4 You have finished preparing the dataset 3",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "Build and train a small neural network from scratch In this section you will familiarize yourself with some high-level concepts of the basic building blocks of a deep learning model You can refer to the original Deep learning research publication for more information Afterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to identify handwritten digits from the MNIST dataset with a certain level of accuracy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "Neural network building blocks with NumPy Layers: These building blocks work as data filters  they process data and learn representations from inputs to better predict the target outputs You will use 1 hidden layer in your model to pass the inputs forward (forward propagation) and propagate the gradients/error derivatives of a loss function backward (backpropagation) These are input, hidden and output layers",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "In the hidden (middle) and output (last) layers, the neural network model will compute the weighted sum of inputs To compute this process, you will use NumPys matrix multiplication function (the dot multiply or np dot(layer, weights)) Note: For simplicity, the bias term is omitted in this example (there is no np dot(layer, weights) + bias) Weights: These are important adjustable parameters that the neural network fine-tunes by forward and backward propagating the data",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "They are optimized through a process called gradient descent Before the model training starts, the weights are randomly initialized with NumPys Generator random() The optimal weights should produce the highest prediction accuracy and the lowest error on the training and test sets Activation function: Deep learning models are capable of determining non-linear relationships between inputs and outputs and these non-linear functions are usually applied to the output of each layer",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "You will use a rectified linear unit (ReLU) to the hidden layers output (for example, relu(np dot(layer, weights)) Regularization: This technique helps prevent the neural network model from overfitting In this example, you will use a method called dropout  dilution  that randomly sets a number of features in a layer to 0s You will define it with NumPys Generator integers() method and apply it to the hidden layer of the network",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "Loss function: The computation determines the quality of predictions by comparing the image labels (the truth) with the predicted values in the final layers output For simplicity, you will use a basic total squared error using NumPys np sum() function (for example, np sum((final_layer_output - image_labels) ** 2)) Accuracy: This metric measures the accuracy of the networks ability to predict on the data it hasnt seen",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": "Model architecture and training summary Here is a summary of the neural network model architecture and the training process: The input layer: It is the input for the network  the previously preprocessed data that is loaded from training_images into layer_0 The hidden (middle) layer: layer_1 takes the output from the previous layer and performs matrix-multiplication of the input by weights (weights_1) with NumPys np dot())",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": "Then, this output is passed through the ReLU activation function for non-linearity and then dropout is applied to help with overfitting The output (last) layer: layer_2 ingests the output from layer_1 and repeats the same dot multiply process with weights_2 The final output returns 10 scores for each of the 0-9 digit labels The network model ends with a size 10 layer  a 10-dimensional vector",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "Forward propagation, backpropagation, training loop: In the beginning of model training, your network randomly initializes the weights and feeds the input data forward through the hidden and output layers This process is the forward pass or forward propagation Then, the network propagates the signal from the loss function back through the hidden layer and adjusts the weights values with the help of the learning rate parameter (more on that later)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "Note: In more technical terms, you: Measure the error by comparing the real label of an image (the truth) with the prediction of the model Differentiate the loss function Ingest the gradients with the respect to the output, and backpropagate them with the respect to the inputs through the layer(s) Since the network contains tensor operations and weight matrices, backpropagation uses the chain rule",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "With each iteration (epoch) of the neural network training, this forward and backward propagation cycle adjusts the weights, which is reflected in the accuracy and error metrics As you train the model, your goal is to minimize the error and maximize the accuracy on the training data, where the model learns from, as well as the test data, where you evaluate the model",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_37"
        },
        {
          "content": "Compose the model and begin training and testing it Having covered the main deep learning concepts and the neural network architecture, lets write the code 1 Well start by creating a new random number generator, providing a seed for reproducibility: seed = 884736743 rng = np random default_rng(seed) 2",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_38"
        },
        {
          "content": "For the hidden layer, define the ReLU activation function for forward propagation and ReLUs derivative that will be used during backpropagation:  Define ReLU that returns the input if it's positive and 0 otherwise def relu(x): return (x = 0) * x  Set up a derivative of the ReLU function that returns 1 for a positive input  and 0 otherwise def relu2deriv(output): return output = 0 3",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_39"
        },
        {
          "content": "Set certain default values of hyperparameters, such as: Learning rate: learning_rate  helps limit the magnitude of weight updates to prevent them from overcorrecting Epochs (iterations): epochs  the number of complete passes  forward and backward propagations  of the data through the network This parameter can positively or negatively affect the results The higher the iterations, the longer the learning process may take",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_40"
        },
        {
          "content": "Because this is a computationally intensive task, we have chosen a very low number of epochs (20) To get meaningful results, you should choose a much larger number Size of the hidden (middle) layer in a network: hidden_size  different sizes of the hidden layer can affect the results during training and testing Size of the input: pixels_per_image  you have established that the image input is 784 (28x28) (in pixels)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_41"
        },
        {
          "content": "Number of labels: num_labels  indicates the output number for the output layer where the predictions occur for 10 (0 to 9) handwritten digit labels learning_rate = 0 005 epochs = 20 hidden_size = 100 pixels_per_image = 784 num_labels = 10 4 Initialize the weight vectors that will be used in the hidden and output layers with random values: weights_1 = 0 2 * rng random((pixels_per_image, hidden_size)) - 0 1 weights_2 = 0 2 * rng random((hidden_size, num_labels)) - 0 1 5",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_42"
        },
        {
          "content": "Set up the neural networks learning experiment with a training loop and start the training process Note that the model is evaluated against the test set at each epoch to track its performance over the training epochs Start the training process:  To store training and test set losses and accurate predictions  for visualization store_training_loss = [] store_training_accurate_pred = [] store_test_loss = [] store_test_accurate_pred = []  This is a training loop",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_43"
        },
        {
          "content": "Run the learning experiment for a defined number of epochs (iterations) for j in range(epochs):   Training step    Set the initial loss/error and the number of accurate predictions to zero training_loss = 0 0 training_accurate_predictions = 0  For all images in the training set, perform a forward pass  and backpropagation and adjust the weights accordingly for i in range(len(training_images)):  Forward propagation/forward pass:  1 The input layer:  Initialize the training image data as inputs",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_44"
        },
        {
          "content": "layer_0 = training_images[i]  2 The hidden layer:  Take in the training image data into the middle layer by  matrix-multiplying it by randomly initialized weights layer_1 = np dot(layer_0, weights_1)  3 Pass the hidden layer's output through the ReLU activation function layer_1 = relu(layer_1)  4 Define the dropout function for regularization dropout_mask = rng integers(low=0, high=2, size=layer_1 shape)  5 Apply dropout to the hidden layer's output layer_1 *= dropout_mask * 2  6",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_45"
        },
        {
          "content": "The output layer:  Ingest the output of the middle layer into the the final layer  by matrix-multiplying it by randomly initialized weights Produce a 10-dimension vector with 10 scores layer_2 = np dot(layer_1, weights_2)  Backpropagation/backward pass:  1 Measure the training error (loss function) between the actual  image labels (the truth) and the prediction by the model training_loss += np sum((training_labels[i] - layer_2) ** 2)  2 Increment the accurate prediction count",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_46"
        },
        {
          "content": "training_accurate_predictions += int( np argmax(layer_2) == np argmax(training_labels[i]) )  3 Differentiate the loss function/error layer_2_delta = training_labels[i] - layer_2  4 Propagate the gradients of the loss function back through the hidden layer layer_1_delta = np dot(weights_2, layer_2_delta) * relu2deriv(layer_1)  5 Apply the dropout to the gradients layer_1_delta *= dropout_mask  6",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_47"
        },
        {
          "content": "Update the weights for the middle and input layers  by multiplying them by the learning rate and the gradients weights_1 += learning_rate * np outer(layer_0, layer_1_delta) weights_2 += learning_rate * np outer(layer_1, layer_2_delta)  Store training set losses and accurate predictions store_training_loss append(training_loss) store_training_accurate_pred append(training_accurate_predictions)   Evaluation step    Evaluate model performance on the test set at each epoch",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_48"
        },
        {
          "content": "Unlike the training step, the weights are not modified for each image  (or batch) Therefore the model can be applied to the test images in a  vectorized manner, eliminating the need to loop over each image  individually: results = relu(test_images  weights_1)  weights_2  Measure the error between the actual label (truth) and prediction values test_loss = np sum((test_labels - results) ** 2)  Measure prediction accuracy on test set test_accurate_predictions = np sum( np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_49"
        },
        {
          "content": "argmax(results, axis=1) == np argmax(test_labels, axis=1) )  Store test set losses and accurate predictions store_test_loss append(test_loss) store_test_accurate_pred append(test_accurate_predictions)  Summarize error and accuracy metrics at each epoch print( ( f\"Epoch: {j}\\n\" f\" Training set error: {training_loss / len(training_images): 3f}\\n\" f\" Training set accuracy: {training_accurate_predictions / len(training_images)}\\n\" f\" Test set error: {test_loss / len(test_images):",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_50"
        },
        {
          "content": "3f}\\n\" f\" Test set accuracy: {test_accurate_predictions / len(test_images)}\" ) ) Epoch: 0 Training set error: 0 898 Training set accuracy: 0 397 Test set error: 0 680 Test set accuracy: 0 582 Epoch: 1 Training set error: 0 656 Training set accuracy: 0 633 Test set error: 0 607 Test set accuracy: 0 641 Epoch: 2 Training set error: 0 592 Training set accuracy: 0 68 Test set error: 0 569 Test set accuracy: 0 679 Epoch: 3 Training set error: 0 556 Training set accuracy: 0 7 Test set error: 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_51"
        },
        {
          "content": "541 Test set accuracy: 0 708 Epoch: 4 Training set error: 0 534 Training set accuracy: 0 732 Test set error: 0 526 Test set accuracy: 0 729 Epoch: 5 Training set error: 0 515 Training set accuracy: 0 715 Test set error: 0 500 Test set accuracy: 0 739 Epoch: 6 Training set error: 0 495 Training set accuracy: 0 748 Test set error: 0 487 Test set accuracy: 0 753 Epoch: 7 Training set error: 0 483 Training set accuracy: 0 769 Test set error: 0 486 Test set accuracy: 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_52"
        },
        {
          "content": "747 Epoch: 8 Training set error: 0 473 Training set accuracy: 0 776 Test set error: 0 473 Test set accuracy: 0 752 Epoch: 9 Training set error: 0 460 Training set accuracy: 0 788 Test set error: 0 462 Test set accuracy: 0 762 Epoch: 10 Training set error: 0 465 Training set accuracy: 0 769 Test set error: 0 462 Test set accuracy: 0 767 Epoch: 11 Training set error: 0 443 Training set accuracy: 0 801 Test set error: 0 456 Test set accuracy: 0 775 Epoch: 12 Training set error: 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_53"
        },
        {
          "content": "448 Training set accuracy: 0 795 Test set error: 0 455 Test set accuracy: 0 772 Epoch: 13 Training set error: 0 438 Training set accuracy: 0 787 Test set error: 0 453 Test set accuracy: 0 778 Epoch: 14 Training set error: 0 446 Training set accuracy: 0 791 Test set error: 0 450 Test set accuracy: 0 779 Epoch: 15 Training set error: 0 441 Training set accuracy: 0 788 Test set error: 0 452 Test set accuracy: 0 772 Epoch: 16 Training set error: 0 437 Training set accuracy: 0 786 Test set error: 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_54"
        },
        {
          "content": "453 Test set accuracy: 0 772 Epoch: 17 Training set error: 0 436 Training set accuracy: 0 794 Test set error: 0 449 Test set accuracy: 0 778 Epoch: 18 Training set error: 0 433 Training set accuracy: 0 801 Test set error: 0 450 Test set accuracy: 0 774 Epoch: 19 Training set error: 0 429 Training set accuracy: 0 785 Test set error: 0 436 Test set accuracy: 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_55"
        },
        {
          "content": "784 The training process may take many minutes, depending on a number of factors, such as the processing power of the machine you are running the experiment on and the number of epochs To reduce the waiting time, you can change the epoch (iteration) variable from 100 to a lower number, reset the runtime (which will reset the weights), and run the notebook cells again",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_56"
        },
        {
          "content": "After executing the cell above, you can visualize the training and test set errors and accuracy for an instance of this training process epoch_range = np arange(epochs) + 1  Starting from 1  The training set metrics training_metrics = { \"accuracy\": np asarray(store_training_accurate_pred) / len(training_images), \"error\": np asarray(store_training_loss) / len(training_images), }  The test set metrics test_metrics = { \"accuracy\": np asarray(store_test_accurate_pred) / len(test_images), \"error\": np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_57"
        },
        {
          "content": "asarray(store_test_loss) / len(test_images), }  Display the plots fig, axes = plt subplots(nrows=1, ncols=2, figsize=(15, 5)) for ax, metrics, title in zip( axes, (training_metrics, test_metrics), (\"Training set\", \"Test set\") ):  Plot the metrics for metric, values in metrics items(): ax plot(epoch_range, values, label=metric capitalize()) ax set_title(title) ax set_xlabel(\"Epochs\") ax legend() plt show() The training and testing error is shown above in the left and right plots, respectively",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_58"
        },
        {
          "content": "As the number of Epochs increases, the total error decreases and the accuracy increases The accuracy rates that your model reaches during training and testing may be somewhat plausible but you may also find the error rates to be quite high To reduce the error during training and testing, you can consider changing the simple loss function to, for example, categorical cross-entropy Other possible solutions are discussed below",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_59"
        },
        {
          "content": "Next steps You have learned how to build and train a simple feed-forward neural network from scratch using just NumPy to classify handwritten MNIST digits To further enhance and optimize your neural network model, you can consider one of a mixture of the following: Increase the training sample size from 1,000 to a higher number (up to 60,000) Use mini-batches and reduce the learning rate Alter the architecture by introducing more hidden layers to make the network deeper",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_60"
        },
        {
          "content": "Combine the cross-entropy loss function with a softmax activation function in the last layer Introduce convolutional layers: replace the feedforward network with a convolutional neural network architecture Use a higher epoch size to train longer and add more regularization techniques, such as early stopping, to prevent overfitting Introduce a validation set for an unbiased valuation of the model fit Apply batch normalization for faster and more stable training",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_61"
        },
        {
          "content": "Tune other parameters, such as the learning rate and hidden layer size Building a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning However, for real-world applications you should use specialized frameworks  such as PyTorch, JAX, TensorFlow or MXNet  that provide NumPy-like APIs, have built-in automatic differentiation and GPU support, and are designed for high-performance numerical computing and machine learning",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_62"
        },
        {
          "content": "Finally, when developing a machine learning model, you should think about potential ethical issues and apply practices to avoid or mitigate those: Document a trained model with a Model Card - see the Model Cards for Model Reporting paper by Margaret Mitchell et al Document a dataset with a Datasheet - see the Datasheets for Datasets paper) by Timnit Gebru et al Consider the impact of your model - who is affected by it, who does it benefit - see the article and talk by Pratyusha Kalluri",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_63"
        },
        {
          "content": "For more resources, see this blog post by Rachel Thomas and the Radical AI podcast (Credit to hsjeong5 for demonstrating how to download MNIST without the use of external libraries ) previous Determining Moores Law with real data in NumPy next X-ray image processing Contents Prerequisites Table of contents 1 Load the MNIST dataset 2 Preprocess the data Convert the image data to the floating-point format Convert the labels to floating point through categorical/one-hot encoding 3",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_64"
        },
        {
          "content": "Build and train a small neural network from scratch Neural network building blocks with NumPy Model architecture and training summary Compose the model and begin training and testing it Next steps By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html",
          "library": "numpy",
          "chunk_id": "numpy_65"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
      "title": "X-ray image processing \u2014 NumPy Tutorials",
      "content": "Binder Repository Open issue .ipynb .md .pdf X-ray image processing Contents Prerequisites Table of contents Examine an X-ray with imageio Combine images into a multidimensional array to demonstrate progression Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters The Laplace filter with Gaussian second derivatives The Gaussian gradient magnitude method The Sobel-Feldman operator (the Sobel filter) The Canny filter Apply masks to X-rays with np.where() Compare the results Next steps X-ray image processing This tutorial demonstrates how to read and process X-ray images with NumPy, imageio, Matplotlib and SciPy. You will learn how to load medical images, focus on certain parts, and visually compare them using the Gaussian, Laplacian-Gaussian, Sobel, and Canny filters for edge detection. X-ray image analysis can be part of your data analysis and machine learning workflow when, for example, youre building an algorithm that helps detect pneumonia as part of a Kaggle competition. In the healthcare industry, medical image processing and analysis is particularly important when images are estimated to account for at least 90 of all medical data. Youll be working with radiology images from the ChestX-ray8 dataset provided by the National Institutes of Health (NIH). ChestX-ray8 contains over 100,000 de-identified X-ray images in the PNG format from more than 30,000 patients. You can find ChestX-ray8s files on NIHs public Box repository in the /images folder. (For more details, refer to the research paper published at CVPR (a computer vision conference) in 2017.) For your convenience, a small number of PNG images have been saved to this tutorials repository under tutorial-x-ray-image-processing/, since ChestX-ray8 contains gigabytes of data and you may find it challenging to download it in batches. Prerequisites The reader should have some knowledge of Python, NumPy arrays, and Matplotlib. To refresh the memory, you can take the Python and Matplotlib PyPlot tutorials, and the NumPy quickstart. The following packages are used in this tutorial: imageio for reading and writing image data. The healthcare industry usually works with the DICOM format for medical imaging and imageio should be well-suited for reading that format. For simplicity, in this tutorial youll be working with PNG files. Matplotlib for data visualization. SciPy for multi-dimensional image processing via ndimage. This tutorial can be run locally in an isolated environment, such as Virtualenv or conda. You can use Jupyter Notebook or JupyterLab to run each notebook cell. Table of contents Examine an X-ray with imageio Combine images into a multi-dimensional array to demonstrate progression Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters Apply masks to X-rays with np.where() Compare the results Examine an X-ray with imageio Lets begin with a simple example using just one X-ray image from the ChestX-ray8 dataset. The file  00000011_001.png  has been downloaded for you and saved in the /tutorial-x-ray-image-processing folder. 1. Load the image with imageio: import os import imageio DIR = \"tutorial-x-ray-image-processing\" xray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\")) 2. Check that its shape is 1024x1024 pixels and that the array is made up of 8-bit integers: print(xray_image.shape) print(xray_image.dtype) (1024, 1024) uint8 3. Import matplotlib and display the image in a grayscale colormap: import matplotlib.pyplot as plt plt.imshow(xray_image, cmap=\"gray\") plt.axis(\"off\") plt.show() Combine images into a multidimensional array to demonstrate progression In the next example, instead of 1 image youll use 9 X-ray 1024x1024-pixel images from the ChestX-ray8 dataset that have been downloaded and extracted from one of the dataset files. They are numbered from ...000.png to ...008.png and lets assume they belong to the same patient. 1. Import NumPy, read in each of the X-rays, and create a three-dimensional array where the first dimension corresponds to image number: import numpy as np num_imgs = 9 combined_xray_images_1 = np.array( [imageio.v3.imread(os.path.join(DIR, f\"00000011_00{i}.png\")) for i in range(num_imgs)] ) 2. Check the shape of the new X-ray image array containing 9 stacked images: combined_xray_images_1.shape (9, 1024, 1024) Note that the shape in the first dimension matches num_imgs, so the combined_xray_images_1 array can be interpreted as a stack of 2D images. 3. You can now display the health progress by plotting each of frames next to each other using Matplotlib: fig, axes = plt.subplots(nrows=1, ncols=num_imgs, figsize=(30, 30)) for img, ax in zip(combined_xray_images_1, axes): ax.imshow(img, cmap='gray') ax.axis('off') 4. In addition, it can be helpful to show the progress as an animation. Lets create a GIF file with imageio.mimwrite() and display the result in the notebook: GIF_PATH = os.path.join(DIR, \"xray_image.gif\") imageio.mimwrite(GIF_PATH, combined_xray_images_1, format= \".gif\", duration=1000) Which gives us: Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters When processing biomedical data, it can be useful to emphasize the 2D edges to focus on particular features in an image. To do that, using image gradients can be particularly helpful when detecting the change of color pixel intensity. The Laplace filter with Gaussian second derivatives Lets start with an n-dimensional Laplace filter (Laplacian-Gaussian) that uses Gaussian second derivatives. This Laplacian method focuses on pixels with rapid intensity change in values and is combined with Gaussian smoothing to remove noise. Lets examine how it can be useful in analyzing 2D X-ray images. The implementation of the Laplacian-Gaussian filter is relatively straightforward: 1) import the ndimage module from SciPy; and 2) call scipy.ndimage.gaussian_laplace() with a sigma (scalar) parameter, which affects the standard deviations of the Gaussian filter (youll use 1 in the example below): from scipy import ndimage xray_image_laplace_gaussian = ndimage.gaussian_laplace(xray_image, sigma=1) Display the original X-ray and the one with the Laplacian-Gaussian filter: fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10)) axes[0].set_title(\"Original\") axes[0].imshow(xray_image, cmap=\"gray\") axes[1].set_title(\"Laplacian-Gaussian (edges)\") axes[1].imshow(xray_image_laplace_gaussian, cmap=\"gray\") for i in axes: i.axis(\"off\") plt.show() The Gaussian gradient magnitude method Another method for edge detection that can be useful is the Gaussian (gradient) filter. It computes the multidimensional gradient magnitude with Gaussian derivatives and helps by remove high-frequency image components. 1. Call scipy.ndimage.gaussian_gradient_magnitude() with a sigma (scalar) parameter (for standard deviations; youll use 2 in the example below): x_ray_image_gaussian_gradient = ndimage.gaussian_gradient_magnitude(xray_image, sigma=2) 2. Display the original X-ray and the one with the Gaussian gradient filter: fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10)) axes[0].set_title(\"Original\") axes[0].imshow(xray_image, cmap=\"gray\") axes[1].set_title(\"Gaussian gradient (edges)\") axes[1].imshow(x_ray_image_gaussian_gradient, cmap=\"gray\") for i in axes: i.axis(\"off\") plt.show() The Sobel-Feldman operator (the Sobel filter) To find regions of high spatial frequency (the edges or the edge maps) along the horizontal and vertical axes of a 2D X-ray image, you can use the Sobel-Feldman operator (Sobel filter) technique. The Sobel filter applies two 3x3 kernel matrices  one for each axis  onto the X-ray through a convolution. Then, these two points (gradients) are combined using the Pythagorean theorem to produce a gradient magnitude. 1. Use the Sobel filters  (scipy.ndimage.sobel())  on x- and y-axes of the X-ray. Then, calculate the distance between x and y (with the Sobel filters applied to them) using the Pythagorean theorem and NumPys np.hypot() to obtain the magnitude. Finally, normalize the rescaled image for the pixel values to be between 0 and 255. Image normalization follows the output_channel = 255.0 * (input_channel - min_value) / (max_value - min_value) formula. Because youre using a grayscale image, you need to normalize just one channel. x_sobel = ndimage.sobel(xray_image, axis=0) y_sobel = ndimage.sobel(xray_image, axis=1) xray_image_sobel = np.hypot(x_sobel, y_sobel) xray_image_sobel *= 255.0 / np.max(xray_image_sobel) 2. Change the new image array data type to the 32-bit floating-point format from float16 to make it compatible with Matplotlib: print(\"The data type - before: \", xray_image_sobel.dtype) xray_image_sobel = xray_image_sobel.astype(\"float32\") print(\"The data type - after: \", xray_image_sobel.dtype) The data type - before: float16 The data type - after: float32 3. Display the original X-ray and the one with the Sobel edge filter applied. Note that both the grayscale and CMRmap colormaps are used to help emphasize the edges: fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 15)) axes[0].set_title(\"Original\") axes[0].imshow(xray_image, cmap=\"gray\") axes[1].set_title(\"Sobel (edges) - grayscale\") axes[1].imshow(xray_image_sobel, cmap=\"gray\") axes[2].set_title(\"Sobel (edges) - CMRmap\") axes[2].imshow(xray_image_sobel, cmap=\"CMRmap\") for i in axes: i.axis(\"off\") plt.show() The Canny filter You can also consider using another well-known filter for edge detection called the Canny filter. First, you apply a Gaussian filter to remove the noise in an image. In this example, youre using using the Fourier filter which smoothens the X-ray through a convolution process. Next, you apply the Prewitt filter on each of the 2 axes of the image to help detect some of the edges  this will result in 2 gradient values. Similar to the Sobel filter, the Prewitt operator also applies two 3x3 kernel matrices  one for each axis  onto the X-ray through a convolution. In the end, you compute the magnitude between the two gradients using the Pythagorean theorem and normalize the images, as before. 1. Use SciPys Fourier filters  scipy.ndimage.fourier_gaussian()  with a small sigma value to remove some of the noise from the X-ray. Then, calculate two gradients using scipy.ndimage.prewitt(). Next, measure the distance between the gradients using NumPys np.hypot(). Finally, normalize the rescaled image, as before. fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05) x_prewitt = ndimage.prewitt(fourier_gaussian, axis=0) y_prewitt = ndimage.prewitt(fourier_gaussian, axis=1) xray_image_canny = np.hypot(x_prewitt, y_prewitt) xray_image_canny *= 255.0 / np.max(xray_image_canny) print(\"The data type - \", xray_image_canny.dtype) The data type - float64 2. Plot the original X-ray image and the ones with the edges detected with the help of the Canny filter technique. The edges can be emphasized using the prism, nipy_spectral, and terrain Matplotlib colormaps. fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 15)) axes[0].set_title(\"Original\") axes[0].imshow(xray_image, cmap=\"gray\") axes[1].set_title(\"Canny (edges) - prism\") axes[1].imshow(xray_image_canny, cmap=\"prism\") axes[2].set_title(\"Canny (edges) - nipy_spectral\") axes[2].imshow(xray_image_canny, cmap=\"nipy_spectral\") axes[3].set_title(\"Canny (edges) - terrain\") axes[3].imshow(xray_image_canny, cmap=\"terrain\") for i in axes: i.axis(\"off\") plt.show() Apply masks to X-rays with np.where() To screen out only certain pixels in X-ray images to help detect particular features, you can apply masks with NumPys np.where(condition: array_like (bool), x: array_like, y: ndarray) that returns x when True and y when False. Identifying regions of interest  certain sets of pixels in an image  can be useful and masks serve as boolean arrays of the same shape as the original image. 1. Retrieve some basics statistics about the pixel values in the original X-ray image youve been working with: print(\"The data type of the X-ray image is: \", xray_image.dtype) print(\"The minimum pixel value is: \", np.min(xray_image)) print(\"The maximum pixel value is: \", np.max(xray_image)) print(\"The average pixel value is: \", np.mean(xray_image)) print(\"The median pixel value is: \", np.median(xray_image)) The data type of the X-ray image is: uint8 The minimum pixel value is: 0 The maximum pixel value is: 255 The average pixel value is: 172.52233219146729 The median pixel value is: 195.0 2. The array data type is uint8 and the minimum/maximum value results suggest that all 256 colors (from 0 to 255) are used in the X-ray. Lets visualize the pixel intensity distribution of the original raw X-ray image with ndimage.histogram() and Matplotlib: pixel_intensity_distribution = ndimage.histogram( xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256 ) plt.plot(pixel_intensity_distribution) plt.title(\"Pixel intensity distribution\") plt.show() As the pixel intensity distribution suggests, there are many low (between around 0 and 20) and very high (between around 200 and 240) pixel values. 3. You can create different conditional masks with NumPys np.where()  for example, lets have only those values of the image with the pixels exceeding a certain threshold:  The threshold is \"greater than 150\"  Return the original image if true, `0` otherwise xray_image_mask_noisy = np.where(xray_image  150, xray_image, 0) plt.imshow(xray_image_mask_noisy, cmap=\"gray\") plt.axis(\"off\") plt.show()  The threshold is \"greater than 150\"  Return `1` if true, `0` otherwise xray_image_mask_less_noisy = np.where(xray_image  150, 1, 0) plt.imshow(xray_image_mask_less_noisy, cmap=\"gray\") plt.axis(\"off\") plt.show() Compare the results Lets display some of the results of processed X-ray images youve worked with so far: fig, axes = plt.subplots(nrows=1, ncols=9, figsize=(30, 30)) axes[0].set_title(\"Original\") axes[0].imshow(xray_image, cmap=\"gray\") axes[1].set_title(\"Laplace-Gaussian (edges)\") axes[1].imshow(xray_image_laplace_gaussian, cmap=\"gray\") axes[2].set_title(\"Gaussian gradient (edges)\") axes[2].imshow(x_ray_image_gaussian_gradient, cmap=\"gray\") axes[3].set_title(\"Sobel (edges) - grayscale\") axes[3].imshow(xray_image_sobel, cmap=\"gray\") axes[4].set_title(\"Sobel (edges) - hot\") axes[4].imshow(xray_image_sobel, cmap=\"hot\") axes[5].set_title(\"Canny (edges) - prism)\") axes[5].imshow(xray_image_canny, cmap=\"prism\") axes[6].set_title(\"Canny (edges) - nipy_spectral)\") axes[6].imshow(xray_image_canny, cmap=\"nipy_spectral\") axes[7].set_title(\"Mask ( 150, noisy)\") axes[7].imshow(xray_image_mask_noisy, cmap=\"gray\") axes[8].set_title(\"Mask ( 150, less noisy)\") axes[8].imshow(xray_image_mask_less_noisy, cmap=\"gray\") for i in axes: i.axis(\"off\") plt.show() Next steps If you want to use your own samples, you can use this image or search for various other ones on the Openi database. Openi contains many biomedical images and it can be especially helpful if you have low bandwidth and/or are restricted by the amount of data you can download. To learn more about image processing in the context of biomedical image data or simply edge detection, you may find the following material useful: DICOM processing and segmentation in Python with Scikit-Image and pydicom (Radiology Data Quest) Image manipulation and processing using Numpy and Scipy (Scipy Lecture Notes) Intensity values (presentation, DataCamp) Object detection with Raspberry Pi and Python (Maker Portal) X-ray data preparation and segmentation with deep learning (a Kaggle-hosted Jupyter notebook) Image filtering (lecture slides, CS6670: Computer Vision, Cornell University) Edge detection in Python and NumPy (Towards Data Science) Edge detection with Scikit-Image (Data Carpentry) Image gradients and gradient filtering (lecture slides, 16-385 Computer Vision, Carnegie Mellon University) previous Deep learning on MNIST next Determining Static Equilibrium in NumPy Contents Prerequisites Table of contents Examine an X-ray with imageio Combine images into a multidimensional array to demonstrate progression Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters The Laplace filter with Gaussian second derivatives The Gaussian gradient magnitude method The Sobel-Feldman operator (the Sobel filter) The Canny filter Apply masks to X-rays with np.where() Compare the results Next steps By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "tutorial-x-ray-image-processing/",
        "00000011_001.png",
        "/tutorial-x-ray-image-processing",
        "import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
        "print(xray_image.shape)\nprint(xray_image.dtype)",
        "(1024, 1024)\nuint8",
        "import matplotlib.pyplot as plt\n\nplt.imshow(xray_image, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()",
        "import numpy as np\nnum_imgs = 9\n\ncombined_xray_images_1 = np.array(\n    [imageio.v3.imread(os.path.join(DIR, f\"00000011_00{i}.png\")) for i in range(num_imgs)]\n)",
        "combined_xray_images_1.shape",
        "(9, 1024, 1024)",
        "combined_xray_images_1",
        "fig, axes = plt.subplots(nrows=1, ncols=num_imgs, figsize=(30, 30))\n\nfor img, ax in zip(combined_xray_images_1, axes):\n    ax.imshow(img, cmap='gray')\n    ax.axis('off')",
        "imageio.mimwrite()",
        "GIF_PATH = os.path.join(DIR, \"xray_image.gif\")\nimageio.mimwrite(GIF_PATH, combined_xray_images_1, format= \".gif\", duration=1000)",
        "scipy.ndimage.gaussian_laplace()",
        "from scipy import ndimage\n\nxray_image_laplace_gaussian = ndimage.gaussian_laplace(xray_image, sigma=1)",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Laplacian-Gaussian (edges)\")\naxes[1].imshow(xray_image_laplace_gaussian, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()",
        "scipy.ndimage.gaussian_gradient_magnitude()",
        "x_ray_image_gaussian_gradient = ndimage.gaussian_gradient_magnitude(xray_image, sigma=2)",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Gaussian gradient (edges)\")\naxes[1].imshow(x_ray_image_gaussian_gradient, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()",
        "scipy.ndimage.sobel()",
        "output_channel = 255.0 * (input_channel - min_value) / (max_value - min_value)",
        "x_sobel = ndimage.sobel(xray_image, axis=0)\ny_sobel = ndimage.sobel(xray_image, axis=1)\n\nxray_image_sobel = np.hypot(x_sobel, y_sobel)\n\nxray_image_sobel *= 255.0 / np.max(xray_image_sobel)",
        "print(\"The data type - before: \", xray_image_sobel.dtype)\n\nxray_image_sobel = xray_image_sobel.astype(\"float32\")\n\nprint(\"The data type - after: \", xray_image_sobel.dtype)",
        "The data type - before:  float16\nThe data type - after:  float32",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 15))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Sobel (edges) - grayscale\")\naxes[1].imshow(xray_image_sobel, cmap=\"gray\")\naxes[2].set_title(\"Sobel (edges) - CMRmap\")\naxes[2].imshow(xray_image_sobel, cmap=\"CMRmap\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()",
        "scipy.ndimage.fourier_gaussian()",
        "scipy.ndimage.prewitt()",
        "fourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)",
        "The data type -  float64",
        "nipy_spectral",
        "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 15))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Canny (edges) - prism\")\naxes[1].imshow(xray_image_canny, cmap=\"prism\")\naxes[2].set_title(\"Canny (edges) - nipy_spectral\")\naxes[2].imshow(xray_image_canny, cmap=\"nipy_spectral\")\naxes[3].set_title(\"Canny (edges) - terrain\")\naxes[3].imshow(xray_image_canny, cmap=\"terrain\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()",
        "np.where(condition: array_like (bool), x: array_like, y: ndarray)",
        "print(\"The data type of the X-ray image is: \", xray_image.dtype)\nprint(\"The minimum pixel value is: \", np.min(xray_image))\nprint(\"The maximum pixel value is: \", np.max(xray_image))\nprint(\"The average pixel value is: \", np.mean(xray_image))\nprint(\"The median pixel value is: \", np.median(xray_image))",
        "The data type of the X-ray image is:  uint8\nThe minimum pixel value is:  0\nThe maximum pixel value is:  255\nThe average pixel value is:  172.52233219146729\nThe median pixel value is:  195.0",
        "ndimage.histogram()",
        "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()",
        "# The threshold is \"greater than 150\"\n# Return the original image if true, `0` otherwise\nxray_image_mask_noisy = np.where(xray_image > 150, xray_image, 0)\n\nplt.imshow(xray_image_mask_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()",
        "# The threshold is \"greater than 150\"\n# Return `1` if true, `0` otherwise\nxray_image_mask_less_noisy = np.where(xray_image > 150, 1, 0)\n\nplt.imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()",
        "fig, axes = plt.subplots(nrows=1, ncols=9, figsize=(30, 30))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Laplace-Gaussian (edges)\")\naxes[1].imshow(xray_image_laplace_gaussian, cmap=\"gray\")\naxes[2].set_title(\"Gaussian gradient (edges)\")\naxes[2].imshow(x_ray_image_gaussian_gradient, cmap=\"gray\")\naxes[3].set_title(\"Sobel (edges) - grayscale\")\naxes[3].imshow(xray_image_sobel, cmap=\"gray\")\naxes[4].set_title(\"Sobel (edges) - hot\")\naxes[4].imshow(xray_image_sobel, cmap=\"hot\")\naxes[5].set_title(\"Canny (edges) - prism)\")\naxes[5].imshow(xray_image_canny, cmap=\"prism\")\naxes[6].set_title(\"Canny (edges) - nipy_spectral)\")\naxes[6].imshow(xray_image_canny, cmap=\"nipy_spectral\")\naxes[7].set_title(\"Mask (> 150, noisy)\")\naxes[7].imshow(xray_image_mask_noisy, cmap=\"gray\")\naxes[8].set_title(\"Mask (> 150, less noisy)\")\naxes[8].imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()"
      ],
      "chunks": [
        {
          "content": "Binder Repository Open issue ipynb md pdf X-ray image processing Contents Prerequisites Table of contents Examine an X-ray with imageio Combine images into a multidimensional array to demonstrate progression Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters The Laplace filter with Gaussian second derivatives The Gaussian gradient magnitude method The Sobel-Feldman operator (the Sobel filter) The Canny filter Apply masks to X-rays with np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "where() Compare the results Next steps X-ray image processing This tutorial demonstrates how to read and process X-ray images with NumPy, imageio, Matplotlib and SciPy You will learn how to load medical images, focus on certain parts, and visually compare them using the Gaussian, Laplacian-Gaussian, Sobel, and Canny filters for edge detection",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "X-ray image analysis can be part of your data analysis and machine learning workflow when, for example, youre building an algorithm that helps detect pneumonia as part of a Kaggle competition In the healthcare industry, medical image processing and analysis is particularly important when images are estimated to account for at least 90 of all medical data Youll be working with radiology images from the ChestX-ray8 dataset provided by the National Institutes of Health (NIH)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "ChestX-ray8 contains over 100,000 de-identified X-ray images in the PNG format from more than 30,000 patients You can find ChestX-ray8s files on NIHs public Box repository in the /images folder (For more details, refer to the research paper published at CVPR (a computer vision conference) in 2017",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": ") For your convenience, a small number of PNG images have been saved to this tutorials repository under tutorial-x-ray-image-processing/, since ChestX-ray8 contains gigabytes of data and you may find it challenging to download it in batches Prerequisites The reader should have some knowledge of Python, NumPy arrays, and Matplotlib To refresh the memory, you can take the Python and Matplotlib PyPlot tutorials, and the NumPy quickstart",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "The following packages are used in this tutorial: imageio for reading and writing image data The healthcare industry usually works with the DICOM format for medical imaging and imageio should be well-suited for reading that format For simplicity, in this tutorial youll be working with PNG files Matplotlib for data visualization SciPy for multi-dimensional image processing via ndimage This tutorial can be run locally in an isolated environment, such as Virtualenv or conda",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "You can use Jupyter Notebook or JupyterLab to run each notebook cell Table of contents Examine an X-ray with imageio Combine images into a multi-dimensional array to demonstrate progression Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters Apply masks to X-rays with np where() Compare the results Examine an X-ray with imageio Lets begin with a simple example using just one X-ray image from the ChestX-ray8 dataset The file  00000011_001",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "png  has been downloaded for you and saved in the /tutorial-x-ray-image-processing folder 1 Load the image with imageio: import os import imageio DIR = \"tutorial-x-ray-image-processing\" xray_image = imageio v3 imread(os path join(DIR, \"00000011_001 png\")) 2 Check that its shape is 1024x1024 pixels and that the array is made up of 8-bit integers: print(xray_image shape) print(xray_image dtype) (1024, 1024) uint8 3 Import matplotlib and display the image in a grayscale colormap: import matplotlib",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "pyplot as plt plt imshow(xray_image, cmap=\"gray\") plt axis(\"off\") plt show() Combine images into a multidimensional array to demonstrate progression In the next example, instead of 1 image youll use 9 X-ray 1024x1024-pixel images from the ChestX-ray8 dataset that have been downloaded and extracted from one of the dataset files They are numbered from 000 png to 008 png and lets assume they belong to the same patient 1",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "Import NumPy, read in each of the X-rays, and create a three-dimensional array where the first dimension corresponds to image number: import numpy as np num_imgs = 9 combined_xray_images_1 = np array( [imageio v3 imread(os path join(DIR, f\"00000011_00{i} png\")) for i in range(num_imgs)] ) 2 Check the shape of the new X-ray image array containing 9 stacked images: combined_xray_images_1",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "shape (9, 1024, 1024) Note that the shape in the first dimension matches num_imgs, so the combined_xray_images_1 array can be interpreted as a stack of 2D images 3 You can now display the health progress by plotting each of frames next to each other using Matplotlib: fig, axes = plt subplots(nrows=1, ncols=num_imgs, figsize=(30, 30)) for img, ax in zip(combined_xray_images_1, axes): ax imshow(img, cmap='gray') ax axis('off') 4 In addition, it can be helpful to show the progress as an animation",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "Lets create a GIF file with imageio mimwrite() and display the result in the notebook: GIF_PATH = os path join(DIR, \"xray_image gif\") imageio mimwrite(GIF_PATH, combined_xray_images_1, format= \" gif\", duration=1000) Which gives us: Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters When processing biomedical data, it can be useful to emphasize the 2D edges to focus on particular features in an image",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "To do that, using image gradients can be particularly helpful when detecting the change of color pixel intensity The Laplace filter with Gaussian second derivatives Lets start with an n-dimensional Laplace filter (Laplacian-Gaussian) that uses Gaussian second derivatives This Laplacian method focuses on pixels with rapid intensity change in values and is combined with Gaussian smoothing to remove noise Lets examine how it can be useful in analyzing 2D X-ray images",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "The implementation of the Laplacian-Gaussian filter is relatively straightforward: 1) import the ndimage module from SciPy; and 2) call scipy ndimage gaussian_laplace() with a sigma (scalar) parameter, which affects the standard deviations of the Gaussian filter (youll use 1 in the example below): from scipy import ndimage xray_image_laplace_gaussian = ndimage gaussian_laplace(xray_image, sigma=1) Display the original X-ray and the one with the Laplacian-Gaussian filter: fig, axes = plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "subplots(nrows=1, ncols=2, figsize=(10, 10)) axes[0] set_title(\"Original\") axes[0] imshow(xray_image, cmap=\"gray\") axes[1] set_title(\"Laplacian-Gaussian (edges)\") axes[1] imshow(xray_image_laplace_gaussian, cmap=\"gray\") for i in axes: i axis(\"off\") plt show() The Gaussian gradient magnitude method Another method for edge detection that can be useful is the Gaussian (gradient) filter",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "It computes the multidimensional gradient magnitude with Gaussian derivatives and helps by remove high-frequency image components 1 Call scipy ndimage gaussian_gradient_magnitude() with a sigma (scalar) parameter (for standard deviations; youll use 2 in the example below): x_ray_image_gaussian_gradient = ndimage gaussian_gradient_magnitude(xray_image, sigma=2) 2 Display the original X-ray and the one with the Gaussian gradient filter: fig, axes = plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "subplots(nrows=1, ncols=2, figsize=(10, 10)) axes[0] set_title(\"Original\") axes[0] imshow(xray_image, cmap=\"gray\") axes[1] set_title(\"Gaussian gradient (edges)\") axes[1] imshow(x_ray_image_gaussian_gradient, cmap=\"gray\") for i in axes: i axis(\"off\") plt show() The Sobel-Feldman operator (the Sobel filter) To find regions of high spatial frequency (the edges or the edge maps) along the horizontal and vertical axes of a 2D X-ray image, you can use the Sobel-Feldman operator (Sobel filter) technique",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "The Sobel filter applies two 3x3 kernel matrices  one for each axis  onto the X-ray through a convolution Then, these two points (gradients) are combined using the Pythagorean theorem to produce a gradient magnitude 1 Use the Sobel filters  (scipy ndimage sobel())  on x- and y-axes of the X-ray Then, calculate the distance between x and y (with the Sobel filters applied to them) using the Pythagorean theorem and NumPys np hypot() to obtain the magnitude",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "Finally, normalize the rescaled image for the pixel values to be between 0 and 255 Image normalization follows the output_channel = 255 0 * (input_channel - min_value) / (max_value - min_value) formula Because youre using a grayscale image, you need to normalize just one channel x_sobel = ndimage sobel(xray_image, axis=0) y_sobel = ndimage sobel(xray_image, axis=1) xray_image_sobel = np hypot(x_sobel, y_sobel) xray_image_sobel *= 255 0 / np max(xray_image_sobel) 2",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "Change the new image array data type to the 32-bit floating-point format from float16 to make it compatible with Matplotlib: print(\"The data type - before: \", xray_image_sobel dtype) xray_image_sobel = xray_image_sobel astype(\"float32\") print(\"The data type - after: \", xray_image_sobel dtype) The data type - before: float16 The data type - after: float32 3 Display the original X-ray and the one with the Sobel edge filter applied",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "Note that both the grayscale and CMRmap colormaps are used to help emphasize the edges: fig, axes = plt subplots(nrows=1, ncols=3, figsize=(15, 15)) axes[0] set_title(\"Original\") axes[0] imshow(xray_image, cmap=\"gray\") axes[1] set_title(\"Sobel (edges) - grayscale\") axes[1] imshow(xray_image_sobel, cmap=\"gray\") axes[2] set_title(\"Sobel (edges) - CMRmap\") axes[2] imshow(xray_image_sobel, cmap=\"CMRmap\") for i in axes: i axis(\"off\") plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "show() The Canny filter You can also consider using another well-known filter for edge detection called the Canny filter First, you apply a Gaussian filter to remove the noise in an image In this example, youre using using the Fourier filter which smoothens the X-ray through a convolution process Next, you apply the Prewitt filter on each of the 2 axes of the image to help detect some of the edges  this will result in 2 gradient values",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "Similar to the Sobel filter, the Prewitt operator also applies two 3x3 kernel matrices  one for each axis  onto the X-ray through a convolution In the end, you compute the magnitude between the two gradients using the Pythagorean theorem and normalize the images, as before 1 Use SciPys Fourier filters  scipy ndimage fourier_gaussian()  with a small sigma value to remove some of the noise from the X-ray Then, calculate two gradients using scipy ndimage prewitt()",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "Next, measure the distance between the gradients using NumPys np hypot() Finally, normalize the rescaled image, as before fourier_gaussian = ndimage fourier_gaussian(xray_image, sigma=0 05) x_prewitt = ndimage prewitt(fourier_gaussian, axis=0) y_prewitt = ndimage prewitt(fourier_gaussian, axis=1) xray_image_canny = np hypot(x_prewitt, y_prewitt) xray_image_canny *= 255 0 / np max(xray_image_canny) print(\"The data type - \", xray_image_canny dtype) The data type - float64 2",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "Plot the original X-ray image and the ones with the edges detected with the help of the Canny filter technique The edges can be emphasized using the prism, nipy_spectral, and terrain Matplotlib colormaps fig, axes = plt subplots(nrows=1, ncols=4, figsize=(20, 15)) axes[0] set_title(\"Original\") axes[0] imshow(xray_image, cmap=\"gray\") axes[1] set_title(\"Canny (edges) - prism\") axes[1] imshow(xray_image_canny, cmap=\"prism\") axes[2] set_title(\"Canny (edges) - nipy_spectral\") axes[2]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": "imshow(xray_image_canny, cmap=\"nipy_spectral\") axes[3] set_title(\"Canny (edges) - terrain\") axes[3] imshow(xray_image_canny, cmap=\"terrain\") for i in axes: i axis(\"off\") plt show() Apply masks to X-rays with np where() To screen out only certain pixels in X-ray images to help detect particular features, you can apply masks with NumPys np where(condition: array_like (bool), x: array_like, y: ndarray) that returns x when True and y when False",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "Identifying regions of interest  certain sets of pixels in an image  can be useful and masks serve as boolean arrays of the same shape as the original image 1 Retrieve some basics statistics about the pixel values in the original X-ray image youve been working with: print(\"The data type of the X-ray image is: \", xray_image dtype) print(\"The minimum pixel value is: \", np min(xray_image)) print(\"The maximum pixel value is: \", np max(xray_image)) print(\"The average pixel value is: \", np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "mean(xray_image)) print(\"The median pixel value is: \", np median(xray_image)) The data type of the X-ray image is: uint8 The minimum pixel value is: 0 The maximum pixel value is: 255 The average pixel value is: 172 52233219146729 The median pixel value is: 195 0 2 The array data type is uint8 and the minimum/maximum value results suggest that all 256 colors (from 0 to 255) are used in the X-ray Lets visualize the pixel intensity distribution of the original raw X-ray image with ndimage",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "histogram() and Matplotlib: pixel_intensity_distribution = ndimage histogram( xray_image, min=np min(xray_image), max=np max(xray_image), bins=256 ) plt plot(pixel_intensity_distribution) plt title(\"Pixel intensity distribution\") plt show() As the pixel intensity distribution suggests, there are many low (between around 0 and 20) and very high (between around 200 and 240) pixel values 3 You can create different conditional masks with NumPys np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "where()  for example, lets have only those values of the image with the pixels exceeding a certain threshold:  The threshold is \"greater than 150\"  Return the original image if true, `0` otherwise xray_image_mask_noisy = np where(xray_image  150, xray_image, 0) plt imshow(xray_image_mask_noisy, cmap=\"gray\") plt axis(\"off\") plt show()  The threshold is \"greater than 150\"  Return `1` if true, `0` otherwise xray_image_mask_less_noisy = np where(xray_image  150, 1, 0) plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "imshow(xray_image_mask_less_noisy, cmap=\"gray\") plt axis(\"off\") plt show() Compare the results Lets display some of the results of processed X-ray images youve worked with so far: fig, axes = plt subplots(nrows=1, ncols=9, figsize=(30, 30)) axes[0] set_title(\"Original\") axes[0] imshow(xray_image, cmap=\"gray\") axes[1] set_title(\"Laplace-Gaussian (edges)\") axes[1] imshow(xray_image_laplace_gaussian, cmap=\"gray\") axes[2] set_title(\"Gaussian gradient (edges)\") axes[2]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "imshow(x_ray_image_gaussian_gradient, cmap=\"gray\") axes[3] set_title(\"Sobel (edges) - grayscale\") axes[3] imshow(xray_image_sobel, cmap=\"gray\") axes[4] set_title(\"Sobel (edges) - hot\") axes[4] imshow(xray_image_sobel, cmap=\"hot\") axes[5] set_title(\"Canny (edges) - prism)\") axes[5] imshow(xray_image_canny, cmap=\"prism\") axes[6] set_title(\"Canny (edges) - nipy_spectral)\") axes[6] imshow(xray_image_canny, cmap=\"nipy_spectral\") axes[7] set_title(\"Mask ( 150, noisy)\") axes[7]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "imshow(xray_image_mask_noisy, cmap=\"gray\") axes[8] set_title(\"Mask ( 150, less noisy)\") axes[8] imshow(xray_image_mask_less_noisy, cmap=\"gray\") for i in axes: i axis(\"off\") plt show() Next steps If you want to use your own samples, you can use this image or search for various other ones on the Openi database Openi contains many biomedical images and it can be especially helpful if you have low bandwidth and/or are restricted by the amount of data you can download",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": "To learn more about image processing in the context of biomedical image data or simply edge detection, you may find the following material useful: DICOM processing and segmentation in Python with Scikit-Image and pydicom (Radiology Data Quest) Image manipulation and processing using Numpy and Scipy (Scipy Lecture Notes) Intensity values (presentation, DataCamp) Object detection with Raspberry Pi and Python (Maker Portal) X-ray data preparation and segmentation with deep learning (a Kaggle-hosted Jupyter notebook) Image filtering (lecture slides, CS6670: Computer Vision, Cornell University) Edge detection in Python and NumPy (Towards Data Science) Edge detection with Scikit-Image (Data Carpentry) Image gradients and gradient filtering (lecture slides, 16-385 Computer Vision, Carnegie Mellon University) previous Deep learning on MNIST next Determining Static Equilibrium in NumPy Contents Prerequisites Table of contents Examine an X-ray with imageio Combine images into a multidimensional array to demonstrate progression Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters The Laplace filter with Gaussian second derivatives The Gaussian gradient magnitude method The Sobel-Feldman operator (the Sobel filter) The Canny filter Apply masks to X-rays with np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": "where() Compare the results Next steps By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-x-ray-image-processing.html",
          "library": "numpy",
          "chunk_id": "numpy_34"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/user/index.html#main-content",
      "title": "NumPy user guide \u2014 NumPy v2.4.dev0 Manual",
      "content": "NumPy user guide NumPy user guide This guide is an overview and explains the important features; details are found in NumPy reference. Getting started What is NumPy? Installation NumPy quickstart NumPy: the absolute basics for beginners Fundamentals and usage NumPy fundamentals Array creation Indexing on ndarrays I/O with NumPy Data types Broadcasting Copies and views Working with Arrays of Strings And Bytes Structured arrays Universal functions (ufunc) basics NumPy for MATLAB users NumPy tutorials NumPy how-tos Advanced usage and interoperability Using NumPy C-API F2PY user guide and reference manual Under-the-hood documentation for developers Interoperability with NumPy previous NumPy documentation next What is NumPy?",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy user guide NumPy user guide This guide is an overview and explains the important features; details are found in NumPy reference Getting started What is NumPy",
          "url": "https://numpy.org/devdocs/user/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Installation NumPy quickstart NumPy: the absolute basics for beginners Fundamentals and usage NumPy fundamentals Array creation Indexing on ndarrays I/O with NumPy Data types Broadcasting Copies and views Working with Arrays of Strings And Bytes Structured arrays Universal functions (ufunc) basics NumPy for MATLAB users NumPy tutorials NumPy how-tos Advanced usage and interoperability Using NumPy C-API F2PY user guide and reference manual Under-the-hood documentation for developers Interoperability with NumPy previous NumPy documentation next What is NumPy",
          "url": "https://numpy.org/devdocs/user/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/index.html",
      "title": "NumPy documentation \u2014 NumPy v2.4.dev0 Manual",
      "content": "NumPy documentation Version: 2.4.dev0 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Getting started New to NumPy? Check out the Absolute Beginners Guide. It contains an introduction to NumPys main concepts and links to additional tutorials. To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation. To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts. To the reference guide Contributors guide Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy. To the contributors guide next NumPy user guide",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy documentation Version: 2 4 dev0 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python",
          "url": "https://numpy.org/devdocs/index.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more Getting started New to NumPy Check out the Absolute Beginners Guide",
          "url": "https://numpy.org/devdocs/index.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "It contains an introduction to NumPys main concepts and links to additional tutorials To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy The reference describes how the methods work and which parameters can be used",
          "url": "https://numpy.org/devdocs/index.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "It assumes that you have an understanding of the key concepts To the reference guide Contributors guide Want to add to the codebase Can help add translation or a flowchart to the documentation The contributing guidelines will guide you through the process of improving NumPy To the contributors guide next NumPy user guide",
          "url": "https://numpy.org/devdocs/index.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/reference/index.html#main-content",
      "title": "NumPy reference \u2014 NumPy v2.4.dev0 Manual",
      "content": "NumPy reference NumPy reference Release: 2.4.dev0 Date: June 26, 2025 This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do. For learning how to use NumPy, see the complete documentation. Python API NumPys module structure Array objects The N-dimensional array (ndarray) Scalars Data type objects (dtype) Data type promotion in NumPy Iterating over arrays Standard array subclasses Masked arrays The array interface protocol Datetimes and timedeltas Universal functions (ufunc) Routines and objects by topic Constants Array creation routines Array manipulation routines Bit-wise operations String functionality Datetime support functions Data type routines Mathematical functions with automatic domain Floating point error handling Exceptions and Warnings Discrete Fourier Transform Functional programming Input and output Indexing routines Linear algebra Logic functions Masked array operations Mathematical functions Miscellaneous routines Polynomials Random sampling Set routines Sorting, searching, and counting Statistics Test support Window functions Typing (numpy.typing) Packaging C API NumPy C-API Python types and C-structures System configuration Data type API Array API Array iterator API ufunc API Generalized universal function API NpyString API NumPy core math library Datetime API C API deprecations Memory management in NumPy Other topics Array API standard compatibility CPU/SIMD optimizations Thread Safety Global Configuration Options NumPy security Status of numpy.distutils and migration advice numpy.distutils user guide NumPy and SWIG Acknowledgements Large parts of this manual originate from Travis E. Oliphants book Guide to NumPy (which generously entered Public Domain in August 2008). The reference documentation for many of the functions are written by numerous contributors and developers of NumPy. previous NumPy license next NumPys module structure On this page Python API C API Other topics Acknowledgements",
      "code_blocks": [
        "numpy.typing",
        "numpy.distutils",
        "numpy.distutils"
      ],
      "chunks": [
        {
          "content": "NumPy reference NumPy reference Release: 2 4 dev0 Date: June 26, 2025 This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do For learning how to use NumPy, see the complete documentation",
          "url": "https://numpy.org/devdocs/reference/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Python API NumPys module structure Array objects The N-dimensional array (ndarray) Scalars Data type objects (dtype) Data type promotion in NumPy Iterating over arrays Standard array subclasses Masked arrays The array interface protocol Datetimes and timedeltas Universal functions (ufunc) Routines and objects by topic Constants Array creation routines Array manipulation routines Bit-wise operations String functionality Datetime support functions Data type routines Mathematical functions with automatic domain Floating point error handling Exceptions and Warnings Discrete Fourier Transform Functional programming Input and output Indexing routines Linear algebra Logic functions Masked array operations Mathematical functions Miscellaneous routines Polynomials Random sampling Set routines Sorting, searching, and counting Statistics Test support Window functions Typing (numpy",
          "url": "https://numpy.org/devdocs/reference/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "typing) Packaging C API NumPy C-API Python types and C-structures System configuration Data type API Array API Array iterator API ufunc API Generalized universal function API NpyString API NumPy core math library Datetime API C API deprecations Memory management in NumPy Other topics Array API standard compatibility CPU/SIMD optimizations Thread Safety Global Configuration Options NumPy security Status of numpy distutils and migration advice numpy",
          "url": "https://numpy.org/devdocs/reference/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "distutils user guide NumPy and SWIG Acknowledgements Large parts of this manual originate from Travis E Oliphants book Guide to NumPy (which generously entered Public Domain in August 2008) The reference documentation for many of the functions are written by numerous contributors and developers of NumPy previous NumPy license next NumPys module structure On this page Python API C API Other topics Acknowledgements",
          "url": "https://numpy.org/devdocs/reference/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/building/index.html#main-content",
      "title": "Building from source \u2014 NumPy v2.4.dev0 Manual",
      "content": "Building from source Building from source Note If you are only trying to install NumPy, we recommend using binaries - see Installation for details on that. Building NumPy from source requires setting up system-level dependencies (compilers, BLAS/LAPACK libraries, etc.) first, and then invoking a build. The build may be done in order to install NumPy for local usage, develop NumPy itself, or build redistributable binary packages. And it may be desired to customize aspects of how the build is done. This guide will cover all these aspects. In addition, it provides background information on how the NumPy build works, and links to up-to-date guides for generic Python build  packaging documentation that is relevant. System-level dependencies NumPy uses compiled code for speed, which means you need compilers and some other system-level (i.e, non-Python / non-PyPI) dependencies to build it on your system. Note If you are using Conda, you can skip the steps in this section - with the exception of installing compilers for Windows or the Apple Developer Tools for macOS. All other dependencies will be installed automatically by the mamba env create -f environment.yml command. Linux If you want to use the system Python and pip, you will need: C and C++ compilers (typically GCC). Python header files (typically a package named python3-dev or python3-devel) BLAS and LAPACK libraries. OpenBLAS is the NumPy default; other variants include Apple Accelerate, MKL, ATLAS and Netlib (or Reference) BLAS and LAPACK. pkg-config for dependency detection. A Fortran compiler is needed only for running the f2py tests. The instructions below include a Fortran compiler, however you can safely leave it out. Debian/Ubuntu Linux To install NumPy build requirements, you can do: sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev Alternatively, you can do: sudo apt build-dep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. Fedora To install NumPy build requirements, you can do: sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo dnf builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. CentOS/RHEL To install NumPy build requirements, you can do: sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo yum-builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers. Arch To install NumPy build requirements, you can do: sudo pacman -S gcc-fortran openblas pkgconf macOS Install Apple Developer Tools. An easy way to do this is to open a terminal window, enter the command: xcode-select --install and follow the prompts. Apple Developer Tools includes Git, the Clang C/C++ compilers, and other development utilities that may be required. Do not use the macOS system Python. Instead, install Python with the python.org installer or with a package manager like Homebrew, MacPorts or Fink. On macOS =13.3, the easiest build option is to use Accelerate, which is already installed and will be automatically used by default. On older macOS versions you need a different BLAS library, most likely OpenBLAS, plus pkg-config to detect OpenBLAS. These are easiest to install with Homebrew: brew install openblas pkg-config gfortran Windows On Windows, the use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together. If you dont need to run the f2py tests, simply using MSVC is easiest. Otherwise, you will need one of these sets of compilers: MSVC + Intel Fortran (ifort) Intel compilers (icc, ifort) Mingw-w64 compilers (gcc, g++, gfortran) Compared to macOS and Linux, building NumPy on Windows is a little more difficult, due to the need to set up these compilers. It is not possible to just call a one-liner on the command prompt as you would on other platforms. First, install Microsoft Visual Studio - the 2019 Community Edition or any newer version will work (see the Visual Studio download site). This is needed even if you use the MinGW-w64 or Intel compilers, in order to ensure you have the Windows Universal C Runtime (the other components of Visual Studio are not needed when using Mingw-w64, and can be deselected if desired, to save disk space). The recommended version of the UCRT is = 10.0.22621.0. MSVC The MSVC installer does not put the compilers on the system path, and the install location may change. To query the install location, MSVC comes with a vswhere.exe command-line utility. And to make the C/C++ compilers available inside the shell you are using, you need to run a .bat file for the correct bitness and architecture (e.g., for 64-bit Intel CPUs, use vcvars64.bat). If using a Conda environment while a version of Visual Studio 2019+ is installed that includes the MSVC v142 package (VS 2019 C++ x86/x64 build tools), activating the conda environment should cause Visual Studio to be found and the appropriate .bat file executed to set these variables. For detailed guidance, see Use the Microsoft C++ toolset from the command line. Intel Similar to MSVC, the Intel compilers are designed to be used with an activation script (Intel\\oneAPI\\setvars.bat) that you run in the shell you are using. This makes the compilers available on the path. For detailed guidance, see Get Started with the Intel oneAPI HPC Toolkit for Windows. MinGW-w64 There are several sources of binaries for MinGW-w64. We recommend the RTools versions, which can be installed with Chocolatey (see Chocolatey install instructions here): choco install rtools -y --no-progress --force --version=4.0.0.20220206 Note Compilers should be on the system path (i.e., the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH. You can use any shell (e.g., Powershell, cmd or Git Bash) to invoke a build. To check that this is the case, try invoking a Fortran compiler in the shell you use (e.g., gfortran --version or ifort --version). Warning When using a conda environment it is possible that the environment creation will not work due to an outdated Fortran compiler. If that happens, remove the compilers entry from environment.yml and try again. The Fortran compiler should be installed as described in this section. Windows on ARM64 In Windows on ARM64, the set of a compiler options that are available for building NumPy are limited. Compilers such as GCC and GFortran are not yet supported for Windows on ARM64. Currently, the NumPy build for Windows on ARM64 is supported with MSVC and LLVM toolchains. The use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together. If you dont need to run the f2py tests, simply using MSVC is easiest. Otherwise, you will need the following set of compilers: MSVC + flang (cl, flang) LLVM + flang (clang-cl, flang) First, install Microsoft Visual Studio - the 2022 Community Edition will work (see the Visual Studio download site). Ensure that you have installed necessary Visual Studio components for building NumPy on WoA from here. To use the flang compiler for Windows on ARM64, install Latest LLVM toolchain for WoA from here. MSVC The MSVC installer does not put the compilers on the system path, and the install location may change. To query the install location, MSVC comes with a vswhere.exe command-line utility. And to make the C/C++ compilers available inside the shell you are using, you need to run a .bat file for the correct bitness and architecture (e.g., for ARM64-based CPUs, use vcvarsarm64.bat). For detailed guidance, see Use the Microsoft C++ toolset from the command line. LLVM Similar to MSVC, LLVM does not put the compilers on the system path. To set system path for LLVM compilers, users may need to use set command to put compilers on the system path. To check compilers path for LLVMs clang-cl, try invoking LLVMs clang-cl compiler in the shell you use (clang-cl --version). Note Compilers should be on the system path (i.e., the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH. You can use any shell (e.g., Powershell, cmd or Git Bash) to invoke a build. To check that this is the case, try invoking a Fortran compiler in the shell you use (e.g., flang --version). Warning Currently, Conda environment is not yet supported officially on Windows on ARM64. The present approach uses virtualenv for building NumPy from source on Windows on ARM64. Building NumPy from source If you want to only install NumPy from source once and not do any development work, then the recommended way to build and install is to use pip. Otherwise, conda is recommended. Note If you dont have a conda installation yet, we recommend using Miniforge; any conda flavor will work though. Building from source to use NumPy Conda env If you are using a conda environment, pip is still the tool you use to invoke a from-source build of NumPy. It is important to always use the --no-build-isolation flag to the pip install command, to avoid building against a numpy wheel from PyPI. In order for that to work you must first install the remaining build dependencies into the conda environment:  Either install all NumPy dev dependencies into a fresh conda environment mamba env create -f environment.yml  Or, install only the required build dependencies mamba install python numpy cython compilers openblas meson-python pkg-config  To build the latest stable release: pip install numpy --no-build-isolation --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init pip install . --no-build-isolation Warning On Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the pip install command to fail. These variables are only needed for flang and can be safely unset prior to running pip install. Virtual env or system Python  To build the latest stable release: pip install numpy --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init pip install . Building from source for NumPy development If you want to build from source in order to work on NumPy itself, first clone the NumPy repository: git clone https://github.com/numpy/numpy.git cd numpy git submodule update --init Then you want to do the following: Create a dedicated development environment (virtual environment or conda environment), Install all needed dependencies (build, and also test, doc and optional dependencies), Build NumPy with the spin developer interface. Step (3) is always the same, steps (1) and (2) are different between conda and virtual environments: Conda env To create a numpy-dev development environment with every required and optional dependency installed, run: mamba env create -f environment.yml mamba activate numpy-dev Virtual env or system Python Note There are many tools to manage virtual environments, like venv, virtualenv/virtualenvwrapper, pyenv/pyenv-virtualenv, Poetry, PDM, Hatch, and more. Here we use the basic venv tool that is part of the Python stdlib. You can use any other tool; all we need is an activated Python environment. Create and activate a virtual environment in a new directory named venv ( note that the exact activation command may be different based on your OS and shell - see How venvs work in the venv docs). Linux python -m venv venv source venv/bin/activate macOS python -m venv venv source venv/bin/activate Windows python -m venv venv .\\venv\\Scripts\\activate Windows on ARM64 python -m venv venv .\\venv\\Scripts\\activate Note Building NumPy with BLAS and LAPACK functions requires OpenBLAS library at Runtime. In Windows on ARM64, this can be done by setting up pkg-config for OpenBLAS dependency. The build steps for OpenBLAS for Windows on ARM64 can be found here. Then install the Python-level dependencies from PyPI with: python -m pip install -r requirements/build_requirements.txt To build NumPy in an activated development environment, run: spin build This will install NumPy inside the repository (by default in a build-install directory). You can then run tests (spin test), drop into IPython (spin ipython), or take other development steps like build the html documentation or running benchmarks. The spin interface is self-documenting, so please see spin --help and spin subcommand --help for detailed guidance. Warning In an activated conda environment on Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the build to fail. These variables are only needed for flang and can be safely unset for build. IDE support  editable installs While the spin interface is our recommended way of working on NumPy, it has one limitation: because of the custom install location, NumPy installed using spin will not be recognized automatically within an IDE (e.g., for running a script via a run button, or setting breakpoints visually). This will work better with an in-place build (or editable install). Editable installs are supported. It is important to understand that you may use either an editable install or ``spin`` in a given repository clone, but not both. If you use editable installs, you have to use pytest and other development tools directly instead of using spin. To use an editable install, ensure you start from a clean repository (run git clean -xdf if youve built with spin before) and have all dependencies set up correctly as described higher up on this page. Then do:  Note: the --no-build-isolation is important! pip install -e . --no-build-isolation  To run the tests for, e.g., the `numpy.linalg` module: pytest numpy/linalg When making changes to NumPy code, including to compiled code, there is no need to manually rebuild or reinstall. NumPy is automatically rebuilt each time NumPy is imported by the Python interpreter; see the meson-python documentation on editable installs for more details on how that works under the hood. When you run git clean -xdf, which removes the built extension modules, remember to also uninstall NumPy with pip uninstall numpy. Warning Note that editable installs are fundamentally incomplete installs. Their only guarantee is that import numpy works - so they are suitable for working on NumPy itself, and for working on pure Python packages that depend on NumPy. Headers, entrypoints, and other such things may not be available from an editable install. Customizing builds Compiler selection and customizing a build BLAS and LAPACK Cross compilation Building redistributable binaries Background information Understanding Meson Introspecting build steps Meson and distutils ways of doing things previous Testing the numpy.i typemaps next Compiler selection and customizing a build On this page System-level dependencies Building NumPy from source Building from source to use NumPy Building from source for NumPy development Customizing builds Background information",
      "code_blocks": [
        "mamba env create -f environment.yml",
        "python3-dev",
        "python3-devel",
        "sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev",
        "sudo apt build-dep numpy",
        "sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig",
        "sudo dnf builddep numpy",
        "sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig",
        "sudo yum-builddep numpy",
        "sudo pacman -S gcc-fortran openblas pkgconf",
        "xcode-select --install",
        "brew install openblas pkg-config gfortran",
        "vswhere.exe",
        "vcvars64.bat",
        "Intel\\oneAPI\\setvars.bat",
        "choco install rtools -y --no-progress --force --version=4.0.0.20220206",
        "gfortran\n--version",
        "ifort --version",
        "environment.yml",
        "vswhere.exe",
        "vcvarsarm64.bat",
        "clang-cl --version",
        "flang\n--version",
        "--no-build-isolation",
        "pip install",
        "# Either install all NumPy dev dependencies into a fresh conda environment\nmamba env create -f environment.yml\n\n# Or, install only the required build dependencies\nmamba install python numpy cython compilers openblas meson-python pkg-config\n\n# To build the latest stable release:\npip install numpy --no-build-isolation --no-binary numpy\n\n# To build a development version, you need a local clone of the NumPy git repository:\ngit clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init\npip install . --no-build-isolation",
        "# To build the latest stable release:\npip install numpy --no-binary numpy\n\n# To build a development version, you need a local clone of the NumPy git repository:\ngit clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init\npip install .",
        "git clone https://github.com/numpy/numpy.git\ncd numpy\ngit submodule update --init",
        "mamba env create -f environment.yml\nmamba activate numpy-dev",
        "virtualenvwrapper",
        "pyenv-virtualenv",
        "python -m venv venv\nsource venv/bin/activate",
        "python -m venv venv\nsource venv/bin/activate",
        "python -m venv venv\n.\\venv\\Scripts\\activate",
        "python -m venv venv\n.\\venv\\Scripts\\activate",
        "python -m pip install -r requirements/build_requirements.txt",
        "build-install",
        "spin ipython",
        "spin --help",
        "spin <subcommand> --help",
        "git clean -xdf",
        "# Note: the --no-build-isolation is important!\npip install -e . --no-build-isolation\n\n# To run the tests for, e.g., the `numpy.linalg` module:\npytest numpy/linalg",
        "git clean -xdf",
        "pip uninstall numpy",
        "import numpy"
      ],
      "chunks": [
        {
          "content": "Building from source Building from source Note If you are only trying to install NumPy, we recommend using binaries - see Installation for details on that Building NumPy from source requires setting up system-level dependencies (compilers, BLAS/LAPACK libraries, etc ) first, and then invoking a build The build may be done in order to install NumPy for local usage, develop NumPy itself, or build redistributable binary packages And it may be desired to customize aspects of how the build is done",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "This guide will cover all these aspects In addition, it provides background information on how the NumPy build works, and links to up-to-date guides for generic Python build  packaging documentation that is relevant System-level dependencies NumPy uses compiled code for speed, which means you need compilers and some other system-level (i e, non-Python / non-PyPI) dependencies to build it on your system",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "Note If you are using Conda, you can skip the steps in this section - with the exception of installing compilers for Windows or the Apple Developer Tools for macOS All other dependencies will be installed automatically by the mamba env create -f environment yml command Linux If you want to use the system Python and pip, you will need: C and C++ compilers (typically GCC) Python header files (typically a package named python3-dev or python3-devel) BLAS and LAPACK libraries",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "OpenBLAS is the NumPy default; other variants include Apple Accelerate, MKL, ATLAS and Netlib (or Reference) BLAS and LAPACK pkg-config for dependency detection A Fortran compiler is needed only for running the f2py tests The instructions below include a Fortran compiler, however you can safely leave it out",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "Debian/Ubuntu Linux To install NumPy build requirements, you can do: sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev Alternatively, you can do: sudo apt build-dep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "Fedora To install NumPy build requirements, you can do: sudo dnf install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo dnf builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "CentOS/RHEL To install NumPy build requirements, you can do: sudo yum install gcc-gfortran python3-devel openblas-devel lapack-devel pkgconfig Alternatively, you can do: sudo yum-builddep numpy This command installs whatever is needed to build NumPy, with the advantage that new dependencies or updates to required versions are handled by the package managers Arch To install NumPy build requirements, you can do: sudo pacman -S gcc-fortran openblas pkgconf macOS Install Apple Developer Tools",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "An easy way to do this is to open a terminal window, enter the command: xcode-select --install and follow the prompts Apple Developer Tools includes Git, the Clang C/C++ compilers, and other development utilities that may be required Do not use the macOS system Python Instead, install Python with the python org installer or with a package manager like Homebrew, MacPorts or Fink On macOS =13",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "3, the easiest build option is to use Accelerate, which is already installed and will be automatically used by default On older macOS versions you need a different BLAS library, most likely OpenBLAS, plus pkg-config to detect OpenBLAS These are easiest to install with Homebrew: brew install openblas pkg-config gfortran Windows On Windows, the use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "If you dont need to run the f2py tests, simply using MSVC is easiest Otherwise, you will need one of these sets of compilers: MSVC + Intel Fortran (ifort) Intel compilers (icc, ifort) Mingw-w64 compilers (gcc, g++, gfortran) Compared to macOS and Linux, building NumPy on Windows is a little more difficult, due to the need to set up these compilers It is not possible to just call a one-liner on the command prompt as you would on other platforms",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "First, install Microsoft Visual Studio - the 2019 Community Edition or any newer version will work (see the Visual Studio download site) This is needed even if you use the MinGW-w64 or Intel compilers, in order to ensure you have the Windows Universal C Runtime (the other components of Visual Studio are not needed when using Mingw-w64, and can be deselected if desired, to save disk space) The recommended version of the UCRT is = 10 0 22621 0",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "MSVC The MSVC installer does not put the compilers on the system path, and the install location may change To query the install location, MSVC comes with a vswhere exe command-line utility And to make the C/C++ compilers available inside the shell you are using, you need to run a bat file for the correct bitness and architecture (e g , for 64-bit Intel CPUs, use vcvars64 bat)",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "If using a Conda environment while a version of Visual Studio 2019+ is installed that includes the MSVC v142 package (VS 2019 C++ x86/x64 build tools), activating the conda environment should cause Visual Studio to be found and the appropriate bat file executed to set these variables For detailed guidance, see Use the Microsoft C++ toolset from the command line Intel Similar to MSVC, the Intel compilers are designed to be used with an activation script (Intel\\oneAPI\\setvars",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "bat) that you run in the shell you are using This makes the compilers available on the path For detailed guidance, see Get Started with the Intel oneAPI HPC Toolkit for Windows MinGW-w64 There are several sources of binaries for MinGW-w64 We recommend the RTools versions, which can be installed with Chocolatey (see Chocolatey install instructions here): choco install rtools -y --no-progress --force --version=4 0 0 20220206 Note Compilers should be on the system path (i e",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": ", the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH You can use any shell (e g , Powershell, cmd or Git Bash) to invoke a build To check that this is the case, try invoking a Fortran compiler in the shell you use (e g , gfortran --version or ifort --version)",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "Warning When using a conda environment it is possible that the environment creation will not work due to an outdated Fortran compiler If that happens, remove the compilers entry from environment yml and try again The Fortran compiler should be installed as described in this section Windows on ARM64 In Windows on ARM64, the set of a compiler options that are available for building NumPy are limited Compilers such as GCC and GFortran are not yet supported for Windows on ARM64",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "Currently, the NumPy build for Windows on ARM64 is supported with MSVC and LLVM toolchains The use of a Fortran compiler is more tricky than on other platforms, because MSVC does not support Fortran, and gfortran and MSVC cant be used together If you dont need to run the f2py tests, simply using MSVC is easiest",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Otherwise, you will need the following set of compilers: MSVC + flang (cl, flang) LLVM + flang (clang-cl, flang) First, install Microsoft Visual Studio - the 2022 Community Edition will work (see the Visual Studio download site) Ensure that you have installed necessary Visual Studio components for building NumPy on WoA from here To use the flang compiler for Windows on ARM64, install Latest LLVM toolchain for WoA from here",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "MSVC The MSVC installer does not put the compilers on the system path, and the install location may change To query the install location, MSVC comes with a vswhere exe command-line utility And to make the C/C++ compilers available inside the shell you are using, you need to run a bat file for the correct bitness and architecture (e g , for ARM64-based CPUs, use vcvarsarm64 bat) For detailed guidance, see Use the Microsoft C++ toolset from the command line",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "LLVM Similar to MSVC, LLVM does not put the compilers on the system path To set system path for LLVM compilers, users may need to use set command to put compilers on the system path To check compilers path for LLVMs clang-cl, try invoking LLVMs clang-cl compiler in the shell you use (clang-cl --version) Note Compilers should be on the system path (i e",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": ", the PATH environment variable should contain the directory in which the compiler executables can be found) in order to be found, with the exception of MSVC which will be found automatically if and only if there are no other compilers on the PATH You can use any shell (e g , Powershell, cmd or Git Bash) to invoke a build To check that this is the case, try invoking a Fortran compiler in the shell you use (e g , flang --version)",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "Warning Currently, Conda environment is not yet supported officially on Windows on ARM64 The present approach uses virtualenv for building NumPy from source on Windows on ARM64 Building NumPy from source If you want to only install NumPy from source once and not do any development work, then the recommended way to build and install is to use pip Otherwise, conda is recommended Note If you dont have a conda installation yet, we recommend using Miniforge; any conda flavor will work though",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "Building from source to use NumPy Conda env If you are using a conda environment, pip is still the tool you use to invoke a from-source build of NumPy It is important to always use the --no-build-isolation flag to the pip install command, to avoid building against a numpy wheel from PyPI",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "In order for that to work you must first install the remaining build dependencies into the conda environment:  Either install all NumPy dev dependencies into a fresh conda environment mamba env create -f environment",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "yml  Or, install only the required build dependencies mamba install python numpy cython compilers openblas meson-python pkg-config  To build the latest stable release: pip install numpy --no-build-isolation --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github com/numpy/numpy git cd numpy git submodule update --init pip install",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": "--no-build-isolation Warning On Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the pip install command to fail These variables are only needed for flang and can be safely unset prior to running pip install Virtual env or system Python  To build the latest stable release: pip install numpy --no-binary numpy  To build a development version, you need a local clone of the NumPy git repository: git clone https://github com/numpy/numpy",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "git cd numpy git submodule update --init pip install Building from source for NumPy development If you want to build from source in order to work on NumPy itself, first clone the NumPy repository: git clone https://github com/numpy/numpy",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "git cd numpy git submodule update --init Then you want to do the following: Create a dedicated development environment (virtual environment or conda environment), Install all needed dependencies (build, and also test, doc and optional dependencies), Build NumPy with the spin developer interface",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "Step (3) is always the same, steps (1) and (2) are different between conda and virtual environments: Conda env To create a numpy-dev development environment with every required and optional dependency installed, run: mamba env create -f environment yml mamba activate numpy-dev Virtual env or system Python Note There are many tools to manage virtual environments, like venv, virtualenv/virtualenvwrapper, pyenv/pyenv-virtualenv, Poetry, PDM, Hatch, and more",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "Here we use the basic venv tool that is part of the Python stdlib You can use any other tool; all we need is an activated Python environment Create and activate a virtual environment in a new directory named venv ( note that the exact activation command may be different based on your OS and shell - see How venvs work in the venv docs) Linux python -m venv venv source venv/bin/activate macOS python -m venv venv source venv/bin/activate Windows python -m venv venv",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "\\venv\\Scripts\\activate Windows on ARM64 python -m venv venv \\venv\\Scripts\\activate Note Building NumPy with BLAS and LAPACK functions requires OpenBLAS library at Runtime In Windows on ARM64, this can be done by setting up pkg-config for OpenBLAS dependency The build steps for OpenBLAS for Windows on ARM64 can be found here Then install the Python-level dependencies from PyPI with: python -m pip install -r requirements/build_requirements",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "txt To build NumPy in an activated development environment, run: spin build This will install NumPy inside the repository (by default in a build-install directory) You can then run tests (spin test), drop into IPython (spin ipython), or take other development steps like build the html documentation or running benchmarks The spin interface is self-documenting, so please see spin --help and spin subcommand --help for detailed guidance",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "Warning In an activated conda environment on Windows, the AR, LD, and LDFLAGS environment variables may be set, which will cause the build to fail These variables are only needed for flang and can be safely unset for build IDE support  editable installs While the spin interface is our recommended way of working on NumPy, it has one limitation: because of the custom install location, NumPy installed using spin will not be recognized automatically within an IDE (e g",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": ", for running a script via a run button, or setting breakpoints visually) This will work better with an in-place build (or editable install) Editable installs are supported It is important to understand that you may use either an editable install or ``spin`` in a given repository clone, but not both If you use editable installs, you have to use pytest and other development tools directly instead of using spin",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": "To use an editable install, ensure you start from a clean repository (run git clean -xdf if youve built with spin before) and have all dependencies set up correctly as described higher up on this page Then do:  Note: the --no-build-isolation is important pip install -e --no-build-isolation  To run the tests for, e g , the `numpy linalg` module: pytest numpy/linalg When making changes to NumPy code, including to compiled code, there is no need to manually rebuild or reinstall",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "NumPy is automatically rebuilt each time NumPy is imported by the Python interpreter; see the meson-python documentation on editable installs for more details on how that works under the hood When you run git clean -xdf, which removes the built extension modules, remember to also uninstall NumPy with pip uninstall numpy Warning Note that editable installs are fundamentally incomplete installs",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "Their only guarantee is that import numpy works - so they are suitable for working on NumPy itself, and for working on pure Python packages that depend on NumPy Headers, entrypoints, and other such things may not be available from an editable install",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "Customizing builds Compiler selection and customizing a build BLAS and LAPACK Cross compilation Building redistributable binaries Background information Understanding Meson Introspecting build steps Meson and distutils ways of doing things previous Testing the numpy",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_37"
        },
        {
          "content": "i typemaps next Compiler selection and customizing a build On this page System-level dependencies Building NumPy from source Building from source to use NumPy Building from source for NumPy development Customizing builds Background information",
          "url": "https://numpy.org/devdocs/building/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_38"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/dev/index.html#main-content",
      "title": "Contributing to NumPy \u2014 NumPy v2.4.dev0 Manual",
      "content": "Contributing to NumPy Contributing to NumPy Not a coder? Not a problem! NumPy is multi-faceted, and we can use a lot of help. These are all activities wed like to get help with (theyre all important, so we list them in alphabetical order): Code maintenance and development Community coordination DevOps Developing educational content  narrative documentation Fundraising Marketing Project management Translating content Website design and development Writing technical documentation We understand that everyone has a different level of experience, also NumPy is a pretty well-established project, so its hard to make assumptions about an ideal first-time-contributor. So, thats why we dont mark issues with the good-first-issue label. Instead, youll find issues labeled Sprintable. These issues can either be: Easily fixed when you have guidance from an experienced contributor (perfect for working in a sprint). A learning opportunity for those ready to dive deeper, even if youre not in a sprint. Additionally, depending on your prior experience, some Sprintable issues might be easy, while others could be more challenging for you. The rest of this document discusses working on the NumPy code base and documentation. Were in the process of updating our descriptions of other activities and roles. If you are interested in these other activities, please contact us! You can do this via the numpy-discussion mailing list, or on GitHub (open an issue or comment on a relevant issue). These are our preferred communication channels (open source is open by nature!), however if you prefer to discuss in a more private space first, you can do so on Slack (see numpy.org/contribute for details). Development process - summary Heres the short summary, complete TOC links are below: If you are a first-time contributor: Go to numpy/numpy and click the fork button to create your own copy of the project. Clone the project to your local computer: git clone --recurse-submodules https://github.com/your-username/numpy.git Change the directory: cd numpy Add the upstream repository: git remote add upstream https://github.com/numpy/numpy.git Now, git remote -v will show two remote repositories named: upstream, which refers to the numpy repository origin, which refers to your personal fork Pull the latest changes from upstream, including tags: git checkout main git pull upstream main --tags Initialize numpys submodules: git submodule update --init Develop your contribution: Create a branch for the feature you want to work on. Since the branch name will appear in the merge message, use a sensible name such as linspace-speedups: git checkout -b linspace-speedups Commit locally as you progress (git add and git commit) Use a properly formatted commit message, write tests that fail before your change and pass afterward, run all the tests locally. Be sure to document any changed behavior in docstrings, keeping to the NumPy docstring standard. To submit your contribution: Push your changes back to your fork on GitHub: git push origin linspace-speedups Go to GitHub. The new branch will show up with a green Pull Request button. Make sure the title and message are clear, concise, and self- explanatory. Then click the button to submit it. If your commit introduces a new feature or changes functionality, post on the mailing list to explain your changes. For bug fixes, documentation updates, etc., this is generally not necessary, though if you do not get any reaction, do feel free to ask for review. Review process: Reviewers (the other developers and interested community members) will write inline and/or general comments on your Pull Request (PR) to help you improve its implementation, documentation and style. Every single developer working on the project has their code reviewed, and weve come to see it as friendly conversation from which we all learn and the overall code quality benefits. Therefore, please dont let the review discourage you from contributing: its only aim is to improve the quality of project, not to criticize (we are, after all, very grateful for the time youre donating!). See our Reviewer Guidelines for more information. To update your PR, make your changes on your local repository, commit, run tests, and only if they succeed push to your fork. As soon as those changes are pushed up (to the same branch as before) the PR will update automatically. If you have no idea how to fix the test failures, you may push your changes anyway and ask for help in a PR comment. Various continuous integration (CI) services are triggered after each PR update to build the code, run unit tests, measure code coverage and check coding style of your branch. The CI tests must pass before your PR can be merged. If CI fails, you can find out why by clicking on the failed icon (red cross) and inspecting the build and test log. To avoid overuse and waste of this resource, test your work locally before committing. A PR must be approved by at least one core team member before merging. Approval means the core team member has carefully reviewed the changes, and the PR is ready for merging. Document changes Beyond changes to a functions docstring and possible description in the general documentation, if your change introduces any user-facing modifications they may need to be mentioned in the release notes. To add your change to the release notes, you need to create a short file with a summary and place it in doc/release/upcoming_changes. The file doc/release/upcoming_changes/README.rst details the format and filename conventions. If your change introduces a deprecation, make sure to discuss this first on GitHub or the mailing list first. If agreement on the deprecation is reached, follow NEP 23 deprecation policy to add the deprecation. Cross referencing issues If the PR relates to any issues, you can add the text xref gh-xxxx where xxxx is the number of the issue to github comments. Likewise, if the PR solves an issue, replace the xref with closes, fixes or any of the other flavors github accepts. In the source code, be sure to preface any issue or PR reference with gh-xxxx. For a more detailed discussion, read on and follow the links at the bottom of this page. Guidelines All code should have tests (see test coverage below for more details). All code should be documented. No changes are ever committed without review and approval by a core team member. Please ask politely on the PR or on the mailing list if you get no response to your pull request within a week. Stylistic guidelines Set up your editor to follow PEP 8 (remove trailing white space, no tabs, etc.). Check code with ruff. Use NumPy data types instead of strings (np.uint8 instead of \"uint8\"). Use the following import conventions: import numpy as np For C code, see NEP 45. Test coverage Pull requests (PRs) that modify code should either have new tests, or modify existing tests to fail before the PR and pass afterwards. You should run the tests before pushing a PR. Running NumPys test suite locally requires some additional packages, such as pytest and hypothesis. The additional testing dependencies are listed in requirements/test_requirements.txt in the top-level directory, and can conveniently be installed with:  python -m pip install -r requirements/test_requirements.txt Tests for a module should ideally cover all code in that module, i.e., statement coverage should be at 100. To measure the test coverage, run:  spin test --coverage This will create a report in html format at build/coverage, which can be viewed with your browser, e.g.:  firefox build/coverage/index.html Building docs To build the HTML documentation, use: spin docs You can also run make from the doc directory. make help lists all targets. To get the appropriate dependencies and other requirements, see Building the NumPy API and reference docs. Development process - details The rest of the story Setting up and using your development environment Recommended development setup Using virtual environments Building from source Testing builds Other build options Running tests Running linting Rebuilding  cleaning the workspace Debugging Understanding the code  getting started Building the NumPy API and reference docs Development environments Prerequisites Instructions Development workflow Basic workflow Additional things you might want to do Advanced debugging tools Finding C errors with additional tooling Using GitHub Codespaces for NumPy development GitHub Codespaces What is a codespace? Forking the NumPy repository Starting GitHub Codespaces Quick workspace tour Development workflow with GitHub Codespaces Rendering the NumPy documentation FAQs and troubleshooting Reviewer guidelines Who can be a reviewer? Communication guidelines Reviewer checklist Standard replies for reviewing NumPy benchmarks Usage Benchmarking versions Writing benchmarks NumPy C style guide For downstream package authors Understanding NumPys versioning and API/ABI stability Testing against the NumPy main branch or pre-releases Adding a dependency on NumPy Releasing a version How to prepare a release Step-by-step directions Branch walkthrough NumPy governance NumPy project governance and decision-making How to contribute to the NumPy documentation Documentation team meetings Whats needed Contributing fixes Contributing new pages Contributing indirectly Documentation style Documentation reading NumPy-specific workflow is in numpy-development-workflow. previous Meson and distutils ways of doing things next Setting up and using your development environment On this page Development process - summary Guidelines Stylistic guidelines Test coverage Building docs Development process - details",
      "code_blocks": [
        "git clone --recurse-submodules https://github.com/your-username/numpy.git",
        "git remote add upstream https://github.com/numpy/numpy.git",
        "git remote -v",
        "git checkout main\ngit pull upstream main --tags",
        "git submodule update --init",
        "git checkout -b linspace-speedups",
        "git push origin linspace-speedups",
        "doc/release/upcoming_changes",
        "doc/release/upcoming_changes/README.rst",
        "xref gh-xxxx",
        "import numpy as np",
        "requirements/test_requirements.txt",
        "$ python -m pip install -r requirements/test_requirements.txt",
        "$ spin test --coverage",
        "build/coverage",
        "$ firefox build/coverage/index.html"
      ],
      "chunks": [
        {
          "content": "Contributing to NumPy Contributing to NumPy Not a coder Not a problem NumPy is multi-faceted, and we can use a lot of help",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "These are all activities wed like to get help with (theyre all important, so we list them in alphabetical order): Code maintenance and development Community coordination DevOps Developing educational content  narrative documentation Fundraising Marketing Project management Translating content Website design and development Writing technical documentation We understand that everyone has a different level of experience, also NumPy is a pretty well-established project, so its hard to make assumptions about an ideal first-time-contributor",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "So, thats why we dont mark issues with the good-first-issue label Instead, youll find issues labeled Sprintable These issues can either be: Easily fixed when you have guidance from an experienced contributor (perfect for working in a sprint) A learning opportunity for those ready to dive deeper, even if youre not in a sprint Additionally, depending on your prior experience, some Sprintable issues might be easy, while others could be more challenging for you",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "The rest of this document discusses working on the NumPy code base and documentation Were in the process of updating our descriptions of other activities and roles If you are interested in these other activities, please contact us You can do this via the numpy-discussion mailing list, or on GitHub (open an issue or comment on a relevant issue) These are our preferred communication channels (open source is open by nature",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "), however if you prefer to discuss in a more private space first, you can do so on Slack (see numpy org/contribute for details) Development process - summary Heres the short summary, complete TOC links are below: If you are a first-time contributor: Go to numpy/numpy and click the fork button to create your own copy of the project Clone the project to your local computer: git clone --recurse-submodules https://github com/your-username/numpy",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "git Change the directory: cd numpy Add the upstream repository: git remote add upstream https://github com/numpy/numpy",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "git Now, git remote -v will show two remote repositories named: upstream, which refers to the numpy repository origin, which refers to your personal fork Pull the latest changes from upstream, including tags: git checkout main git pull upstream main --tags Initialize numpys submodules: git submodule update --init Develop your contribution: Create a branch for the feature you want to work on",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "Since the branch name will appear in the merge message, use a sensible name such as linspace-speedups: git checkout -b linspace-speedups Commit locally as you progress (git add and git commit) Use a properly formatted commit message, write tests that fail before your change and pass afterward, run all the tests locally Be sure to document any changed behavior in docstrings, keeping to the NumPy docstring standard",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "To submit your contribution: Push your changes back to your fork on GitHub: git push origin linspace-speedups Go to GitHub The new branch will show up with a green Pull Request button Make sure the title and message are clear, concise, and self- explanatory Then click the button to submit it If your commit introduces a new feature or changes functionality, post on the mailing list to explain your changes For bug fixes, documentation updates, etc",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": ", this is generally not necessary, though if you do not get any reaction, do feel free to ask for review Review process: Reviewers (the other developers and interested community members) will write inline and/or general comments on your Pull Request (PR) to help you improve its implementation, documentation and style Every single developer working on the project has their code reviewed, and weve come to see it as friendly conversation from which we all learn and the overall code quality benefits",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "Therefore, please dont let the review discourage you from contributing: its only aim is to improve the quality of project, not to criticize (we are, after all, very grateful for the time youre donating ) See our Reviewer Guidelines for more information To update your PR, make your changes on your local repository, commit, run tests, and only if they succeed push to your fork As soon as those changes are pushed up (to the same branch as before) the PR will update automatically",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "If you have no idea how to fix the test failures, you may push your changes anyway and ask for help in a PR comment Various continuous integration (CI) services are triggered after each PR update to build the code, run unit tests, measure code coverage and check coding style of your branch The CI tests must pass before your PR can be merged If CI fails, you can find out why by clicking on the failed icon (red cross) and inspecting the build and test log",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "To avoid overuse and waste of this resource, test your work locally before committing A PR must be approved by at least one core team member before merging Approval means the core team member has carefully reviewed the changes, and the PR is ready for merging Document changes Beyond changes to a functions docstring and possible description in the general documentation, if your change introduces any user-facing modifications they may need to be mentioned in the release notes",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "To add your change to the release notes, you need to create a short file with a summary and place it in doc/release/upcoming_changes The file doc/release/upcoming_changes/README rst details the format and filename conventions If your change introduces a deprecation, make sure to discuss this first on GitHub or the mailing list first If agreement on the deprecation is reached, follow NEP 23 deprecation policy to add the deprecation",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "Cross referencing issues If the PR relates to any issues, you can add the text xref gh-xxxx where xxxx is the number of the issue to github comments Likewise, if the PR solves an issue, replace the xref with closes, fixes or any of the other flavors github accepts In the source code, be sure to preface any issue or PR reference with gh-xxxx For a more detailed discussion, read on and follow the links at the bottom of this page",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "Guidelines All code should have tests (see test coverage below for more details) All code should be documented No changes are ever committed without review and approval by a core team member Please ask politely on the PR or on the mailing list if you get no response to your pull request within a week Stylistic guidelines Set up your editor to follow PEP 8 (remove trailing white space, no tabs, etc ) Check code with ruff Use NumPy data types instead of strings (np uint8 instead of \"uint8\")",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "Use the following import conventions: import numpy as np For C code, see NEP 45 Test coverage Pull requests (PRs) that modify code should either have new tests, or modify existing tests to fail before the PR and pass afterwards You should run the tests before pushing a PR Running NumPys test suite locally requires some additional packages, such as pytest and hypothesis The additional testing dependencies are listed in requirements/test_requirements",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "txt in the top-level directory, and can conveniently be installed with:  python -m pip install -r requirements/test_requirements txt Tests for a module should ideally cover all code in that module, i e , statement coverage should be at 100 To measure the test coverage, run:  spin test --coverage This will create a report in html format at build/coverage, which can be viewed with your browser, e g :  firefox build/coverage/index",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "html Building docs To build the HTML documentation, use: spin docs You can also run make from the doc directory make help lists all targets To get the appropriate dependencies and other requirements, see Building the NumPy API and reference docs",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "Development process - details The rest of the story Setting up and using your development environment Recommended development setup Using virtual environments Building from source Testing builds Other build options Running tests Running linting Rebuilding  cleaning the workspace Debugging Understanding the code  getting started Building the NumPy API and reference docs Development environments Prerequisites Instructions Development workflow Basic workflow Additional things you might want to do Advanced debugging tools Finding C errors with additional tooling Using GitHub Codespaces for NumPy development GitHub Codespaces What is a codespace",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "Forking the NumPy repository Starting GitHub Codespaces Quick workspace tour Development workflow with GitHub Codespaces Rendering the NumPy documentation FAQs and troubleshooting Reviewer guidelines Who can be a reviewer",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "Communication guidelines Reviewer checklist Standard replies for reviewing NumPy benchmarks Usage Benchmarking versions Writing benchmarks NumPy C style guide For downstream package authors Understanding NumPys versioning and API/ABI stability Testing against the NumPy main branch or pre-releases Adding a dependency on NumPy Releasing a version How to prepare a release Step-by-step directions Branch walkthrough NumPy governance NumPy project governance and decision-making How to contribute to the NumPy documentation Documentation team meetings Whats needed Contributing fixes Contributing new pages Contributing indirectly Documentation style Documentation reading NumPy-specific workflow is in numpy-development-workflow",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "previous Meson and distutils ways of doing things next Setting up and using your development environment On this page Development process - summary Guidelines Stylistic guidelines Test coverage Building docs Development process - details",
          "url": "https://numpy.org/devdocs/dev/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_22"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/devdocs/release.html#main-content",
      "title": "Release notes \u2014 NumPy v2.4.dev0 Manual",
      "content": "NumPy user guide Release notes Release notes 2.4.0 Highlights Deprecations Compatibility notes Performance improvements and changes Changes 2.3.1 Contributors Pull requests merged 2.3.0 Highlights New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2.2.6 Contributors Pull requests merged 2.2.5 Contributors Pull requests merged 2.2.4 Contributors Pull requests merged 2.2.3 Contributors Pull requests merged 2.2.2 Contributors Pull requests merged 2.2.1 Contributors Pull requests merged 2.2.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 2.1.3 Improvements Changes Contributors Pull requests merged 2.1.2 Contributors Pull requests merged 2.1.1 Contributors Pull requests merged 2.1.0 New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2.0.2 Contributors Pull requests merged 2.0.1 Improvements Contributors Pull requests merged 2.0.0 Highlights NumPy 2.0 Python API removals Deprecations Expired deprecations Compatibility notes C API changes NumPy 2.0 C API removals New Features Improvements Changes 1.26.4 Contributors Pull requests merged 1.26.3 Compatibility Improvements Contributors Pull requests merged 1.26.2 Contributors Pull requests merged 1.26.1 Build system changes New features Contributors Pull requests merged 1.26.0 New Features Improvements Build system changes Contributors Pull requests merged 1.25.2 Contributors Pull requests merged 1.25.1 Contributors Pull requests merged 1.25.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1.24.4 Contributors Pull requests merged 1.24.3 Contributors Pull requests merged 1.24.2 Contributors Pull requests merged 1.24.1 Contributors Pull requests merged 1.24.0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1.23.5 Contributors Pull requests merged 1.23.4 Contributors Pull requests merged 1.23.3 Contributors Pull requests merged 1.23.2 Contributors Pull requests merged 1.23.1 Contributors Pull requests merged 1.23.0 New functions Deprecations Expired deprecations New Features Compatibility notes Improvements Performance improvements and changes 1.22.4 Contributors Pull requests merged 1.22.3 Contributors Pull requests merged 1.22.2 Contributors Pull requests merged 1.22.1 Contributors Pull requests merged 1.22.0 Expired deprecations Deprecations Compatibility notes C API changes New Features Improvements 1.21.6 1.21.5 Contributors Pull requests merged 1.21.4 Contributors Pull requests merged 1.21.3 Contributors Pull requests merged 1.21.2 Contributors Pull requests merged 1.21.1 Contributors Pull requests merged 1.21.0 New functions Expired deprecations Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements Changes 1.20.3 Contributors Pull requests merged 1.20.2 Contributors Pull requests merged 1.20.1 Highlights Contributors Pull requests merged 1.20.0 New functions Deprecations Future Changes Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements and changes Changes 1.19.5 Contributors Pull requests merged 1.19.4 Contributors Pull requests merged 1.19.3 Contributors Pull requests merged 1.19.2 Improvements Contributors Pull requests merged 1.19.1 Contributors Pull requests merged 1.19.0 Highlights Expired deprecations Compatibility notes Deprecations C API changes New Features Improvements Improve detection of CPU features Changes 1.18.5 Contributors Pull requests merged 1.18.4 Contributors Pull requests merged 1.18.3 Highlights Contributors Pull requests merged 1.18.2 Contributors Pull requests merged 1.18.1 Contributors Pull requests merged 1.18.0 Highlights New functions Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Changes 1.17.5 Contributors Pull requests merged 1.17.4 Highlights Contributors Pull requests merged 1.17.3 Highlights Compatibility notes Contributors Pull requests merged 1.17.2 Contributors Pull requests merged 1.17.1 Contributors Pull requests merged 1.17.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1.16.6 Highlights New functions Compatibility notes Improvements Contributors Pull requests merged 1.16.5 Contributors Pull requests merged 1.16.4 New deprecations Compatibility notes Changes Contributors Pull requests merged 1.16.3 Compatibility notes Improvements Changes 1.16.2 Compatibility notes Contributors Pull requests merged 1.16.1 Contributors Enhancements Compatibility notes New Features Improvements Changes 1.16.0 Highlights New functions New deprecations Expired deprecations Future changes Compatibility notes C API changes New Features Improvements Changes 1.15.4 Compatibility Note Contributors Pull requests merged 1.15.3 Compatibility Note Contributors Pull requests merged 1.15.2 Compatibility Note Contributors Pull requests merged 1.15.1 Compatibility Note Contributors Pull requests merged 1.15.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements 1.14.6 Contributors Pull requests merged 1.14.5 Contributors Pull requests merged 1.14.4 Contributors Pull requests merged 1.14.3 Contributors Pull requests merged 1.14.2 Contributors Pull requests merged 1.14.1 Contributors Pull requests merged 1.14.0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1.13.3 Contributors Pull requests merged 1.13.2 Contributors Pull requests merged 1.13.1 Pull requests merged Contributors 1.13.0 Highlights New functions Deprecations Future Changes Build System Changes Compatibility notes C API changes New Features Improvements Changes 1.12.1 Bugs Fixed 1.12.0 Highlights Dropped Support Added Support Build System Changes Deprecations Future Changes Compatibility notes New Features Improvements Changes 1.11.3 Contributors to maintenance/1.11.3 Pull Requests Merged 1.11.2 Pull Requests Merged 1.11.1 Fixes Merged 1.11.0 Highlights Build System Changes Future Changes Compatibility notes New Features Improvements Changes Deprecations FutureWarnings 1.10.4 Compatibility notes Issues Fixed Merged PRs 1.10.3 1.10.2 Compatibility notes Issues Fixed Merged PRs Notes 1.10.1 1.10.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations 1.9.2 Issues fixed 1.9.1 Issues fixed 1.9.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Deprecations 1.8.2 Issues fixed 1.8.1 Issues fixed Changes Deprecations 1.8.0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations Authors 1.7.2 Issues fixed 1.7.1 Issues fixed 1.7.0 Highlights Compatibility notes New features Changes Deprecations 1.6.2 Issues fixed Changes 1.6.1 Issues Fixed 1.6.0 Highlights New features Changes Deprecated features Removed features 1.5.0 Highlights New features Changes 1.4.0 Highlights New features Improvements Deprecations Internal changes 1.3.0 Highlights New features Deprecated features Documentation changes New C API Internal changes previous Glossary next NumPy 2.4.0 Release Notes",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy user guide Release notes Release notes 2 4 0 Highlights Deprecations Compatibility notes Performance improvements and changes Changes 2 3 1 Contributors Pull requests merged 2 3 0 Highlights New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2 2 6 Contributors Pull requests merged 2 2 5 Contributors Pull requests merged 2 2 4 Contributors Pull requests merged 2 2 3 Contributors Pull requests merged 2 2",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "2 Contributors Pull requests merged 2 2 1 Contributors Pull requests merged 2 2 0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 2 1 3 Improvements Changes Contributors Pull requests merged 2 1 2 Contributors Pull requests merged 2 1 1 Contributors Pull requests merged 2 1 0 New functions Deprecations Expired deprecations C API changes New Features Improvements Performance improvements and changes Changes 2 0",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "2 Contributors Pull requests merged 2 0 1 Improvements Contributors Pull requests merged 2 0 0 Highlights NumPy 2 0 Python API removals Deprecations Expired deprecations Compatibility notes C API changes NumPy 2 0 C API removals New Features Improvements Changes 1 26 4 Contributors Pull requests merged 1 26 3 Compatibility Improvements Contributors Pull requests merged 1 26 2 Contributors Pull requests merged 1 26 1 Build system changes New features Contributors Pull requests merged 1 26",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "0 New Features Improvements Build system changes Contributors Pull requests merged 1 25 2 Contributors Pull requests merged 1 25 1 Contributors Pull requests merged 1 25 0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1 24 4 Contributors Pull requests merged 1 24 3 Contributors Pull requests merged 1 24 2 Contributors Pull requests merged 1 24 1 Contributors Pull requests merged 1 24",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "0 Deprecations Expired deprecations Compatibility notes New Features Improvements Performance improvements and changes Changes 1 23 5 Contributors Pull requests merged 1 23 4 Contributors Pull requests merged 1 23 3 Contributors Pull requests merged 1 23 2 Contributors Pull requests merged 1 23 1 Contributors Pull requests merged 1 23 0 New functions Deprecations Expired deprecations New Features Compatibility notes Improvements Performance improvements and changes 1 22",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "4 Contributors Pull requests merged 1 22 3 Contributors Pull requests merged 1 22 2 Contributors Pull requests merged 1 22 1 Contributors Pull requests merged 1 22 0 Expired deprecations Deprecations Compatibility notes C API changes New Features Improvements 1 21 6 1 21 5 Contributors Pull requests merged 1 21 4 Contributors Pull requests merged 1 21 3 Contributors Pull requests merged 1 21 2 Contributors Pull requests merged 1 21 1 Contributors Pull requests merged 1 21",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "0 New functions Expired deprecations Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements Changes 1 20 3 Contributors Pull requests merged 1 20 2 Contributors Pull requests merged 1 20 1 Highlights Contributors Pull requests merged 1 20 0 New functions Deprecations Future Changes Expired deprecations Compatibility notes C API changes New Features Improvements Performance improvements and changes Changes 1 19",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "5 Contributors Pull requests merged 1 19 4 Contributors Pull requests merged 1 19 3 Contributors Pull requests merged 1 19 2 Improvements Contributors Pull requests merged 1 19 1 Contributors Pull requests merged 1 19 0 Highlights Expired deprecations Compatibility notes Deprecations C API changes New Features Improvements Improve detection of CPU features Changes 1 18 5 Contributors Pull requests merged 1 18 4 Contributors Pull requests merged 1 18",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "3 Highlights Contributors Pull requests merged 1 18 2 Contributors Pull requests merged 1 18 1 Contributors Pull requests merged 1 18 0 Highlights New functions Deprecations Expired deprecations Compatibility notes C API changes New Features Improvements Changes 1 17 5 Contributors Pull requests merged 1 17 4 Highlights Contributors Pull requests merged 1 17 3 Highlights Compatibility notes Contributors Pull requests merged 1 17 2 Contributors Pull requests merged 1 17",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "1 Contributors Pull requests merged 1 17 0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1 16 6 Highlights New functions Compatibility notes Improvements Contributors Pull requests merged 1 16 5 Contributors Pull requests merged 1 16 4 New deprecations Compatibility notes Changes Contributors Pull requests merged 1 16 3 Compatibility notes Improvements Changes 1 16 2 Compatibility notes Contributors Pull requests merged 1",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "16 1 Contributors Enhancements Compatibility notes New Features Improvements Changes 1 16 0 Highlights New functions New deprecations Expired deprecations Future changes Compatibility notes C API changes New Features Improvements Changes 1 15 4 Compatibility Note Contributors Pull requests merged 1 15 3 Compatibility Note Contributors Pull requests merged 1 15 2 Compatibility Note Contributors Pull requests merged 1 15 1 Compatibility Note Contributors Pull requests merged 1 15",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements 1 14 6 Contributors Pull requests merged 1 14 5 Contributors Pull requests merged 1 14 4 Contributors Pull requests merged 1 14 3 Contributors Pull requests merged 1 14 2 Contributors Pull requests merged 1 14 1 Contributors Pull requests merged 1 14 0 Highlights New functions Deprecations Future Changes Compatibility notes C API changes New Features Improvements Changes 1 13",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "3 Contributors Pull requests merged 1 13 2 Contributors Pull requests merged 1 13 1 Pull requests merged Contributors 1 13 0 Highlights New functions Deprecations Future Changes Build System Changes Compatibility notes C API changes New Features Improvements Changes 1 12 1 Bugs Fixed 1 12 0 Highlights Dropped Support Added Support Build System Changes Deprecations Future Changes Compatibility notes New Features Improvements Changes 1 11 3 Contributors to maintenance/1 11 3 Pull Requests Merged 1",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "11 2 Pull Requests Merged 1 11 1 Fixes Merged 1 11 0 Highlights Build System Changes Future Changes Compatibility notes New Features Improvements Changes Deprecations FutureWarnings 1 10 4 Compatibility notes Issues Fixed Merged PRs 1 10 3 1 10 2 Compatibility notes Issues Fixed Merged PRs Notes 1 10 1 1 10 0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations 1 9 2 Issues fixed 1 9 1 Issues fixed 1 9",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Deprecations 1 8 2 Issues fixed 1 8 1 Issues fixed Changes Deprecations 1 8 0 Highlights Dropped Support Future Changes Compatibility notes New Features Improvements Changes Deprecations Authors 1 7 2 Issues fixed 1 7 1 Issues fixed 1 7 0 Highlights Compatibility notes New features Changes Deprecations 1 6 2 Issues fixed Changes 1 6 1 Issues Fixed 1 6",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "0 Highlights New features Changes Deprecated features Removed features 1 5 0 Highlights New features Changes 1 4 0 Highlights New features Improvements Deprecations Internal changes 1 3 0 Highlights New features Deprecated features Documentation changes New C API Internal changes previous Glossary next NumPy 2 4 0 Release Notes",
          "url": "https://numpy.org/devdocs/release.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_15"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/doc/stable/index.html#main-content",
      "title": "NumPy documentation \u2014 NumPy v2.3 Manual",
      "content": "NumPy documentation Version: 2.3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Getting started New to NumPy? Check out the Absolute Beginners Guide. It contains an introduction to NumPys main concepts and links to additional tutorials. To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation. To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts. To the reference guide Contributors guide Want to add to the codebase? Can help add translation or a flowchart to the documentation? The contributing guidelines will guide you through the process of improving NumPy. To the contributors guide next NumPy user guide",
      "code_blocks": [],
      "chunks": [
        {
          "content": "NumPy documentation Version: 2 3 Download documentation: Historical versions of documentation Useful links: Installation  Source Repository  Issue Tracker  QA Support  Mailing List NumPy is the fundamental package for scientific computing in Python",
          "url": "https://numpy.org/doc/stable/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more Getting started New to NumPy Check out the Absolute Beginners Guide",
          "url": "https://numpy.org/doc/stable/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "It contains an introduction to NumPys main concepts and links to additional tutorials To the absolute beginners guide User guide The user guide provides in-depth information on the key concepts of NumPy with useful background information and explanation To the user guide API reference The reference guide contains a detailed description of the functions, modules, and objects included in NumPy The reference describes how the methods work and which parameters can be used",
          "url": "https://numpy.org/doc/stable/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "It assumes that you have an understanding of the key concepts To the reference guide Contributors guide Want to add to the codebase Can help add translation or a flowchart to the documentation The contributing guidelines will guide you through the process of improving NumPy To the contributors guide next NumPy user guide",
          "url": "https://numpy.org/doc/stable/index.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/features.html#main-content",
      "title": "NumPy Features \u2014 NumPy Tutorials",
      "content": "Repository Open issue .md .pdf NumPy Features NumPy Features A collection of notebooks pertaining to built-in NumPy functionality. Linear algebra on n-dimensional arrays Saving and sharing your NumPy arrays Masked Arrays previous NumPy tutorials next Linear algebra on n-dimensional arrays By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [],
      "chunks": [
        {
          "content": "Repository Open issue md pdf NumPy Features NumPy Features A collection of notebooks pertaining to built-in NumPy functionality Linear algebra on n-dimensional arrays Saving and sharing your NumPy arrays Masked Arrays previous NumPy tutorials next Linear algebra on n-dimensional arrays By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/features.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/index.html",
      "title": "NumPy tutorials \u2014 NumPy Tutorials",
      "content": "Repository Open issue .md .pdf NumPy tutorials Contents Content Non-executable articles Useful links and resources NumPy tutorials This set of tutorials and educational materials is being developed in the numpy-tutorials repository, and is not a part of the NumPy source tree. The goal of this repository is to provide high-quality resources by the NumPy project, both for self-learning and for teaching classes with. If youre interested in adding your own content, check the Contributing section. To open a live version of the content, click the launch Binder button above. To open each of the .md files, right click and select Open with - Notebook. You can also launch individual tutorials on Binder by clicking on the rocket icon that appears in the upper-right corner of each tutorial. To download a local copy of the .ipynb files, you can either clone this repository or use the download icon in the upper-right corner of each tutorial. Content NumPy Features Linear algebra on n-dimensional arrays Saving and sharing your NumPy arrays Masked Arrays NumPy Applications Determining Moores Law with real data in NumPy Deep learning on MNIST X-ray image processing Determining Static Equilibrium in NumPy Plotting Fractals Analyzing the impact of the lockdown on air quality in Delhi, India Contributing Why Jupyter Notebooks? Adding your own tutorials Non-executable articles Help improve the tutorials! Want to make a valuable contribution to the tutorials? Consider contributing to these existing articles to help make them fully executable and reproducible! Articles Deep reinforcement learning with Pong from pixels Sentiment Analysis on notable speeches of the last decade Useful links and resources The following links may be useful: NumPy Code of Conduct Main NumPy documentation NumPy documentation team meeting notes NEP 44 - Restructuring the NumPy documentation Blog post - Documentation as a way to build Community Note that regular documentation issues for NumPy can be found in the main NumPy repository (see the Documentation labels there). next NumPy Features Contents Content Non-executable articles Useful links and resources By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "Documentation"
      ],
      "chunks": [
        {
          "content": "Repository Open issue md pdf NumPy tutorials Contents Content Non-executable articles Useful links and resources NumPy tutorials This set of tutorials and educational materials is being developed in the numpy-tutorials repository, and is not a part of the NumPy source tree The goal of this repository is to provide high-quality resources by the NumPy project, both for self-learning and for teaching classes with If youre interested in adding your own content, check the Contributing section",
          "url": "https://numpy.org/numpy-tutorials/index.html",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "To open a live version of the content, click the launch Binder button above To open each of the md files, right click and select Open with - Notebook You can also launch individual tutorials on Binder by clicking on the rocket icon that appears in the upper-right corner of each tutorial To download a local copy of the ipynb files, you can either clone this repository or use the download icon in the upper-right corner of each tutorial",
          "url": "https://numpy.org/numpy-tutorials/index.html",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "Content NumPy Features Linear algebra on n-dimensional arrays Saving and sharing your NumPy arrays Masked Arrays NumPy Applications Determining Moores Law with real data in NumPy Deep learning on MNIST X-ray image processing Determining Static Equilibrium in NumPy Plotting Fractals Analyzing the impact of the lockdown on air quality in Delhi, India Contributing Why Jupyter Notebooks Adding your own tutorials Non-executable articles Help improve the tutorials",
          "url": "https://numpy.org/numpy-tutorials/index.html",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "Want to make a valuable contribution to the tutorials Consider contributing to these existing articles to help make them fully executable and reproducible",
          "url": "https://numpy.org/numpy-tutorials/index.html",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "Articles Deep reinforcement learning with Pong from pixels Sentiment Analysis on notable speeches of the last decade Useful links and resources The following links may be useful: NumPy Code of Conduct Main NumPy documentation NumPy documentation team meeting notes NEP 44 - Restructuring the NumPy documentation Blog post - Documentation as a way to build Community Note that regular documentation issues for NumPy can be found in the main NumPy repository (see the Documentation labels there)",
          "url": "https://numpy.org/numpy-tutorials/index.html",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "next NumPy Features Contents Content Non-executable articles Useful links and resources By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/index.html",
          "library": "numpy",
          "chunk_id": "numpy_5"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
      "title": "Linear algebra on n-dimensional arrays \u2014 NumPy Tutorials",
      "content": "Binder Repository Open issue .ipynb .md .pdf Linear algebra on n-dimensional arrays Contents Prerequisites Learner profile Learning Objectives Content Shape, axis and array properties Operations on an axis Approximation Applying to all colors Products with n-dimensional arrays Final words Further reading Linear algebra on n-dimensional arrays Prerequisites Before reading this tutorial, you should know a bit of Python. If you would like to refresh your memory, take a look at the Python tutorial. If you want to be able to run the examples in this tutorial, you should also have matplotlib and SciPy installed on your computer. Learner profile This tutorial is for people who have a basic understanding of linear algebra and arrays in NumPy and want to understand how n-dimensional (\\(n=2\\)) arrays are represented and can be manipulated. In particular, if you dont know how to apply common functions to n-dimensional arrays (without using for-loops), or if you want to understand axis and shape properties for n-dimensional arrays, this tutorial might be of help. Learning Objectives After this tutorial, you should be able to: Understand the difference between one-, two- and n-dimensional arrays in NumPy; Understand how to apply some linear algebra operations to n-dimensional arrays without using for-loops; Understand axis and shape properties for n-dimensional arrays. Content In this tutorial, we will use a matrix decomposition from linear algebra, the Singular Value Decomposition, to generate a compressed approximation of an image. Well use the face image from the scipy.datasets module:  TODO: Rm try-except with scipy 1.10 is the minimum supported version try: from scipy.datasets import face except ImportError:  Data was in scipy.misc prior to scipy v1.10 from scipy.misc import face img = face() Downloading file 'face.dat' from 'https://raw.githubusercontent.com/scipy/dataset-face/main/face.dat' to '/home/circleci/.cache/scipy-data'. Note: If you prefer, you can use your own image as you work through this tutorial. In order to transform your image into a NumPy array that can be manipulated, you can use the imread function from the matplotlib.pyplot submodule. Alternatively, you can use the imageio.imread function from the imageio library. Be aware that if you use your own image, youll likely need to adapt the steps below. For more information on how images are treated when converted to NumPy arrays, see A crash course on NumPy for images from the scikit-image documentation. Now, img is a NumPy array, as we can see when using the type function: type(img) numpy.ndarray We can see the image using the matplotlib.pyplot.imshow function  the special iPython command, matplotlib inline to display plots inline: import matplotlib.pyplot as plt matplotlib inline plt.imshow(img) plt.show() Shape, axis and array properties Note that, in linear algebra, the dimension of a vector refers to the number of entries in an array. In NumPy, it instead defines the number of axes. For example, a 1D array is a vector such as [1, 2, 3], a 2D array is a matrix, and so forth. First, lets check for the shape of the data in our array. Since this image is two-dimensional (the pixels in the image form a rectangle), we might expect a two-dimensional array to represent it (a matrix). However, using the shape property of this NumPy array gives us a different result: img.shape (768, 1024, 3) The output is a tuple with three elements, which means that this is a three-dimensional array. In fact, since this is a color image, and we have used the imread function to read it, the data is organized in three 2D arrays, representing color channels (in this case, red, green and blue - RGB). You can see this by looking at the shape above: it indicates that we have an array of 3 matrices, each having shape 768x1024. Furthermore, using the ndim property of this array, we can see that img.ndim 3 NumPy refers to each dimension as an axis. Because of how imread works, the first index in the 3rd axis is the red pixel data for our image. We can access this by using the syntax img[:, :, 0] array([[121, 138, 153, ..., 119, 131, 139], [ 89, 110, 130, ..., 118, 134, 146], [ 73, 94, 115, ..., 117, 133, 144], ..., [ 87, 94, 107, ..., 120, 119, 119], [ 85, 95, 112, ..., 121, 120, 120], [ 85, 97, 111, ..., 120, 119, 118]], shape=(768, 1024), dtype=uint8) From the output above, we can see that every value in img[:, :, 0] is an integer value between 0 and 255, representing the level of red in each corresponding image pixel (keep in mind that this might be different if you use your own image instead of scipy.datasets.face). As expected, this is a 768x1024 matrix: img[:, :, 0].shape (768, 1024) Since we are going to perform linear algebra operations on this data, it might be more interesting to have real numbers between 0 and 1 in each entry of the matrices to represent the RGB values. We can do that by setting img_array = img / 255 This operation, dividing an array by a scalar, works because of NumPys broadcasting rules. (Note that in real-world applications, it would be better to use, for example, the img_as_float utility function from scikit-image). You can check that the above works by doing some tests; for example, inquiring about maximum and minimum values for this array: img_array.max(), img_array.min() (np.float64(1.0), np.float64(0.0)) or checking the type of data in the array: img_array.dtype dtype('float64') Note that we can assign each color channel to a separate matrix using the slice syntax: red_array = img_array[:, :, 0] green_array = img_array[:, :, 1] blue_array = img_array[:, :, 2] Operations on an axis It is possible to use methods from linear algebra to approximate an existing set of data. Here, we will use the SVD (Singular Value Decomposition) to try to rebuild an image that uses less singular value information than the original one, while still retaining some of its features. Note: We will use NumPys linear algebra module, numpy.linalg, to perform the operations in this tutorial. Most of the linear algebra functions in this module can also be found in scipy.linalg, and users are encouraged to use the scipy module for real-world applications. However, some functions in the scipy.linalg module, such as the SVD function, only support 2D arrays. For more information on this, check the scipy.linalg page. To proceed, import the linear algebra submodule from NumPy: from numpy import linalg In order to extract information from a given matrix, we can use the SVD to obtain 3 arrays which can be multiplied to obtain the original matrix. From the theory of linear algebra, given a matrix \\(A\\), the following product can be computed: \\[U \\Sigma VT = A\\] where \\(U\\) and \\(VT\\) are square and \\(\\Sigma\\) is the same size as \\(A\\). \\(\\Sigma\\) is a diagonal matrix and contains the singular values of \\(A\\), organized from largest to smallest. These values are always non-negative and can be used as an indicator of the importance of some features represented by the matrix \\(A\\). Lets see how this works in practice with just one matrix first. Note that according to colorimetry, it is possible to obtain a fairly reasonable grayscale version of our color image if we apply the formula \\[Y = 0.2126 R + 0.7152 G + 0.0722 B\\] where \\(Y\\) is the array representing the grayscale image, and \\(R\\), \\(G\\) and \\(B\\) are the red, green and blue channel arrays we had originally. Notice we can use the  operator (the matrix multiplication operator for NumPy arrays, see numpy.matmul) for this: img_gray = img_array  [0.2126, 0.7152, 0.0722] Now, img_gray has shape img_gray.shape (768, 1024) To see if this makes sense in our image, we should use a colormap from matplotlib corresponding to the color we wish to see in out image (otherwise, matplotlib will default to a colormap that does not correspond to the real data). In our case, we are approximating the grayscale portion of the image, so we will use the colormap gray: plt.imshow(img_gray, cmap=\"gray\") plt.show() Now, applying the linalg.svd function to this matrix, we obtain the following decomposition: U, s, Vt = linalg.svd(img_gray) Note If you are using your own image, this command might take a while to run, depending on the size of your image and your hardware. Dont worry, this is normal! The SVD can be a pretty intensive computation. Lets check that this is what we expected: U.shape, s.shape, Vt.shape ((768, 768), (768,), (1024, 1024)) Note that s has a particular shape: it has only one dimension. This means that some linear algebra functions that expect 2d arrays might not work. For example, from the theory, one might expect s and Vt to be compatible for multiplication. However, this is not true as s does not have a second axis. Executing s  Vt results in a ValueError. This happens because having a one-dimensional array for s, in this case, is much more economic in practice than building a diagonal matrix with the same data. To reconstruct the original matrix, we can rebuild the diagonal matrix \\(\\Sigma\\) with the elements of s in its diagonal and with the appropriate dimensions for multiplying: in our case, \\(\\Sigma\\) should be 768x1024 since U is 768x768 and Vt is 1024x1024. In order to add the singular values to the diagonal of Sigma, we will use the fill_diagonal function from NumPy: import numpy as np Sigma = np.zeros((U.shape[1], Vt.shape[0])) np.fill_diagonal(Sigma, s) Now, we want to check if the reconstructed U  Sigma  Vt is close to the original img_gray matrix. Approximation The linalg module includes a norm function, which computes the norm of a vector or matrix represented in a NumPy array. For example, from the SVD explanation above, we would expect the norm of the difference between img_gray and the reconstructed SVD product to be small. As expected, you should see something like linalg.norm(img_gray - U  Sigma  Vt) np.float64(1.436745484142884e-12) (The actual result of this operation might be different depending on your architecture and linear algebra setup. Regardless, you should see a small number.) We could also have used the numpy.allclose function to make sure the reconstructed product is, in fact, close to our original matrix (the difference between the two arrays is small): np.allclose(img_gray, U  Sigma  Vt) True To see if an approximation is reasonable, we can check the values in s: plt.plot(s) plt.show() In the graph, we can see that although we have 768 singular values in s, most of those (after the 150th entry or so) are pretty small. So it might make sense to use only the information related to the first (say, 50) singular values to build a more economical approximation to our image. The idea is to consider all but the first k singular values in Sigma (which are the same as in s) as zeros, keeping U and Vt intact, and computing the product of these matrices as the approximation. For example, if we choose k = 10 we can build the approximation by doing approx = U  Sigma[:, :k]  Vt[:k, :] Note that we had to use only the first k rows of Vt, since all other rows would be multiplied by the zeros corresponding to the singular values we eliminated from this approximation. plt.imshow(approx, cmap=\"gray\") plt.show() Now, you can go ahead and repeat this experiment with other values of k, and each of your experiments should give you a slightly better (or worse) image depending on the value you choose. Applying to all colors Now we want to do the same kind of operation, but to all three colors. Our first instinct might be to repeat the same operation we did above to each color matrix individually. However, NumPys broadcasting takes care of this for us. If our array has more than two dimensions, then the SVD can be applied to all axes at once. However, the linear algebra functions in NumPy expect to see an array of the form (n, M, N), where the first axis n represents the number of MxN matrices in the stack. In our case, img_array.shape (768, 1024, 3) so we need to permutate the axis on this array to get a shape like (3, 768, 1024). Fortunately, the numpy.transpose function can do that for us: np.transpose(x, axes=(i, j, k)) indicates that the axis will be reordered such that the final shape of the transposed array will be reordered according to the indices (i, j, k). Lets see how this goes for our array: img_array_transposed = np.transpose(img_array, (2, 0, 1)) img_array_transposed.shape (3, 768, 1024) Now we are ready to apply the SVD: U, s, Vt = linalg.svd(img_array_transposed) Finally, to obtain the full approximated image, we need to reassemble these matrices into the approximation. Now, note that U.shape, s.shape, Vt.shape ((3, 768, 768), (3, 768), (3, 1024, 1024)) To build the final approximation matrix, we must understand how multiplication across different axes works. Products with n-dimensional arrays If you have worked before with only one- or two-dimensional arrays in NumPy, you might use numpy.dot and numpy.matmul (or the  operator) interchangeably. However, for n-dimensional arrays, they work in very different ways. For more details, check the documentation on numpy.matmul. Now, to build our approximation, we first need to make sure that our singular values are ready for multiplication, so we build our Sigma matrix similarly to what we did before. The Sigma array must have dimensions (3, 768, 1024). In order to add the singular values to the diagonal of Sigma, we will again use the fill_diagonal function, using each of the 3 rows in s as the diagonal for each of the 3 matrices in Sigma: Sigma = np.zeros((3, 768, 1024)) for j in range(3): np.fill_diagonal(Sigma[j, :, :], s[j, :]) Now, if we wish to rebuild the full SVD (with no approximation), we can do reconstructed = U  Sigma  Vt Note that reconstructed.shape (3, 768, 1024) The reconstructed image should be indistinguishable from the original one, except for differences due to floating point errors from the reconstruction. Recall that our original image consisted of floating point values in the range [0., 1.]. The accumulation of floating point error from the reconstruction can result in values slightly outside this original range: reconstructed.min(), reconstructed.max() (np.float64(-7.431555371084642e-15), np.float64(1.000000000000005)) Since imshow expects values in the range, we can use clip to excise the floating point error: reconstructed = np.clip(reconstructed, 0, 1) plt.imshow(np.transpose(reconstructed, (1, 2, 0))) plt.show() In fact, imshow peforms this clipping under-the-hood, so if you skip the first line in the previous code cell, you might see a warning message saying \"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\" Now, to do the approximation, we must choose only the first k singular values for each color channel. This can be done using the following syntax: approx_img = U  Sigma[..., :k]  Vt[..., :k, :] You can see that we have selected only the first k components of the last axis for Sigma (this means that we have used only the first k columns of each of the three matrices in the stack), and that we have selected only the first k components in the second-to-last axis of Vt (this means we have selected only the first k rows from every matrix in the stack Vt and all columns). If you are unfamiliar with the ellipsis syntax, it is a placeholder for other axes. For more details, see the documentation on Indexing. Now, approx_img.shape (3, 768, 1024) which is not the right shape for showing the image. Finally, reordering the axes back to our original shape of (768, 1024, 3), we can see our approximation: plt.imshow(np.transpose(approx_img, (1, 2, 0))) plt.show() Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.13567131472460806..1.079453607915587]. Even though the image is not as sharp, using a small number of k singular values (compared to the original set of 768 values), we can recover many of the distinguishing features from this image. Final words Of course, this is not the best method to approximate an image. However, there is, in fact, a result in linear algebra that says that the approximation we built above is the best we can get to the original matrix in terms of the norm of the difference. For more information, see G. H. Golub and C. F. Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985. Further reading Python tutorial NumPy Reference SciPy Tutorial SciPy Lecture Notes A matlab, R, IDL, NumPy/SciPy dictionary previous NumPy Features next Saving and sharing your NumPy arrays Contents Prerequisites Learner profile Learning Objectives Content Shape, axis and array properties Operations on an axis Approximation Applying to all colors Products with n-dimensional arrays Final words Further reading By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "# TODO: Rm try-except with scipy 1.10 is the minimum supported version\ntry:\n    from scipy.datasets import face\nexcept ImportError:  # Data was in scipy.misc prior to scipy v1.10\n    from scipy.misc import face\n\nimg = face()",
        "Downloading file 'face.dat' from 'https://raw.githubusercontent.com/scipy/dataset-face/main/face.dat' to '/home/circleci/.cache/scipy-data'.",
        "scikit-image",
        "numpy.ndarray",
        "%matplotlib inline",
        "import matplotlib.pyplot as plt\n\n%matplotlib inline",
        "plt.imshow(img)\nplt.show()",
        "(768, 1024, 3)",
        "img[:, :, 0]",
        "array([[121, 138, 153, ..., 119, 131, 139],\n       [ 89, 110, 130, ..., 118, 134, 146],\n       [ 73,  94, 115, ..., 117, 133, 144],\n       ...,\n       [ 87,  94, 107, ..., 120, 119, 119],\n       [ 85,  95, 112, ..., 121, 120, 120],\n       [ 85,  97, 111, ..., 120, 119, 118]],\n      shape=(768, 1024), dtype=uint8)",
        "img[:, :, 0]",
        "img[:, :, 0].shape",
        "(768, 1024)",
        "img_array = img / 255",
        "scikit-image",
        "img_array.max(), img_array.min()",
        "(np.float64(1.0), np.float64(0.0))",
        "img_array.dtype",
        "dtype('float64')",
        "red_array = img_array[:, :, 0]\ngreen_array = img_array[:, :, 1]\nblue_array = img_array[:, :, 2]",
        "from numpy import linalg",
        "img_gray = img_array @ [0.2126, 0.7152, 0.0722]",
        "img_gray.shape",
        "(768, 1024)",
        "plt.imshow(img_gray, cmap=\"gray\")\nplt.show()",
        "U, s, Vt = linalg.svd(img_gray)",
        "U.shape, s.shape, Vt.shape",
        "((768, 768), (768,), (1024, 1024))",
        "import numpy as np\n\nSigma = np.zeros((U.shape[1], Vt.shape[0]))\nnp.fill_diagonal(Sigma, s)",
        "U @ Sigma @ Vt",
        "linalg.norm(img_gray - U @ Sigma @ Vt)",
        "np.float64(1.436745484142884e-12)",
        "np.allclose(img_gray, U @ Sigma @ Vt)",
        "plt.plot(s)\nplt.show()",
        "approx = U @ Sigma[:, :k] @ Vt[:k, :]",
        "plt.imshow(approx, cmap=\"gray\")\nplt.show()",
        "img_array.shape",
        "(768, 1024, 3)",
        "(3, 768, 1024)",
        "np.transpose(x, axes=(i, j, k))",
        "img_array_transposed = np.transpose(img_array, (2, 0, 1))\nimg_array_transposed.shape",
        "(3, 768, 1024)",
        "U, s, Vt = linalg.svd(img_array_transposed)",
        "U.shape, s.shape, Vt.shape",
        "((3, 768, 768), (3, 768), (3, 1024, 1024))",
        "(3, 768, 1024)",
        "Sigma = np.zeros((3, 768, 1024))\nfor j in range(3):\n    np.fill_diagonal(Sigma[j, :, :], s[j, :])",
        "reconstructed = U @ Sigma @ Vt",
        "reconstructed.shape",
        "(3, 768, 1024)",
        "reconstructed.min(), reconstructed.max()",
        "(np.float64(-7.431555371084642e-15), np.float64(1.000000000000005))",
        "reconstructed = np.clip(reconstructed, 0, 1)\nplt.imshow(np.transpose(reconstructed, (1, 2, 0)))\nplt.show()",
        "\"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\"",
        "approx_img = U @ Sigma[..., :k] @ Vt[..., :k, :]",
        "approx_img.shape",
        "(3, 768, 1024)",
        "(768, 1024, 3)",
        "plt.imshow(np.transpose(approx_img, (1, 2, 0)))\nplt.show()",
        "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.13567131472460806..1.079453607915587]."
      ],
      "chunks": [
        {
          "content": "Binder Repository Open issue ipynb md pdf Linear algebra on n-dimensional arrays Contents Prerequisites Learner profile Learning Objectives Content Shape, axis and array properties Operations on an axis Approximation Applying to all colors Products with n-dimensional arrays Final words Further reading Linear algebra on n-dimensional arrays Prerequisites Before reading this tutorial, you should know a bit of Python If you would like to refresh your memory, take a look at the Python tutorial",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "If you want to be able to run the examples in this tutorial, you should also have matplotlib and SciPy installed on your computer Learner profile This tutorial is for people who have a basic understanding of linear algebra and arrays in NumPy and want to understand how n-dimensional (\\(n=2\\)) arrays are represented and can be manipulated",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "In particular, if you dont know how to apply common functions to n-dimensional arrays (without using for-loops), or if you want to understand axis and shape properties for n-dimensional arrays, this tutorial might be of help",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "Learning Objectives After this tutorial, you should be able to: Understand the difference between one-, two- and n-dimensional arrays in NumPy; Understand how to apply some linear algebra operations to n-dimensional arrays without using for-loops; Understand axis and shape properties for n-dimensional arrays Content In this tutorial, we will use a matrix decomposition from linear algebra, the Singular Value Decomposition, to generate a compressed approximation of an image",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "Well use the face image from the scipy datasets module:  TODO: Rm try-except with scipy 1 10 is the minimum supported version try: from scipy datasets import face except ImportError:  Data was in scipy misc prior to scipy v1 10 from scipy misc import face img = face() Downloading file 'face dat' from 'https://raw githubusercontent com/scipy/dataset-face/main/face dat' to '/home/circleci/ cache/scipy-data' Note: If you prefer, you can use your own image as you work through this tutorial",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "In order to transform your image into a NumPy array that can be manipulated, you can use the imread function from the matplotlib pyplot submodule Alternatively, you can use the imageio imread function from the imageio library Be aware that if you use your own image, youll likely need to adapt the steps below For more information on how images are treated when converted to NumPy arrays, see A crash course on NumPy for images from the scikit-image documentation",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "Now, img is a NumPy array, as we can see when using the type function: type(img) numpy ndarray We can see the image using the matplotlib pyplot imshow function  the special iPython command, matplotlib inline to display plots inline: import matplotlib pyplot as plt matplotlib inline plt imshow(img) plt show() Shape, axis and array properties Note that, in linear algebra, the dimension of a vector refers to the number of entries in an array In NumPy, it instead defines the number of axes",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "For example, a 1D array is a vector such as [1, 2, 3], a 2D array is a matrix, and so forth First, lets check for the shape of the data in our array Since this image is two-dimensional (the pixels in the image form a rectangle), we might expect a two-dimensional array to represent it (a matrix) However, using the shape property of this NumPy array gives us a different result: img shape (768, 1024, 3) The output is a tuple with three elements, which means that this is a three-dimensional array",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "In fact, since this is a color image, and we have used the imread function to read it, the data is organized in three 2D arrays, representing color channels (in this case, red, green and blue - RGB) You can see this by looking at the shape above: it indicates that we have an array of 3 matrices, each having shape 768x1024 Furthermore, using the ndim property of this array, we can see that img ndim 3 NumPy refers to each dimension as an axis",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "Because of how imread works, the first index in the 3rd axis is the red pixel data for our image We can access this by using the syntax img[:, :, 0] array([[121, 138, 153, , 119, 131, 139], [ 89, 110, 130, , 118, 134, 146], [ 73, 94, 115, , 117, 133, 144], , [ 87, 94, 107, , 120, 119, 119], [ 85, 95, 112, , 121, 120, 120], [ 85, 97, 111,",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": ", 120, 119, 118]], shape=(768, 1024), dtype=uint8) From the output above, we can see that every value in img[:, :, 0] is an integer value between 0 and 255, representing the level of red in each corresponding image pixel (keep in mind that this might be different if you use your own image instead of scipy datasets face) As expected, this is a 768x1024 matrix: img[:, :, 0]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "shape (768, 1024) Since we are going to perform linear algebra operations on this data, it might be more interesting to have real numbers between 0 and 1 in each entry of the matrices to represent the RGB values We can do that by setting img_array = img / 255 This operation, dividing an array by a scalar, works because of NumPys broadcasting rules (Note that in real-world applications, it would be better to use, for example, the img_as_float utility function from scikit-image)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "You can check that the above works by doing some tests; for example, inquiring about maximum and minimum values for this array: img_array max(), img_array min() (np float64(1 0), np float64(0 0)) or checking the type of data in the array: img_array",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "dtype dtype('float64') Note that we can assign each color channel to a separate matrix using the slice syntax: red_array = img_array[:, :, 0] green_array = img_array[:, :, 1] blue_array = img_array[:, :, 2] Operations on an axis It is possible to use methods from linear algebra to approximate an existing set of data",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "Here, we will use the SVD (Singular Value Decomposition) to try to rebuild an image that uses less singular value information than the original one, while still retaining some of its features Note: We will use NumPys linear algebra module, numpy linalg, to perform the operations in this tutorial Most of the linear algebra functions in this module can also be found in scipy linalg, and users are encouraged to use the scipy module for real-world applications However, some functions in the scipy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "linalg module, such as the SVD function, only support 2D arrays For more information on this, check the scipy linalg page To proceed, import the linear algebra submodule from NumPy: from numpy import linalg In order to extract information from a given matrix, we can use the SVD to obtain 3 arrays which can be multiplied to obtain the original matrix",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "From the theory of linear algebra, given a matrix \\(A\\), the following product can be computed: \\[U \\Sigma VT = A\\] where \\(U\\) and \\(VT\\) are square and \\(\\Sigma\\) is the same size as \\(A\\) \\(\\Sigma\\) is a diagonal matrix and contains the singular values of \\(A\\), organized from largest to smallest These values are always non-negative and can be used as an indicator of the importance of some features represented by the matrix \\(A\\) Lets see how this works in practice with just one matrix first",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Note that according to colorimetry, it is possible to obtain a fairly reasonable grayscale version of our color image if we apply the formula \\[Y = 0 2126 R + 0 7152 G + 0 0722 B\\] where \\(Y\\) is the array representing the grayscale image, and \\(R\\), \\(G\\) and \\(B\\) are the red, green and blue channel arrays we had originally Notice we can use the  operator (the matrix multiplication operator for NumPy arrays, see numpy matmul) for this: img_gray = img_array  [0 2126, 0 7152, 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "0722] Now, img_gray has shape img_gray shape (768, 1024) To see if this makes sense in our image, we should use a colormap from matplotlib corresponding to the color we wish to see in out image (otherwise, matplotlib will default to a colormap that does not correspond to the real data) In our case, we are approximating the grayscale portion of the image, so we will use the colormap gray: plt imshow(img_gray, cmap=\"gray\") plt show() Now, applying the linalg",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "svd function to this matrix, we obtain the following decomposition: U, s, Vt = linalg svd(img_gray) Note If you are using your own image, this command might take a while to run, depending on the size of your image and your hardware Dont worry, this is normal The SVD can be a pretty intensive computation Lets check that this is what we expected: U shape, s shape, Vt shape ((768, 768), (768,), (1024, 1024)) Note that s has a particular shape: it has only one dimension",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "This means that some linear algebra functions that expect 2d arrays might not work For example, from the theory, one might expect s and Vt to be compatible for multiplication However, this is not true as s does not have a second axis Executing s  Vt results in a ValueError This happens because having a one-dimensional array for s, in this case, is much more economic in practice than building a diagonal matrix with the same data",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "To reconstruct the original matrix, we can rebuild the diagonal matrix \\(\\Sigma\\) with the elements of s in its diagonal and with the appropriate dimensions for multiplying: in our case, \\(\\Sigma\\) should be 768x1024 since U is 768x768 and Vt is 1024x1024 In order to add the singular values to the diagonal of Sigma, we will use the fill_diagonal function from NumPy: import numpy as np Sigma = np zeros((U shape[1], Vt shape[0])) np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "fill_diagonal(Sigma, s) Now, we want to check if the reconstructed U  Sigma  Vt is close to the original img_gray matrix Approximation The linalg module includes a norm function, which computes the norm of a vector or matrix represented in a NumPy array For example, from the SVD explanation above, we would expect the norm of the difference between img_gray and the reconstructed SVD product to be small As expected, you should see something like linalg norm(img_gray - U  Sigma  Vt) np float64(1",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "436745484142884e-12) (The actual result of this operation might be different depending on your architecture and linear algebra setup Regardless, you should see a small number ) We could also have used the numpy allclose function to make sure the reconstructed product is, in fact, close to our original matrix (the difference between the two arrays is small): np allclose(img_gray, U  Sigma  Vt) True To see if an approximation is reasonable, we can check the values in s: plt plot(s) plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "show() In the graph, we can see that although we have 768 singular values in s, most of those (after the 150th entry or so) are pretty small So it might make sense to use only the information related to the first (say, 50) singular values to build a more economical approximation to our image The idea is to consider all but the first k singular values in Sigma (which are the same as in s) as zeros, keeping U and Vt intact, and computing the product of these matrices as the approximation",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": "For example, if we choose k = 10 we can build the approximation by doing approx = U  Sigma[:, :k]  Vt[:k, :] Note that we had to use only the first k rows of Vt, since all other rows would be multiplied by the zeros corresponding to the singular values we eliminated from this approximation plt imshow(approx, cmap=\"gray\") plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "show() Now, you can go ahead and repeat this experiment with other values of k, and each of your experiments should give you a slightly better (or worse) image depending on the value you choose Applying to all colors Now we want to do the same kind of operation, but to all three colors Our first instinct might be to repeat the same operation we did above to each color matrix individually However, NumPys broadcasting takes care of this for us",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "If our array has more than two dimensions, then the SVD can be applied to all axes at once However, the linear algebra functions in NumPy expect to see an array of the form (n, M, N), where the first axis n represents the number of MxN matrices in the stack In our case, img_array shape (768, 1024, 3) so we need to permutate the axis on this array to get a shape like (3, 768, 1024) Fortunately, the numpy transpose function can do that for us: np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "transpose(x, axes=(i, j, k)) indicates that the axis will be reordered such that the final shape of the transposed array will be reordered according to the indices (i, j, k) Lets see how this goes for our array: img_array_transposed = np transpose(img_array, (2, 0, 1)) img_array_transposed shape (3, 768, 1024) Now we are ready to apply the SVD: U, s, Vt = linalg svd(img_array_transposed) Finally, to obtain the full approximated image, we need to reassemble these matrices into the approximation",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "Now, note that U shape, s shape, Vt shape ((3, 768, 768), (3, 768), (3, 1024, 1024)) To build the final approximation matrix, we must understand how multiplication across different axes works Products with n-dimensional arrays If you have worked before with only one- or two-dimensional arrays in NumPy, you might use numpy dot and numpy matmul (or the  operator) interchangeably However, for n-dimensional arrays, they work in very different ways For more details, check the documentation on numpy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "matmul Now, to build our approximation, we first need to make sure that our singular values are ready for multiplication, so we build our Sigma matrix similarly to what we did before The Sigma array must have dimensions (3, 768, 1024) In order to add the singular values to the diagonal of Sigma, we will again use the fill_diagonal function, using each of the 3 rows in s as the diagonal for each of the 3 matrices in Sigma: Sigma = np zeros((3, 768, 1024)) for j in range(3): np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "fill_diagonal(Sigma[j, :, :], s[j, :]) Now, if we wish to rebuild the full SVD (with no approximation), we can do reconstructed = U  Sigma  Vt Note that reconstructed shape (3, 768, 1024) The reconstructed image should be indistinguishable from the original one, except for differences due to floating point errors from the reconstruction Recall that our original image consisted of floating point values in the range [0 , 1 ]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "The accumulation of floating point error from the reconstruction can result in values slightly outside this original range: reconstructed min(), reconstructed max() (np float64(-7 431555371084642e-15), np float64(1 000000000000005)) Since imshow expects values in the range, we can use clip to excise the floating point error: reconstructed = np clip(reconstructed, 0, 1) plt imshow(np transpose(reconstructed, (1, 2, 0))) plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": "show() In fact, imshow peforms this clipping under-the-hood, so if you skip the first line in the previous code cell, you might see a warning message saying \"Clipping input data to the valid range for imshow with RGB data ([0 1] for floats or [0 255] for integers) \" Now, to do the approximation, we must choose only the first k singular values for each color channel This can be done using the following syntax: approx_img = U  Sigma[ , :k]  Vt[",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": ", :k, :] You can see that we have selected only the first k components of the last axis for Sigma (this means that we have used only the first k columns of each of the three matrices in the stack), and that we have selected only the first k components in the second-to-last axis of Vt (this means we have selected only the first k rows from every matrix in the stack Vt and all columns) If you are unfamiliar with the ellipsis syntax, it is a placeholder for other axes",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "For more details, see the documentation on Indexing Now, approx_img shape (3, 768, 1024) which is not the right shape for showing the image Finally, reordering the axes back to our original shape of (768, 1024, 3), we can see our approximation: plt imshow(np transpose(approx_img, (1, 2, 0))) plt show() Clipping input data to the valid range for imshow with RGB data ([0 1] for floats or [0 255] for integers) Got range [-0 13567131472460806 1 079453607915587]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "Even though the image is not as sharp, using a small number of k singular values (compared to the original set of 768 values), we can recover many of the distinguishing features from this image Final words Of course, this is not the best method to approximate an image However, there is, in fact, a result in linear algebra that says that the approximation we built above is the best we can get to the original matrix in terms of the norm of the difference For more information, see G H Golub and C F",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_37"
        },
        {
          "content": "Further reading Python tutorial NumPy Reference SciPy Tutorial SciPy Lecture Notes A matlab, R, IDL, NumPy/SciPy dictionary previous NumPy Features next Saving and sharing your NumPy arrays Contents Prerequisites Learner profile Learning Objectives Content Shape, axis and array properties Operations on an axis Approximation Applying to all colors Products with n-dimensional arrays Final words Further reading By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-svd.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_38"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
      "title": "Saving and sharing your NumPy arrays \u2014 NumPy Tutorials",
      "content": "Binder Repository Open issue .ipynb .md .pdf Saving and sharing your NumPy arrays Contents What youll learn What youll do What youll need Create your arrays Save your arrays with NumPys savez Remove the saved arrays and load them back with NumPys load Reassign the NpzFile arrays to x and y Success Another option: saving to human-readable csv Rearrange the data into a single 2D array Save the data to csv file using savetxt Our arrays as a csv file Success, but remember your types Wrapping up Saving and sharing your NumPy arrays What youll learn Youll save your NumPy arrays as zipped files and human-readable comma-delimited files i.e. *.csv. You will also learn to load both of these file types back into NumPy workspaces. What youll do Youll learn two ways of saving and reading filesas compressed and as text filesthat will serve most of your storage needs in NumPy. Youll create two 1D arrays and one 2D array Youll save these arrays to files Youll remove variables from your workspace Youll load the variables from your saved file Youll compare zipped binary files to human-readable delimited files Youll finish with the skills of saving, loading, and sharing NumPy arrays What youll need NumPy read-write access to your working directory Load the necessary functions using the following command. import numpy as np In this tutorial, you will use the following Python, IPython magic, and NumPy functions: np.arange np.savez del whos np.load np.block np.newaxis np.savetxt np.loadtxt Create your arrays Now that you have imported the NumPy library, you can make a couple of arrays; lets start with two 1D arrays, x and y, where y = x**2.You will assign x to the integers from 0 to 9 using np.arange. x = np.arange(10) y = x ** 2 print(x) print(y) [0 1 2 3 4 5 6 7 8 9] [ 0 1 4 9 16 25 36 49 64 81] Save your arrays with NumPys savez Now you have two arrays in your workspace, x: [0 1 2 3 4 5 6 7 8 9] y: [ 0 1 4 9 16 25 36 49 64 81] The first thing you will do is save them to a file as zipped arrays using savez. You will use two options to label the arrays in the file, x_axis = x: this option is assigning the name x_axis to the variable x y_axis = y: this option is assigning the name y_axis to the variable y np.savez(\"x_y-squared.npz\", x_axis=x, y_axis=y) Remove the saved arrays and load them back with NumPys load In your current working directory, you should have a new file with the name x_y-squared.npz. This file is a zipped binary of the two arrays, x and y. Lets clear the workspace and load the values back in. This x_y-squared.npz file contains two NPY format files. The NPY format is a native binary format. You cannot read the numbers in a standard text editor or spreadsheet. remove x and y from the workspaec with del load the arrays into the workspace in a dictionary with np.load To see what variables are in the workspace, use the Jupyter/IPython magic command whos. del x, y whos Variable Type Data/Info ------------------------------ np module module 'numpy' from '/ho...kages/numpy/__init__.py' load_xy = np.load(\"x_y-squared.npz\") print(load_xy.files) ['x_axis', 'y_axis'] whos Variable Type Data/Info ------------------------------- load_xy NpzFile NpzFile 'x_y-squared.npz'...with keys: x_axis, y_axis np module module 'numpy' from '/ho...kages/numpy/__init__.py' Reassign the NpzFile arrays to x and y Youve now created the dictionary with an NpzFile-type. The included files are x_axis and y_axis that you defined in your savez command. You can reassign x and y to the load_xy files. x = load_xy[\"x_axis\"] y = load_xy[\"y_axis\"] print(x) print(y) [0 1 2 3 4 5 6 7 8 9] [ 0 1 4 9 16 25 36 49 64 81] Success You have created, saved, deleted, and loaded the variables x and y using savez and load. Nice work. Another option: saving to human-readable csv Lets consider another scenario, you want to share x and y with other people or other programs. You may need human-readable text file that is easier to share. Next, you use the savetxt to save x and y in a comma separated value file, x_y-squared.csv. The resulting csv is composed of ASCII characters. You can load the file back into NumPy or read it with other programs. Rearrange the data into a single 2D array First, you have to create a single 2D array from your two 1D arrays. The csv-filetype is a spreadsheet-style dataset. The csv arranges numbers in rowsseparated by new linesand columnsseparated by commas. If the data is more complex e.g. multiple 2D arrays or higher dimensional arrays, it is better to use savez. Here, you use two NumPy functions to format the data: np.block: this function appends arrays together into a 2D array np.newaxis: this function forces the 1D array into a 2D column vector with 10 rows and 1 column. array_out = np.block([x[:, np.newaxis], y[:, np.newaxis]]) print(\"the output array has shape \", array_out.shape, \" with values:\") print(array_out) the output array has shape (10, 2) with values: [[ 0 0] [ 1 1] [ 2 4] [ 3 9] [ 4 16] [ 5 25] [ 6 36] [ 7 49] [ 8 64] [ 9 81]] Save the data to csv file using savetxt You use savetxt with a three options to make your file easier to read: X = array_out: this option tells savetxt to save your 2D array, array_out, to the file x_y-squared.csv header = 'x, y': this option writes a header before any data that labels the columns of the csv delimiter = ',': this option tells savetxt to place a comma between each column in the file np.savetxt(\"x_y-squared.csv\", X=array_out, header=\"x, y\", delimiter=\",\") Open the file, x_y-squared.csv, and youll see the following: !head x_y-squared.csv  x, y 0.000000000000000000e+00,0.000000000000000000e+00 1.000000000000000000e+00,1.000000000000000000e+00 2.000000000000000000e+00,4.000000000000000000e+00 3.000000000000000000e+00,9.000000000000000000e+00 4.000000000000000000e+00,1.600000000000000000e+01 5.000000000000000000e+00,2.500000000000000000e+01 6.000000000000000000e+00,3.600000000000000000e+01 7.000000000000000000e+00,4.900000000000000000e+01 8.000000000000000000e+00,6.400000000000000000e+01 Our arrays as a csv file There are two features that you shoud notice here: NumPy uses  to ignore headings when using loadtxt. If youre using loadtxt with other csv files, you can skip header rows with skiprows = number_of_header_lines. The integers were written in scientific notation. You can specify the format of the text using the savetxt option, fmt = , but it will still be written with ASCII characters. In general, you cannot preserve the type of ASCII numbers as float or int. Now, delete x and y again and assign them to your columns in x-y_squared.csv. del x, y load_xy = np.loadtxt(\"x_y-squared.csv\", delimiter=\",\") load_xy.shape (10, 2) x = load_xy[:, 0] y = load_xy[:, 1] print(x) print(y) [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.] [ 0. 1. 4. 9. 16. 25. 36. 49. 64. 81.] Success, but remember your types When you saved the arrays to the csv file, you did not preserve the int type. When loading the arrays back into your workspace the default process will be to load the csv file as a 2D floating point array e.g. load_xy.dtype == 'float64' and load_xy.shape == (10, 2). Wrapping up In conclusion, you can create, save, and load arrays in NumPy. Saving arrays makes sharing your work and collaboration much easier. There are other ways Python can save data to files, such as pickle, but savez and savetxt will serve most of your storage needs for future NumPy work and sharing with other people, respectively. Next steps: you can import data with missing values from Importing with genfromtext or learn more about general NumPy IO with Reading and Writing Files. previous Linear algebra on n-dimensional arrays next Masked Arrays Contents What youll learn What youll do What youll need Create your arrays Save your arrays with NumPys savez Remove the saved arrays and load them back with NumPys load Reassign the NpzFile arrays to x and y Success Another option: saving to human-readable csv Rearrange the data into a single 2D array Save the data to csv file using savetxt Our arrays as a csv file Success, but remember your types Wrapping up By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "import numpy as np",
        "x = np.arange(10)\ny = x ** 2\nprint(x)\nprint(y)",
        "[0 1 2 3 4 5 6 7 8 9]\n[ 0  1  4  9 16 25 36 49 64 81]",
        "x: [0 1 2 3 4 5 6 7 8 9]",
        "y: [ 0\u00a0 1\u00a0 4\u00a0 9 16 25 36 49 64 81]",
        "np.savez(\"x_y-squared.npz\", x_axis=x, y_axis=y)",
        "x_y-squared.npz",
        "x_y-squared.npz",
        "Variable   Type      Data/Info\n------------------------------\nnp         module    <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>",
        "load_xy = np.load(\"x_y-squared.npz\")\n\nprint(load_xy.files)",
        "['x_axis', 'y_axis']",
        "Variable   Type       Data/Info\n-------------------------------\nload_xy    NpzFile    NpzFile 'x_y-squared.npz'<...>with keys: x_axis, y_axis\nnp         module     <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>",
        "x = load_xy[\"x_axis\"]\ny = load_xy[\"y_axis\"]\nprint(x)\nprint(y)",
        "[0 1 2 3 4 5 6 7 8 9]\n[ 0  1  4  9 16 25 36 49 64 81]",
        "x_y-squared.csv",
        "array_out = np.block([x[:, np.newaxis], y[:, np.newaxis]])\nprint(\"the output array has shape \", array_out.shape, \" with values:\")\nprint(array_out)",
        "the output array has shape  (10, 2)  with values:\n[[ 0  0]\n [ 1  1]\n [ 2  4]\n [ 3  9]\n [ 4 16]\n [ 5 25]\n [ 6 36]\n [ 7 49]\n [ 8 64]\n [ 9 81]]",
        "X = array_out",
        "x_y-squared.csv",
        "header = 'x, y'",
        "delimiter = ','",
        "np.savetxt(\"x_y-squared.csv\", X=array_out, header=\"x, y\", delimiter=\",\")",
        "x_y-squared.csv",
        "!head x_y-squared.csv",
        "# x, y\n0.000000000000000000e+00,0.000000000000000000e+00\n1.000000000000000000e+00,1.000000000000000000e+00\n2.000000000000000000e+00,4.000000000000000000e+00\n3.000000000000000000e+00,9.000000000000000000e+00\n4.000000000000000000e+00,1.600000000000000000e+01\n5.000000000000000000e+00,2.500000000000000000e+01\n6.000000000000000000e+00,3.600000000000000000e+01\n7.000000000000000000e+00,4.900000000000000000e+01\n8.000000000000000000e+00,6.400000000000000000e+01",
        "skiprows = <number_of_header_lines>",
        "x-y_squared.csv",
        "load_xy = np.loadtxt(\"x_y-squared.csv\", delimiter=\",\")",
        "load_xy.shape",
        "x = load_xy[:, 0]\ny = load_xy[:, 1]\nprint(x)\nprint(y)",
        "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n[ 0.  1.  4.  9. 16. 25. 36. 49. 64. 81.]",
        "load_xy.dtype == 'float64'",
        "load_xy.shape == (10, 2)"
      ],
      "chunks": [
        {
          "content": "Binder Repository Open issue ipynb md",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "pdf Saving and sharing your NumPy arrays Contents What youll learn What youll do What youll need Create your arrays Save your arrays with NumPys savez Remove the saved arrays and load them back with NumPys load Reassign the NpzFile arrays to x and y Success Another option: saving to human-readable csv Rearrange the data into a single 2D array Save the data to csv file using savetxt Our arrays as a csv file Success, but remember your types Wrapping up Saving and sharing your NumPy arrays What youll learn Youll save your NumPy arrays as zipped files and human-readable comma-delimited files i",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "e * csv You will also learn to load both of these file types back into NumPy workspaces What youll do Youll learn two ways of saving and reading filesas compressed and as text filesthat will serve most of your storage needs in NumPy",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "Youll create two 1D arrays and one 2D array Youll save these arrays to files Youll remove variables from your workspace Youll load the variables from your saved file Youll compare zipped binary files to human-readable delimited files Youll finish with the skills of saving, loading, and sharing NumPy arrays What youll need NumPy read-write access to your working directory Load the necessary functions using the following command",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "import numpy as np In this tutorial, you will use the following Python, IPython magic, and NumPy functions: np arange np savez del whos np load np block np newaxis np savetxt np loadtxt Create your arrays Now that you have imported the NumPy library, you can make a couple of arrays; lets start with two 1D arrays, x and y, where y = x**2 You will assign x to the integers from 0 to 9 using np arange x = np",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "arange(10) y = x ** 2 print(x) print(y) [0 1 2 3 4 5 6 7 8 9] [ 0 1 4 9 16 25 36 49 64 81] Save your arrays with NumPys savez Now you have two arrays in your workspace, x: [0 1 2 3 4 5 6 7 8 9] y: [ 0 1 4 9 16 25 36 49 64 81] The first thing you will do is save them to a file as zipped arrays using savez",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "You will use two options to label the arrays in the file, x_axis = x: this option is assigning the name x_axis to the variable x y_axis = y: this option is assigning the name y_axis to the variable y np savez(\"x_y-squared npz\", x_axis=x, y_axis=y) Remove the saved arrays and load them back with NumPys load In your current working directory, you should have a new file with the name x_y-squared npz This file is a zipped binary of the two arrays, x and y",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "Lets clear the workspace and load the values back in This x_y-squared npz file contains two NPY format files The NPY format is a native binary format You cannot read the numbers in a standard text editor or spreadsheet remove x and y from the workspaec with del load the arrays into the workspace in a dictionary with np load To see what variables are in the workspace, use the Jupyter/IPython magic command whos",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "del x, y whos Variable Type Data/Info ------------------------------ np module module 'numpy' from '/ho kages/numpy/__init__ py' load_xy = np load(\"x_y-squared npz\") print(load_xy files) ['x_axis', 'y_axis'] whos Variable Type Data/Info ------------------------------- load_xy NpzFile NpzFile 'x_y-squared npz' with keys: x_axis, y_axis np module module 'numpy' from '/ho kages/numpy/__init__ py' Reassign the NpzFile arrays to x and y Youve now created the dictionary with an NpzFile-type",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "The included files are x_axis and y_axis that you defined in your savez command You can reassign x and y to the load_xy files x = load_xy[\"x_axis\"] y = load_xy[\"y_axis\"] print(x) print(y) [0 1 2 3 4 5 6 7 8 9] [ 0 1 4 9 16 25 36 49 64 81] Success You have created, saved, deleted, and loaded the variables x and y using savez and load Nice work Another option: saving to human-readable csv Lets consider another scenario, you want to share x and y with other people or other programs",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "You may need human-readable text file that is easier to share Next, you use the savetxt to save x and y in a comma separated value file, x_y-squared csv The resulting csv is composed of ASCII characters You can load the file back into NumPy or read it with other programs Rearrange the data into a single 2D array First, you have to create a single 2D array from your two 1D arrays The csv-filetype is a spreadsheet-style dataset",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "The csv arranges numbers in rowsseparated by new linesand columnsseparated by commas If the data is more complex e g multiple 2D arrays or higher dimensional arrays, it is better to use savez Here, you use two NumPy functions to format the data: np block: this function appends arrays together into a 2D array np newaxis: this function forces the 1D array into a 2D column vector with 10 rows and 1 column array_out = np block([x[:, np newaxis], y[:, np",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "newaxis]]) print(\"the output array has shape \", array_out shape, \" with values:\") print(array_out) the output array has shape (10, 2) with values: [[ 0 0] [ 1 1] [ 2 4] [ 3 9] [ 4 16] [ 5 25] [ 6 36] [ 7 49] [ 8 64] [ 9 81]] Save the data to csv file using savetxt You use savetxt with a three options to make your file easier to read: X = array_out: this option tells savetxt to save your 2D array, array_out, to the file x_y-squared",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "csv header = 'x, y': this option writes a header before any data that labels the columns of the csv delimiter = ',': this option tells savetxt to place a comma between each column in the file np savetxt(\"x_y-squared csv\", X=array_out, header=\"x, y\", delimiter=\",\") Open the file, x_y-squared csv, and youll see the following: head x_y-squared csv  x, y 0 000000000000000000e+00,0 000000000000000000e+00 1 000000000000000000e+00,1 000000000000000000e+00 2 000000000000000000e+00,4",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "000000000000000000e+00 3 000000000000000000e+00,9 000000000000000000e+00 4 000000000000000000e+00,1 600000000000000000e+01 5 000000000000000000e+00,2 500000000000000000e+01 6 000000000000000000e+00,3 600000000000000000e+01 7 000000000000000000e+00,4 900000000000000000e+01 8 000000000000000000e+00,6 400000000000000000e+01 Our arrays as a csv file There are two features that you shoud notice here: NumPy uses  to ignore headings when using loadtxt",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "If youre using loadtxt with other csv files, you can skip header rows with skiprows = number_of_header_lines The integers were written in scientific notation You can specify the format of the text using the savetxt option, fmt = , but it will still be written with ASCII characters In general, you cannot preserve the type of ASCII numbers as float or int Now, delete x and y again and assign them to your columns in x-y_squared csv del x, y load_xy = np loadtxt(\"x_y-squared",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "csv\", delimiter=\",\") load_xy shape (10, 2) x = load_xy[:, 0] y = load_xy[:, 1] print(x) print(y) [0 1 2 3 4 5 6 7 8 9 ] [ 0 1 4 9 16 25 36 49 64 81 ] Success, but remember your types When you saved the arrays to the csv file, you did not preserve the int type When loading the arrays back into your workspace the default process will be to load the csv file as a 2D floating point array e g load_xy dtype == 'float64' and load_xy shape == (10, 2)",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Wrapping up In conclusion, you can create, save, and load arrays in NumPy Saving arrays makes sharing your work and collaboration much easier There are other ways Python can save data to files, such as pickle, but savez and savetxt will serve most of your storage needs for future NumPy work and sharing with other people, respectively Next steps: you can import data with missing values from Importing with genfromtext or learn more about general NumPy IO with Reading and Writing Files",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "previous Linear algebra on n-dimensional arrays next Masked Arrays Contents What youll learn What youll do What youll need Create your arrays Save your arrays with NumPys savez Remove the saved arrays and load them back with NumPys load Reassign the NpzFile arrays to x and y Success Another option: saving to human-readable csv Rearrange the data into a single 2D array Save the data to csv file using savetxt Our arrays as a csv file Success, but remember your types Wrapping up By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/content/save-load-arrays.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_18"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
      "title": "Masked Arrays \u2014 NumPy Tutorials",
      "content": "Binder Repository Open issue .ipynb .md .pdf Masked Arrays Contents What youll do What youll learn What youll need What are masked arrays? When can they be useful? Using masked arrays to see COVID-19 data Exploring the data Missing data Fitting Data In practice Further reading Reference Masked Arrays What youll do Use the masked arrays module from NumPy to analyze COVID-19 data and deal with missing values. What youll learn Youll understand what are masked arrays and how they can be created Youll see how to access and modify data for masked arrays Youll be able to decide when the use of masked arrays is appropriate in some of your applications What youll need Basic familiarity with Python. If you would like to refresh your memory, take a look at the Python tutorial. Basic familiarity with NumPy To run the plots on your computer, you need matplotlib. What are masked arrays? Consider the following problem. You have a dataset with missing or invalid entries. If youre doing any kind of processing on this data, and want to skip or flag these unwanted entries without just deleting them, you may have to use conditionals or filter your data somehow. The numpy.ma module provides some of the same functionality of NumPy ndarrays with added structure to ensure invalid entries are not used in computation. From the Reference Guide: A masked array is the combination of a standard numpy.ndarray and a mask. A mask is either nomask, indicating that no value of the associated array is invalid, or an array of booleans that determines for each element of the associated array whether the value is valid or not. When an element of the mask is False, the corresponding element of the associated array is valid and is said to be unmasked. When an element of the mask is True, the corresponding element of the associated array is said to be masked (invalid). We can think of a MaskedArray as a combination of: Data, as a regular numpy.ndarray of any shape or datatype; A boolean mask with the same shape as the data; A fill_value, a value that may be used to replace the invalid entries in order to return a standard numpy.ndarray. When can they be useful? There are a few situations where masked arrays can be more useful than just eliminating the invalid entries of an array: When you want to preserve the values you masked for later processing, without copying the array; When you have to handle many arrays, each with their own mask. If the mask is part of the array, you avoid bugs and the code is possibly more compact; When you have different flags for missing or invalid values, and wish to preserve these flags without replacing them in the original dataset, but exclude them from computations; If you cant avoid or eliminate missing values, but dont want to deal with NaN (Not a Number) values in your operations. Masked arrays are also a good idea since the numpy.ma module also comes with a specific implementation of most NumPy universal functions (ufuncs), which means that you can still apply fast vectorized functions and operations on masked data. The output is then a masked array. Well see some examples of how this works in practice below. Using masked arrays to see COVID-19 data From Kaggle it is possible to download a dataset with initial data about the COVID-19 outbreak in the beginning of 2020. We are going to look at a small subset of this data, contained in the file who_covid_19_sit_rep_time_series.csv. (Note that this file has been replaced with a version without missing data sometime in late 2020.) import numpy as np import os  The os.getcwd() function returns the current folder; you can change  the filepath variable to point to the folder where you saved the .csv file filepath = os.getcwd() filename = os.path.join(filepath, \"who_covid_19_sit_rep_time_series.csv\") The data file contains data of different types and is organized as follows: The first row is a header line that (mostly) describes the data in each column that follow in the rows below, and beginning in the fourth column, the header is the date of the observation. The second through seventh row contain summary data that is of a different type than that which we are going to examine, so we will need to exclude that from the data with which we will work. The numerical data we wish to work with begins at column 4, row 8, and extends from there to the rightmost column and the lowermost row. Lets explore the data inside this file for the first 14 days of records. To gather data from the .csv file, we will use the numpy.genfromtxt function, making sure we select only the columns with actual numbers instead of the first four columns which contain location data. We also skip the first 6 rows of this file, since they contain other data we are not interested in. Separately, we will extract the information about dates and location for this data.  Note we are using skip_header and usecols to read only portions of the  data file into each variable.  Read just the dates for columns 4-18 from the first row dates = np.genfromtxt( filename, dtype=np.str_, delimiter=\",\", max_rows=1, usecols=range(4, 18), encoding=\"utf-8-sig\", )  Read the names of the geographic locations from the first two  columns, skipping the first six rows locations = np.genfromtxt( filename, dtype=np.str_, delimiter=\",\", skip_header=6, usecols=(0, 1), encoding=\"utf-8-sig\", )  Read the numeric data from just the first 14 days nbcases = np.genfromtxt( filename, dtype=np.int_, delimiter=\",\", skip_header=6, usecols=range(4, 18), encoding=\"utf-8-sig\", ) Included in the numpy.genfromtxt function call, we have selected the numpy.dtype for each subset of the data (either an integer - numpy.int_ - or a string of characters - numpy.str_). We have also used the encoding argument to select utf-8-sig as the encoding for the file (read more about encoding in the official Python documentation. You can read more about the numpy.genfromtxt function from the Reference Documentation or from the Basic IO tutorial. Exploring the data First of all, we can plot the whole set of data we have and see what it looks like. In order to get a readable plot, we select only a few of the dates to show in our x-axis ticks. Note also that in our plot command, we use nbcases.T (the transpose of the nbcases array) since this means we will plot each row of the file as a separate line. We choose to plot a dashed line (using the '--' line style). See the matplotlib documentation for more info on this. import matplotlib.pyplot as plt selected_dates = [0, 3, 11, 13] plt.plot(dates, nbcases.T, \"--\") plt.xticks(selected_dates, dates[selected_dates]) plt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\") Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020') The graph has a strange shape from January 24th to February 1st. It would be interesting to know where this data comes from. If we look at the locations array we extracted from the .csv file, we can see that we have two columns, where the first would contain regions and the second would contain the name of the country. However, only the first few rows contain data for the the first column (province names in China). Following that, we only have country names. So it would make sense to group all the data from China into a single row. For this, well select from the nbcases array only the rows for which the second entry of the locations array corresponds to China. Next, well use the numpy.sum function to sum all the selected rows (axis=0). Note also that row 35 corresponds to the total counts for the whole country for each date. Since we want to calculate the sum ourselves from the provinces data, we have to remove that row first from both locations and nbcases: totals_row = 35 locations = np.delete(locations, (totals_row), axis=0) nbcases = np.delete(nbcases, (totals_row), axis=0) china_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0) china_total array([ 247, 288, 556, 817, -22, -22, -15, -10, -9, -7, -4, 11820, 14410, 17237]) Somethings wrong with this data - we are not supposed to have negative values in a cumulative data set. Whats going on? Missing data Looking at the data, heres what we find: there is a period with missing data: nbcases array([[ 258, 270, 375, ..., 7153, 9074, 11177], [ 14, 17, 26, ..., 520, 604, 683], [ -1, 1, 1, ..., 422, 493, 566], ..., [ -1, -1, -1, ..., -1, -1, -1], [ -1, -1, -1, ..., -1, -1, -1], [ -1, -1, -1, ..., -1, -1, -1]], shape=(263, 14)) All the -1 values we are seeing come from numpy.genfromtxt attempting to read missing data from the original .csv file. Obviously, we dont want to compute missing data as -1 - we just want to skip this value so it doesnt interfere in our analysis. After importing the numpy.ma module, well create a new array, this time masking the invalid values: from numpy import ma nbcases_ma = ma.masked_values(nbcases, -1) If we look at the nbcases_ma masked array, this is what we have: nbcases_ma masked_array( data=[[258, 270, 375, ..., 7153, 9074, 11177], [14, 17, 26, ..., 520, 604, 683], [--, 1, 1, ..., 422, 493, 566], ..., [--, --, --, ..., --, --, --], [--, --, --, ..., --, --, --], [--, --, --, ..., --, --, --]], mask=[[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [ True, False, False, ..., False, False, False], ..., [ True, True, True, ..., True, True, True], [ True, True, True, ..., True, True, True], [ True, True, True, ..., True, True, True]], fill_value=-1) We can see that this is a different kind of array. As mentioned in the introduction, it has three attributes (data, mask and fill_value). Keep in mind that the mask attribute has a True value for elements corresponding to invalid data (represented by two dashes in the data attribute). Lets try and see what the data looks like excluding the first row (data from the Hubei province in China) so we can look at the missing data more closely: plt.plot(dates, nbcases_ma[1:].T, \"--\") plt.xticks(selected_dates, dates[selected_dates]) plt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\") Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020') Now that our data has been masked, lets try summing up all the cases in China: china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0) china_masked masked_array(data=[278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821, 14411, 17238], mask=[False, False, False, False, False, False, False, False, False, False, False, False, False, False], fill_value=999999) Note that china_masked is a masked array, so it has a different data structure than a regular NumPy array. Now, we can access its data directly by using the .data attribute: china_total = china_masked.data china_total array([ 278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821, 14411, 17238]) That is better: no more negative values. However, we can still see that for some days, the cumulative number of cases seems to go down (from 835 to 10, for example), which does not agree with the definition of cumulative data. If we look more closely at the data, we can see that in the period where there was missing data in mainland China, there was valid data for Hong Kong, Taiwan, Macau and Unspecified regions of China. Maybe we can remove those from the total sum of cases in China, to get a better understanding of the data. First, well identify the indices of locations in mainland China: china_mask = ( (locations[:, 1] == \"China\")  (locations[:, 0] != \"Hong Kong\")  (locations[:, 0] != \"Taiwan\")  (locations[:, 0] != \"Macau\")  (locations[:, 0] != \"Unspecified*\") ) Now, china_mask is an array of boolean values (True or False); we can check that the indices are what we wanted with the ma.nonzero method for masked arrays: china_mask.nonzero() (array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 31, 33]),) Now we can correctly sum entries for mainland China: china_total = nbcases_ma[china_mask].sum(axis=0) china_total masked_array(data=[278, 308, 440, 446, --, --, --, --, --, --, --, 11791, 14380, 17205], mask=[False, False, False, False, True, True, True, True, True, True, True, False, False, False], fill_value=999999) We can replace the data with this information and plot a new graph, focusing on Mainland China: plt.plot(dates, china_total.T, \"--\") plt.xticks(selected_dates, dates[selected_dates]) plt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\") Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China') Its clear that masked arrays are the right solution here. We cannot represent the missing data without mischaracterizing the evolution of the curve. Fitting Data One possibility we can think of is to interpolate the missing data to estimate the number of cases in late January. Observe that we can select the masked elements using the .mask attribute: china_total.mask invalid = china_total[china_total.mask] invalid masked_array(data=[--, --, --, --, --, --, --], mask=[ True, True, True, True, True, True, True], fill_value=999999, dtype=int64) We can also access the valid entries by using the logical negation for this mask: valid = china_total[china_total.mask] valid masked_array(data=[278, 308, 440, 446, 11791, 14380, 17205], mask=[False, False, False, False, False, False, False], fill_value=999999) Now, if we want to create a very simple approximation for this data, we should take into account the valid entries around the invalid ones. So first lets select the dates for which the data is valid. Note that we can use the mask from the china_total masked array to index the dates array: dates[china_total.mask] array(['1/21/20', '1/22/20', '1/23/20', '1/24/20', '2/1/20', '2/2/20', '2/3/20'], dtype='U7') Finally, we can use the fitting functionality of the numpy.polynomial package to create a cubic polynomial model that fits the data as best as possible: t = np.arange(len(china_total)) model = np.polynomial.Polynomial.fit(t[china_total.mask], valid, deg=3) plt.plot(t, china_total) plt.plot(t, model(t), \"--\") [matplotlib.lines.Line2D at 0x7f6048898bb0] This plot is not so readable since the lines seem to be over each other, so lets summarize in a more elaborate plot. Well plot the real data when available, and show the cubic fit for unavailable data, using this fit to compute an estimate to the observed number of cases on January 28th 2020, 7 days after the beginning of the records: plt.plot(t, china_total) plt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\") plt.plot(7, model(7), \"r*\") plt.xticks([0, 7, 13], dates[[0, 7, 13]]) plt.yticks([0, model(7), 10000, 17500]) plt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"]) plt.title( \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\" \"Cubic estimate for 7 days after start\" ) Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\nCubic estimate for 7 days after start') In practice Adding -1 to missing data is not a problem with numpy.genfromtxt; in this particular case, substituting the missing value with 0 might have been fine, but well see later that this is far from a general solution. Also, it is possible to call the numpy.genfromtxt function using the usemask parameter. If usemask=True, numpy.genfromtxt automatically returns a masked array. Further reading Topics not covered in this tutorial can be found in the documentation: Hardmasks vs. softmasks The numpy.ma module Reference Ensheng Dong, Hongru Du, Lauren Gardner, An interactive web-based dashboard to track COVID-19 in real time, The Lancet Infectious Diseases, Volume 20, Issue 5, 2020, Pages 533-534, ISSN 1473-3099, https://doi.org/10.1016/S1473-3099(20)30120-1. previous Saving and sharing your NumPy arrays next NumPy Applications Contents What youll do What youll learn What youll need What are masked arrays? When can they be useful? Using masked arrays to see COVID-19 data Exploring the data Missing data Fitting Data In practice Further reading Reference By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "numpy.ndarray",
        "numpy.ndarray",
        "who_covid_19_sit_rep_time_series.csv",
        "import numpy as np\nimport os\n\n# The os.getcwd() function returns the current folder; you can change\n# the filepath variable to point to the folder where you saved the .csv file\nfilepath = os.getcwd()\nfilename = os.path.join(filepath, \"who_covid_19_sit_rep_time_series.csv\")",
        "# Note we are using skip_header and usecols to read only portions of the\n# data file into each variable.\n# Read just the dates for columns 4-18 from the first row\ndates = np.genfromtxt(\n    filename,\n    dtype=np.str_,\n    delimiter=\",\",\n    max_rows=1,\n    usecols=range(4, 18),\n    encoding=\"utf-8-sig\",\n)\n# Read the names of the geographic locations from the first two\n# columns, skipping the first six rows\nlocations = np.genfromtxt(\n    filename,\n    dtype=np.str_,\n    delimiter=\",\",\n    skip_header=6,\n    usecols=(0, 1),\n    encoding=\"utf-8-sig\",\n)\n# Read the numeric data from just the first 14 days\nnbcases = np.genfromtxt(\n    filename,\n    dtype=np.int_,\n    delimiter=\",\",\n    skip_header=6,\n    usecols=range(4, 18),\n    encoding=\"utf-8-sig\",\n)",
        "numpy.genfromtxt",
        "numpy.genfromtxt",
        "import matplotlib.pyplot as plt\n\nselected_dates = [0, 3, 11, 13]\nplt.plot(dates, nbcases.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020')",
        "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
        "array([  247,   288,   556,   817,   -22,   -22,   -15,   -10,    -9,\n          -7,    -4, 11820, 14410, 17237])",
        "array([[  258,   270,   375, ...,  7153,  9074, 11177],\n       [   14,    17,    26, ...,   520,   604,   683],\n       [   -1,     1,     1, ...,   422,   493,   566],\n       ...,\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1],\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1],\n       [   -1,    -1,    -1, ...,    -1,    -1,    -1]], shape=(263, 14))",
        "numpy.genfromtxt",
        "from numpy import ma\n\nnbcases_ma = ma.masked_values(nbcases, -1)",
        "masked_array(\n  data=[[258, 270, 375, ..., 7153, 9074, 11177],\n        [14, 17, 26, ..., 520, 604, 683],\n        [--, 1, 1, ..., 422, 493, 566],\n        ...,\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --]],\n  mask=[[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [ True, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n  fill_value=-1)",
        "plt.plot(dates, nbcases_ma[1:].T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020')",
        "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
        "masked_array(data=[278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821,\n                   14411, 17238],\n             mask=[False, False, False, False, False, False, False, False,\n                   False, False, False, False, False, False],\n       fill_value=999999)",
        "china_masked",
        "china_total = china_masked.data\nchina_total",
        "array([  278,   309,   574,   835,    10,    10,    17,    22,    23,\n          25,    28, 11821, 14411, 17238])",
        "china_mask = (\n    (locations[:, 1] == \"China\")\n    & (locations[:, 0] != \"Hong Kong\")\n    & (locations[:, 0] != \"Taiwan\")\n    & (locations[:, 0] != \"Macau\")\n    & (locations[:, 0] != \"Unspecified*\")\n)",
        "china_mask.nonzero()",
        "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 31, 33]),)",
        "china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
        "masked_array(data=[278, 308, 440, 446, --, --, --, --, --, --, --, 11791,\n                   14380, 17205],\n             mask=[False, False, False, False,  True,  True,  True,  True,\n                    True,  True,  True, False, False, False],\n       fill_value=999999)",
        "plt.plot(dates, china_total.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\")",
        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China')",
        "china_total.mask\ninvalid = china_total[china_total.mask]\ninvalid",
        "masked_array(data=[--, --, --, --, --, --, --],\n             mask=[ True,  True,  True,  True,  True,  True,  True],\n       fill_value=999999,\n            dtype=int64)",
        "valid = china_total[~china_total.mask]\nvalid",
        "masked_array(data=[278, 308, 440, 446, 11791, 14380, 17205],\n             mask=[False, False, False, False, False, False, False],\n       fill_value=999999)",
        "china_total",
        "dates[~china_total.mask]",
        "array(['1/21/20', '1/22/20', '1/23/20', '1/24/20', '2/1/20', '2/2/20',\n       '2/3/20'], dtype='<U7')",
        "t = np.arange(len(china_total))\nmodel = np.polynomial.Polynomial.fit(t[~china_total.mask], valid, deg=3)\nplt.plot(t, china_total)\nplt.plot(t, model(t), \"--\")",
        "[<matplotlib.lines.Line2D at 0x7f6048898bb0>]",
        "plt.plot(t, china_total)\nplt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\")\nplt.plot(7, model(7), \"r*\")\nplt.xticks([0, 7, 13], dates[[0, 7, 13]])\nplt.yticks([0, model(7), 10000, 17500])\nplt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"])\nplt.title(\n    \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\"\n    \"Cubic estimate for 7 days after start\"\n)",
        "Text(0.5, 1.0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\nCubic estimate for 7 days after start')",
        "numpy.genfromtxt",
        "numpy.genfromtxt",
        "usemask=True",
        "numpy.genfromtxt"
      ],
      "chunks": [
        {
          "content": "Binder Repository Open issue ipynb md pdf Masked Arrays Contents What youll do What youll learn What youll need What are masked arrays When can they be useful Using masked arrays to see COVID-19 data Exploring the data Missing data Fitting Data In practice Further reading Reference Masked Arrays What youll do Use the masked arrays module from NumPy to analyze COVID-19 data and deal with missing values",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "What youll learn Youll understand what are masked arrays and how they can be created Youll see how to access and modify data for masked arrays Youll be able to decide when the use of masked arrays is appropriate in some of your applications What youll need Basic familiarity with Python If you would like to refresh your memory, take a look at the Python tutorial Basic familiarity with NumPy To run the plots on your computer, you need matplotlib What are masked arrays Consider the following problem",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "You have a dataset with missing or invalid entries If youre doing any kind of processing on this data, and want to skip or flag these unwanted entries without just deleting them, you may have to use conditionals or filter your data somehow The numpy ma module provides some of the same functionality of NumPy ndarrays with added structure to ensure invalid entries are not used in computation From the Reference Guide: A masked array is the combination of a standard numpy ndarray and a mask",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "A mask is either nomask, indicating that no value of the associated array is invalid, or an array of booleans that determines for each element of the associated array whether the value is valid or not When an element of the mask is False, the corresponding element of the associated array is valid and is said to be unmasked When an element of the mask is True, the corresponding element of the associated array is said to be masked (invalid)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "We can think of a MaskedArray as a combination of: Data, as a regular numpy ndarray of any shape or datatype; A boolean mask with the same shape as the data; A fill_value, a value that may be used to replace the invalid entries in order to return a standard numpy ndarray When can they be useful",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "There are a few situations where masked arrays can be more useful than just eliminating the invalid entries of an array: When you want to preserve the values you masked for later processing, without copying the array; When you have to handle many arrays, each with their own mask",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "If the mask is part of the array, you avoid bugs and the code is possibly more compact; When you have different flags for missing or invalid values, and wish to preserve these flags without replacing them in the original dataset, but exclude them from computations; If you cant avoid or eliminate missing values, but dont want to deal with NaN (Not a Number) values in your operations Masked arrays are also a good idea since the numpy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "ma module also comes with a specific implementation of most NumPy universal functions (ufuncs), which means that you can still apply fast vectorized functions and operations on masked data The output is then a masked array Well see some examples of how this works in practice below Using masked arrays to see COVID-19 data From Kaggle it is possible to download a dataset with initial data about the COVID-19 outbreak in the beginning of 2020",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "We are going to look at a small subset of this data, contained in the file who_covid_19_sit_rep_time_series csv (Note that this file has been replaced with a version without missing data sometime in late 2020 ) import numpy as np import os  The os getcwd() function returns the current folder; you can change  the filepath variable to point to the folder where you saved the csv file filepath = os getcwd() filename = os path join(filepath, \"who_covid_19_sit_rep_time_series",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "csv\") The data file contains data of different types and is organized as follows: The first row is a header line that (mostly) describes the data in each column that follow in the rows below, and beginning in the fourth column, the header is the date of the observation The second through seventh row contain summary data that is of a different type than that which we are going to examine, so we will need to exclude that from the data with which we will work",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "The numerical data we wish to work with begins at column 4, row 8, and extends from there to the rightmost column and the lowermost row Lets explore the data inside this file for the first 14 days of records To gather data from the csv file, we will use the numpy genfromtxt function, making sure we select only the columns with actual numbers instead of the first four columns which contain location data",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "We also skip the first 6 rows of this file, since they contain other data we are not interested in Separately, we will extract the information about dates and location for this data Note we are using skip_header and usecols to read only portions of the  data file into each variable Read just the dates for columns 4-18 from the first row dates = np genfromtxt( filename, dtype=np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "str_, delimiter=\",\", max_rows=1, usecols=range(4, 18), encoding=\"utf-8-sig\", )  Read the names of the geographic locations from the first two  columns, skipping the first six rows locations = np genfromtxt( filename, dtype=np str_, delimiter=\",\", skip_header=6, usecols=(0, 1), encoding=\"utf-8-sig\", )  Read the numeric data from just the first 14 days nbcases = np genfromtxt( filename, dtype=np int_, delimiter=\",\", skip_header=6, usecols=range(4, 18), encoding=\"utf-8-sig\", ) Included in the numpy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "genfromtxt function call, we have selected the numpy dtype for each subset of the data (either an integer - numpy int_ - or a string of characters - numpy str_) We have also used the encoding argument to select utf-8-sig as the encoding for the file (read more about encoding in the official Python documentation You can read more about the numpy genfromtxt function from the Reference Documentation or from the Basic IO tutorial",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "Exploring the data First of all, we can plot the whole set of data we have and see what it looks like In order to get a readable plot, we select only a few of the dates to show in our x-axis ticks Note also that in our plot command, we use nbcases T (the transpose of the nbcases array) since this means we will plot each row of the file as a separate line We choose to plot a dashed line (using the '--' line style) See the matplotlib documentation for more info on this import matplotlib",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "pyplot as plt selected_dates = [0, 3, 11, 13] plt plot(dates, nbcases T, \"--\") plt xticks(selected_dates, dates[selected_dates]) plt title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\") Text(0 5, 1 0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020') The graph has a strange shape from January 24th to February 1st It would be interesting to know where this data comes from If we look at the locations array we extracted from the",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "csv file, we can see that we have two columns, where the first would contain regions and the second would contain the name of the country However, only the first few rows contain data for the the first column (province names in China) Following that, we only have country names So it would make sense to group all the data from China into a single row For this, well select from the nbcases array only the rows for which the second entry of the locations array corresponds to China",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Next, well use the numpy sum function to sum all the selected rows (axis=0) Note also that row 35 corresponds to the total counts for the whole country for each date Since we want to calculate the sum ourselves from the provinces data, we have to remove that row first from both locations and nbcases: totals_row = 35 locations = np delete(locations, (totals_row), axis=0) nbcases = np delete(nbcases, (totals_row), axis=0) china_total = nbcases[locations[:, 1] == \"China\"]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "sum(axis=0) china_total array([ 247, 288, 556, 817, -22, -22, -15, -10, -9, -7, -4, 11820, 14410, 17237]) Somethings wrong with this data - we are not supposed to have negative values in a cumulative data set Whats going on Missing data Looking at the data, heres what we find: there is a period with missing data: nbcases array([[ 258, 270, 375, , 7153, 9074, 11177], [ 14, 17, 26, , 520, 604, 683], [ -1, 1, 1, , 422, 493, 566], , [ -1, -1, -1, , -1, -1, -1], [ -1, -1, -1,",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": ", -1, -1, -1], [ -1, -1, -1, , -1, -1, -1]], shape=(263, 14)) All the -1 values we are seeing come from numpy genfromtxt attempting to read missing data from the original csv file Obviously, we dont want to compute missing data as -1 - we just want to skip this value so it doesnt interfere in our analysis After importing the numpy ma module, well create a new array, this time masking the invalid values: from numpy import ma nbcases_ma = ma",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "masked_values(nbcases, -1) If we look at the nbcases_ma masked array, this is what we have: nbcases_ma masked_array( data=[[258, 270, 375, , 7153, 9074, 11177], [14, 17, 26, , 520, 604, 683], [--, 1, 1, , 422, 493, 566], , [--, --, --, , --, --, --], [--, --, --, , --, --, --], [--, --, --, , --, --, --]], mask=[[False, False, False, , False, False, False], [False, False, False, , False, False, False], [ True, False, False, , False, False, False], , [ True, True, True,",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": ", True, True, True], [ True, True, True, , True, True, True], [ True, True, True, , True, True, True]], fill_value=-1) We can see that this is a different kind of array As mentioned in the introduction, it has three attributes (data, mask and fill_value) Keep in mind that the mask attribute has a True value for elements corresponding to invalid data (represented by two dashes in the data attribute)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "Lets try and see what the data looks like excluding the first row (data from the Hubei province in China) so we can look at the missing data more closely: plt plot(dates, nbcases_ma[1:] T, \"--\") plt xticks(selected_dates, dates[selected_dates]) plt title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\") Text(0 5, 1",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020') Now that our data has been masked, lets try summing up all the cases in China: china_masked = nbcases_ma[locations[:, 1] == \"China\"]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "sum(axis=0) china_masked masked_array(data=[278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821, 14411, 17238], mask=[False, False, False, False, False, False, False, False, False, False, False, False, False, False], fill_value=999999) Note that china_masked is a masked array, so it has a different data structure than a regular NumPy array Now, we can access its data directly by using the data attribute: china_total = china_masked",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": "data china_total array([ 278, 309, 574, 835, 10, 10, 17, 22, 23, 25, 28, 11821, 14411, 17238]) That is better: no more negative values However, we can still see that for some days, the cumulative number of cases seems to go down (from 835 to 10, for example), which does not agree with the definition of cumulative data",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "If we look more closely at the data, we can see that in the period where there was missing data in mainland China, there was valid data for Hong Kong, Taiwan, Macau and Unspecified regions of China Maybe we can remove those from the total sum of cases in China, to get a better understanding of the data First, well identify the indices of locations in mainland China: china_mask = ( (locations[:, 1] == \"China\")  (locations[:, 0] = \"Hong Kong\")  (locations[:, 0] = \"Taiwan\")  (locations[:, 0]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "= \"Macau\")  (locations[:, 0] = \"Unspecified*\") ) Now, china_mask is an array of boolean values (True or False); we can check that the indices are what we wanted with the ma nonzero method for masked arrays: china_mask nonzero() (array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 31, 33]),) Now we can correctly sum entries for mainland China: china_total = nbcases_ma[china_mask]",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "sum(axis=0) china_total masked_array(data=[278, 308, 440, 446, --, --, --, --, --, --, --, 11791, 14380, 17205], mask=[False, False, False, False, True, True, True, True, True, True, True, False, False, False], fill_value=999999) We can replace the data with this information and plot a new graph, focusing on Mainland China: plt plot(dates, china_total T, \"--\") plt xticks(selected_dates, dates[selected_dates]) plt title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\") Text(0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "5, 1 0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China') Its clear that masked arrays are the right solution here We cannot represent the missing data without mischaracterizing the evolution of the curve Fitting Data One possibility we can think of is to interpolate the missing data to estimate the number of cases in late January Observe that we can select the masked elements using the mask attribute: china_total mask invalid = china_total[china_total",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "mask] invalid masked_array(data=[--, --, --, --, --, --, --], mask=[ True, True, True, True, True, True, True], fill_value=999999, dtype=int64) We can also access the valid entries by using the logical negation for this mask: valid = china_total[china_total",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "mask] valid masked_array(data=[278, 308, 440, 446, 11791, 14380, 17205], mask=[False, False, False, False, False, False, False], fill_value=999999) Now, if we want to create a very simple approximation for this data, we should take into account the valid entries around the invalid ones So first lets select the dates for which the data is valid Note that we can use the mask from the china_total masked array to index the dates array: dates[china_total",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "mask] array(['1/21/20', '1/22/20', '1/23/20', '1/24/20', '2/1/20', '2/2/20', '2/3/20'], dtype='U7') Finally, we can use the fitting functionality of the numpy polynomial package to create a cubic polynomial model that fits the data as best as possible: t = np arange(len(china_total)) model = np polynomial Polynomial fit(t[china_total mask], valid, deg=3) plt plot(t, china_total) plt plot(t, model(t), \"--\") [matplotlib lines",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": "Line2D at 0x7f6048898bb0] This plot is not so readable since the lines seem to be over each other, so lets summarize in a more elaborate plot Well plot the real data when available, and show the cubic fit for unavailable data, using this fit to compute an estimate to the observed number of cases on January 28th 2020, 7 days after the beginning of the records: plt plot(t, china_total) plt plot(t[china_total mask], model(t)[china_total mask], \"--\", color=\"orange\") plt plot(7, model(7), \"r*\") plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": "xticks([0, 7, 13], dates[[0, 7, 13]]) plt yticks([0, model(7), 10000, 17500]) plt legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"]) plt title( \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\" \"Cubic estimate for 7 days after start\" ) Text(0 5, 1 0, 'COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\nCubic estimate for 7 days after start') In practice Adding -1 to missing data is not a problem with numpy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "genfromtxt; in this particular case, substituting the missing value with 0 might have been fine, but well see later that this is far from a general solution Also, it is possible to call the numpy genfromtxt function using the usemask parameter If usemask=True, numpy genfromtxt automatically returns a masked array Further reading Topics not covered in this tutorial can be found in the documentation: Hardmasks vs softmasks The numpy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "ma module Reference Ensheng Dong, Hongru Du, Lauren Gardner, An interactive web-based dashboard to track COVID-19 in real time, The Lancet Infectious Diseases, Volume 20, Issue 5, 2020, Pages 533-534, ISSN 1473-3099, https://doi org/10 1016/S1473-3099(20)30120-1 previous Saving and sharing your NumPy arrays next NumPy Applications Contents What youll do What youll learn What youll need What are masked arrays When can they be useful",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "Using masked arrays to see COVID-19 data Exploring the data Missing data Fitting Data In practice Further reading Reference By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-ma.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_37"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/applications.html#main-content",
      "title": "NumPy Applications \u2014 NumPy Tutorials",
      "content": "Repository Open issue .md .pdf NumPy Applications NumPy Applications A collection of highlighting the use of NumPy for applications in science, engineering, and data analysis. Determining Moores Law with real data in NumPy Deep learning on MNIST X-ray image processing Determining Static Equilibrium in NumPy Plotting Fractals Analyzing the impact of the lockdown on air quality in Delhi, India previous Masked Arrays next Determining Moores Law with real data in NumPy By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [],
      "chunks": [
        {
          "content": "Repository Open issue md pdf NumPy Applications NumPy Applications A collection of highlighting the use of NumPy for applications in science, engineering, and data analysis",
          "url": "https://numpy.org/numpy-tutorials/applications.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Determining Moores Law with real data in NumPy Deep learning on MNIST X-ray image processing Determining Static Equilibrium in NumPy Plotting Fractals Analyzing the impact of the lockdown on air quality in Delhi, India previous Masked Arrays next Determining Moores Law with real data in NumPy By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/applications.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
      "title": "Determining Moore\u2019s Law with real data in NumPy \u2014 NumPy Tutorials",
      "content": "Binder Repository Open issue .ipynb .md .pdf Determining Moores Law with real data in NumPy Contents What youll do Skills youll learn What youll need Building Moores law as an exponential function Loading historical manufacturing data to your workspace Calculating the historical growth curve for transistors Sharing your results as zipped arrays and a csv Zipping the arrays into a file Creating your own comma separated value file Wrapping up References Determining Moores Law with real data in NumPy The number of transistors reported per a given chip plotted on a log scale in the y axis with the date of introduction on the linear scale x-axis. The blue data points are from a transistor count table. The red line is an ordinary least squares prediction and the orange line is Moores law. What youll do In 1965, engineer Gordon Moore predicted that transistors on a chip would double every two years in the coming decade [1]. Youll compare Moores prediction against actual transistor counts in the 53 years following his prediction. You will determine the best-fit constants to describe the exponential growth of transistors on semiconductors compared to Moores Law. Skills youll learn Load data from a *.csv file Perform linear regression and predict exponential growth using ordinary least squares Youll compare exponential growth constants between models Share your analysis in a file: as NumPy zipped files *.npz as a *.csv file Assess the amazing progress semiconductor manufacturers have made in the last five decades What youll need 1. These packages: NumPy Matplotlib imported with the following commands import matplotlib.pyplot as plt import numpy as np 2. Since this is an exponential growth law you need a little background in doing math with natural logs and exponentials. Youll use these NumPy and Matplotlib functions: np.loadtxt: this function loads text into a NumPy array np.log: this function takes the natural log of all elements in a NumPy array np.exp: this function takes the exponential of all elements in a NumPy array lambda: this is a minimal function definition for creating a function model plt.semilogy: this function will plot x-y data onto a figure with a linear x-axis and \\(\\log_{10}\\) y-axis plt.plot: this function will plot x-y data on linear axes slicing arrays: view parts of the data loaded into the workspace, slice the arrays e.g. x[:10] for the first 10 values in the array, x boolean array indexing: to view parts of the data that match a given condition use boolean operations to index an array np.block: to combine arrays into 2D arrays np.newaxis: to change a 1D vector to a row or column vector np.savez and np.savetxt: these two functions will save your arrays in zipped array format and text, respectively Building Moores law as an exponential function Your empirical model assumes that the number of transistors per semiconductor follows an exponential growth, \\(\\log(\\text{transistor_count})= f(\\text{year}) = A\\cdot \\text{year}+B,\\) where \\(A\\) and \\(B\\) are fitting constants. You use semiconductor manufacturers data to find the fitting constants. You determine these constants for Moores law by specifying the rate for added transistors, 2, and giving an initial number of transistors for a given year. You state Moores law in an exponential form as follows, \\(\\text{transistor_count}= e{A_M\\cdot \\text{year} +B_M}.\\) Where \\(A_M\\) and \\(B_M\\) are constants that double the number of transistors every two years and start at 2250 transistors in 1971, \\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = 2 = \\dfrac{e{B_M}e{A_M \\text{year} + 2A_M}}{e{B_M}e{A_M \\text{year}}} = e{2A_M} \\rightarrow A_M = \\frac{\\log(2)}{2}\\) \\(\\log(2250) = \\frac{\\log(2)}{2}\\cdot 1971 + B_M \\rightarrow B_M = \\log(2250)-\\frac{\\log(2)}{2}\\cdot 1971\\) so Moores law stated as an exponential function is \\(\\log(\\text{transistor_count})= A_M\\cdot \\text{year}+B_M,\\) where \\(A_M=0.3466\\) \\(B_M=-675.4\\) Since the function represents Moores law, define it as a Python function using lambda A_M = np.log(2) / 2 B_M = np.log(2250) - A_M * 1971 Moores_law = lambda year: np.exp(B_M) * np.exp(A_M * year) In 1971, there were 2250 transistors on the Intel 4004 chip. Use Moores_law to check the number of semiconductors Gordon Moore would expect in 1973. ML_1971 = Moores_law(1971) ML_1973 = Moores_law(1973) print(\"In 1973, G. Moore expects {:.0f} transistors on Intels chips\".format(ML_1973)) print(\"This is x{:.2f} more transistors than 1971\".format(ML_1973 / ML_1971)) In 1973, G. Moore expects 4500 transistors on Intels chips This is x2.00 more transistors than 1971 Loading historical manufacturing data to your workspace Now, make a prediction based upon the historical data for semiconductors per chip. The Transistor Count [3] each year is in the transistor_data.csv file. Before loading a *.csv file into a NumPy array, its a good idea to inspect the structure of the file first. Then, locate the columns of interest and save them to a variable. Save two columns of the file to the array, data. Here, print out the first 10 rows of transistor_data.csv. The columns are Processor MOS transistor count Date of Introduction Designer MOSprocess Area Intel 4004 (4-bit 16-pin) 2250 1971 Intel 10,000 nm 12 mm\u00b2       ! head transistor_data.csv Processor,MOS transistor count,Date of Introduction,Designer,MOSprocess,Area Intel 4004 (4-bit 16-pin),2250,1971,Intel,\"10,000 nm\",12 mm\u00b2 Intel 8008 (8-bit 18-pin),3500,1972,Intel,\"10,000 nm\",14 mm\u00b2 NEC \u03bcCOM-4 (4-bit 42-pin),2500,1973,NEC,\"7,500 nm\",? Intel 4040 (4-bit 16-pin),3000,1974,Intel,\"10,000 nm\",12 mm\u00b2 Motorola 6800 (8-bit 40-pin),4100,1974,Motorola,\"6,000 nm\",16 mm\u00b2 Intel 8080 (8-bit 40-pin),6000,1974,Intel,\"6,000 nm\",20 mm\u00b2 TMS 1000 (4-bit 28-pin),8000,1974,Texas Instruments,\"8,000 nm\",11 mm\u00b2 MOS Technology 6502 (8-bit 40-pin),4528,1975,MOS Technology,\"8,000 nm\",21 mm\u00b2 Intersil IM6100 (12-bit 40-pin; clone of PDP-8),4000,1975,Intersil,, You dont need the columns that specify Processor, Designer, MOSprocess, or Area. That leaves the second and third columns, MOS transistor count and Date of Introduction, respectively. Next, you load these two columns into a NumPy array using np.loadtxt. The extra options below will put the data in the desired format: delimiter = ',': specify delimeter as a comma , (this is the default behavior) usecols = [1,2]: import the second and third columns from the csv skiprows = 1: do not use the first row, because its a header row data = np.loadtxt(\"transistor_data.csv\", delimiter=\",\", usecols=[1, 2], skiprows=1) You loaded the entire history of semiconducting into a NumPy array named data. The first column is the MOS transistor count and the second column is the Date of Introduction in a four-digit year. Next, make the data easier to read and manage by assigning the two columns to variables, year and transistor_count. Print out the first 10 values by slicing the year and transistor_count arrays with [:10]. Print these values out to check that you have the saved the data to the correct variables. year = data[:, 1]  grab the second column and assign transistor_count = data[:, 0]  grab the first column and assign print(\"year:\\t\\t\", year[:10]) print(\"trans. cnt:\\t\", transistor_count[:10]) year: [1971. 1972. 1973. 1974. 1974. 1974. 1974. 1975. 1975. 1975.] trans. cnt: [2250. 3500. 2500. 3000. 4100. 6000. 8000. 4528. 4000. 5000.] You are creating a function that predicts the transistor count given a year. You have an independent variable, year, and a dependent variable, transistor_count. Transform the dependent variable to log-scale, \\(y_i = \\log(\\) transistor_count[i] \\(),\\) resulting in a linear equation, \\(y_i = A\\cdot \\text{year} +B\\). yi = np.log(transistor_count) Calculating the historical growth curve for transistors Your model assume that yi is a function of year. Now, find the best-fit model that minimizes the difference between \\(y_i\\) and \\(A\\cdot \\text{year} +B, \\) as such \\(\\min \\sumy_i - (A\\cdot \\text{year}_i + B)2.\\) This sum of squares error can be succinctly represented as arrays as such \\(\\sum\\mathbf{y}-\\mathbf{Z} [A,B]T2,\\) where \\(\\mathbf{y}\\) are the observations of the log of the number of transistors in a 1D array and \\(\\mathbf{Z}=[\\text{year}_i1,\\text{year}_i0]\\) are the polynomial terms for \\(\\text{year}_i\\) in the first and second columns. By creating this set of regressors in the \\(\\mathbf{Z}-\\)matrix you set up an ordinary least squares statistical model. Z is a linear model with two parameters, i.e. a polynomial with degree 1. Therefore we can represent the model with numpy.polynomial.Polynomial and use the fitting functionality to determine the model parameters: model = np.polynomial.Polynomial.fit(year, yi, deg=1) By default, Polynomial.fit performs the fit in the domain determined by the independent variable (year in this case). The coefficients for the unscaled and unshifted model can be recovered with the convert method: model = model.convert() model \\[x \\mapsto \\text{-666.32640635} + \\text{0.34163208}\\,x\\] The individual parameters \\(A\\) and \\(B\\) are the coefficients of our linear model: B, A = model Did manufacturers double the transistor count every two years? You have the final formula, \\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = xFactor = \\dfrac{e{B}e{A( \\text{year} + 2)}}{e{B}e{A \\text{year}}} = e{2A}\\) where increase in number of transistors is \\(xFactor,\\) number of years is 2, and \\(A\\) is the best fit slope on the semilog function. print(f\"Rate of semiconductors added on a chip every 2 years: {np.exp(2 * A):.2f}\") Rate of semiconductors added on a chip every 2 years: 1.98 Based upon your least-squares regression model, the number of semiconductors per chip increased by a factor of \\(1.98\\) every two years. You have a model that predicts the number of semiconductors each year. Now compare your model to the actual manufacturing reports. Plot the linear regression results and all of the transistor counts. Here, use plt.semilogy to plot the number of transistors on a log-scale and the year on a linear scale. You have defined a three arrays to get to a final model \\(y_i = \\log(\\text{transistor_count}),\\) \\(y_i = A \\cdot \\text{year} + B,\\) and \\(\\log(\\text{transistor_count}) = A\\cdot \\text{year} + B,\\) your variables, transistor_count, year, and yi all have the same dimensions, (179,). NumPy arrays need the same dimensions to make a plot. The predicted number of transistors is now \\(\\text{transistor_count}_{\\text{predicted}} = eBe{A\\cdot \\text{year}}\\). In the next plot, use the fivethirtyeight style sheet. The style sheet replicates https://fivethirtyeight.com elements. Change the matplotlib style with plt.style.use. transistor_count_predicted = np.exp(B) * np.exp(A * year) transistor_Moores_law = Moores_law(year) plt.style.use(\"fivethirtyeight\") plt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\") plt.semilogy(year, transistor_count_predicted, label=\"linear regression\") plt.plot(year, transistor_Moores_law, label=\"Moore's Law\") plt.title( \"MOS transistor count per microprocessor\\n\" + \"every two years \\n\" + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2)) ) plt.xlabel(\"year introduced\") plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5)) plt.ylabel(\" of transistors\\nper microprocessor\") Text(0, 0.5, ' of transistors\\nper microprocessor') A scatter plot of MOS transistor count per microprocessor every two years with a red line for the ordinary least squares prediction and an orange line for Moores law. The linear regression captures the increase in the number of transistors per semiconductors each year. In 2015, semiconductor manufacturers claimed they could not keep up with Moores law anymore. Your analysis shows that since 1971, the average increase in transistor count was x1.98 every 2 years, but Gordon Moore predicted it would be x2 every 2 years. That is an amazing prediction. Consider the year 2017. Compare the data to your linear regression model and Gordon Moores prediction. First, get the transistor counts from the year 2017. You can do this with a Boolean comparator, year == 2017. Then, make a prediction for 2017 with Moores_law defined above and plugging in your best fit constants into your function \\(\\text{transistor_count} = e{B}e{A\\cdot \\text{year}}\\). A great way to compare these measurements is to compare your prediction and Moores prediction to the average transistor count and look at the range of reported values for that year. Use the plt.plot option, alpha=0.2, to increase the transparency of the data. The more opaque the points appear, the more reported values lie on that measurement. The green \\(+\\) is the average reported transistor count for 2017. Plot your predictions for \\pm\\frac{1}{2}years. transistor_count2017 = transistor_count[year == 2017] print( transistor_count2017.max(), transistor_count2017.min(), transistor_count2017.mean() ) y = np.linspace(2016.5, 2017.5) your_model2017 = np.exp(B) * np.exp(A * y) Moore_Model2017 = Moores_law(y) plt.plot( 2017 * np.ones(np.sum(year == 2017)), transistor_count2017, \"ro\", label=\"2017\", alpha=0.2, ) plt.plot(2017, transistor_count2017.mean(), \"g+\", markersize=20, mew=6) plt.plot(y, your_model2017, label=\"Your prediction\") plt.plot(y, Moore_Model2017, label=\"Moores law\") plt.ylabel(\" of transistors\\nper microprocessor\") plt.legend() 19200000000.0 250000000.0 7050000000.0 matplotlib.legend.Legend at 0x7fa70c3043d0 The result is that your model is close to the mean, but Gordon Moores prediction is closer to the maximum number of transistors per microprocessor produced in 2017. Even though semiconductor manufacturers thought that the growth would slow, once in 1975 and now again approaching 2025, manufacturers are still producing semiconductors every 2 years that nearly double the number of transistors. The linear regression model is much better at predicting the average than extreme values because it satisfies the condition to minimize \\(\\sum y_i - A\\cdot \\text{year}[i]+B2\\). Sharing your results as zipped arrays and a csv The last step, is to share your findings. You created new arrays that represent a linear regression model and Gordon Moores prediction. You started this process by importing a csv file into a NumPy array using np.loadtxt, to save your model use two approaches np.savez: save NumPy arrays for other Python sessions np.savetxt: save a csv file with the original data and your predicted data Zipping the arrays into a file Using np.savez, you can save thousands of arrays and give them names. The function np.load will load the arrays back into the workspace as a dictionary. Youll save a five arrays so the next user will have the year, transistor count, predicted transistor count, Gordon Moores predicted count, and fitting constants. Add one more variable that other users can use to understand the model, notes. notes = \"the arrays in this file are the result of a linear regression model\\n\" notes += \"the arrays include\\nyear: year of manufacture\\n\" notes += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\" notes += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format( B, A ) notes += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format( B_M, A_M ) notes += \"regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B\" print(notes) the arrays in this file are the result of a linear regression model the arrays include year: year of manufacture transistor_count: number of transistors reported by manufacturers in a given year transistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year) transistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year) regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B np.savez( \"mooreslaw_regression.npz\", notes=notes, year=year, transistor_count=transistor_count, transistor_count_predicted=transistor_count_predicted, transistor_Moores_law=transistor_Moores_law, regression_csts=(A, B), ) results = np.load(\"mooreslaw_regression.npz\") print(results[\"regression_csts\"][1]) -666.3264063536233 ! ls air-quality-data.csv mooreslaw_regression.npz mooreslaw-tutorial.md pairing.md save-load-arrays.md _static text_preprocessing.py transistor_data.csv tutorial-air-quality-analysis.md tutorial-deep-learning-on-mnist.md tutorial-deep-reinforcement-learning-with-pong-from-pixels.md tutorial-ma.md tutorial-nlp-from-scratch tutorial-nlp-from-scratch.md tutorial-plotting-fractals tutorial-plotting-fractals.md tutorial-static_equilibrium.md tutorial-style-guide.md tutorial-svd.md tutorial-x-ray-image-processing tutorial-x-ray-image-processing.md who_covid_19_sit_rep_time_series.csv The benefit of np.savez is you can save hundreds of arrays with different shapes and types. Here, you saved 4 arrays that are double precision floating point numbers shape = (179,), one array that was text, and one array of double precision floating point numbers shape = (2,). This is the preferred method for saving NumPy arrays for use in another analysis. Creating your own comma separated value file If you want to share data and view the results in a table, then you have to create a text file. Save the data using np.savetxt. This function is more limited than np.savez. Delimited files, like csvs, need 2D arrays. Prepare the data for export by creating a new 2D array whose columns contain the data of interest. Use the header option to describe the data and the columns of the file. Define another variable that contains file information as head. head = \"the columns in this file are the result of a linear regression model\\n\" head += \"the columns include\\nyear: year of manufacture\\n\" head += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\" head += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format( B, A ) head += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format( B_M, A_M ) head += \"year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\" print(head) the columns in this file are the result of a linear regression model the columns include year: year of manufacture transistor_count: number of transistors reported by manufacturers in a given year transistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year) transistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year) year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law: Build a single 2D array to export to csv. Tabular data is inherently two dimensional. You need to organize your data to fit this 2D structure. Use year, transistor_count, transistor_count_predicted, and transistor_Moores_law as the first through fourth columns, respectively. Put the calculated constants in the header since they do not fit the (179,) shape. The np.block function appends arrays together to create a new, larger array. Arrange the 1D vectors as columns using np.newaxis e.g.  year.shape (179,)  year[:,np.newaxis].shape (179,1) output = np.block( [ year[:, np.newaxis], transistor_count[:, np.newaxis], transistor_count_predicted[:, np.newaxis], transistor_Moores_law[:, np.newaxis], ] ) Creating the mooreslaw_regression.csv with np.savetxt, use three options to create the desired file format: X = output : use output block to write the data into the file delimiter = ',' : use commas to separate columns in the file header = head : use the header head defined above np.savetxt(\"mooreslaw_regression.csv\", X=output, delimiter=\",\", header=head) ! head mooreslaw_regression.csv  the columns in this file are the result of a linear regression model  the columns include  year: year of manufacture  transistor_count: number of transistors reported by manufacturers in a given year  transistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)  transistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)  year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law: 1.971000000000000000e+03,2.250000000000000000e+03,1.130514785642591505e+03,2.249999999999916326e+03 1.972000000000000000e+03,3.500000000000000000e+03,1.590908400344571419e+03,3.181980515339620069e+03 1.973000000000000000e+03,2.500000000000000000e+03,2.238793840142739100e+03,4.500000000000097316e+03 Wrapping up In conclusion, you have compared historical data for semiconductor manufacturers to Moores law and created a linear regression model to find the average number of transistors added to each microprocessor every two years. Gordon Moore predicted the number of transistors would double every two years from 1965 through 1975, but the average growth has maintained a consistent increase of \\(\\times 1.98 \\pm 0.01\\) every two years from 1971 through 2019. In 2015, Moore revised his prediction to say Moores law should hold until 2025. [2]. You can share these results as a zipped NumPy array file, mooreslaw_regression.npz, or as another csv, mooreslaw_regression.csv. The amazing progress in semiconductor manufacturing has enabled new industries and computational power. This analysis should give you a small insight into how incredible this growth has been over the last half-century. References Moores Law. Wikipedia article. Accessed Oct. 1, 2020. Courtland, Rachel. Gordon Moore: The Man Whose Name Means Progress. IEEE Spectrum. 30 Mar. 2015.. Transistor Count. Wikipedia article. Accessed Oct. 1, 2020. previous NumPy Applications next Deep learning on MNIST Contents What youll do Skills youll learn What youll need Building Moores law as an exponential function Loading historical manufacturing data to your workspace Calculating the historical growth curve for transistors Sharing your results as zipped arrays and a csv Zipping the arrays into a file Creating your own comma separated value file Wrapping up References By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "import matplotlib.pyplot as plt\nimport numpy as np",
        "plt.semilogy",
        "A_M = np.log(2) / 2\nB_M = np.log(2250) - A_M * 1971\nMoores_law = lambda year: np.exp(B_M) * np.exp(A_M * year)",
        "ML_1971 = Moores_law(1971)\nML_1973 = Moores_law(1973)\nprint(\"In 1973, G. Moore expects {:.0f} transistors on Intels chips\".format(ML_1973))\nprint(\"This is x{:.2f} more transistors than 1971\".format(ML_1973 / ML_1971))",
        "In 1973, G. Moore expects 4500 transistors on Intels chips\nThis is x2.00 more transistors than 1971",
        "transistor_data.csv",
        "transistor_data.csv",
        "! head transistor_data.csv",
        "Processor,MOS transistor count,Date of Introduction,Designer,MOSprocess,Area\nIntel 4004 (4-bit  16-pin),2250,1971,Intel,\"10,000\u00a0nm\",12\u00a0mm\u00b2\nIntel 8008 (8-bit  18-pin),3500,1972,Intel,\"10,000\u00a0nm\",14\u00a0mm\u00b2\nNEC \u03bcCOM-4 (4-bit  42-pin),2500,1973,NEC,\"7,500\u00a0nm\",?\nIntel 4040 (4-bit  16-pin),3000,1974,Intel,\"10,000\u00a0nm\",12\u00a0mm\u00b2\nMotorola 6800 (8-bit  40-pin),4100,1974,Motorola,\"6,000\u00a0nm\",16\u00a0mm\u00b2\nIntel 8080 (8-bit  40-pin),6000,1974,Intel,\"6,000\u00a0nm\",20\u00a0mm\u00b2\nTMS 1000 (4-bit  28-pin),8000,1974,Texas Instruments,\"8,000\u00a0nm\",11\u00a0mm\u00b2\nMOS Technology 6502 (8-bit  40-pin),4528,1975,MOS Technology,\"8,000\u00a0nm\",21\u00a0mm\u00b2\nIntersil IM6100 (12-bit  40-pin; clone of PDP-8),4000,1975,Intersil,,",
        "delimiter = ','",
        "usecols = [1,2]",
        "skiprows = 1",
        "data = np.loadtxt(\"transistor_data.csv\", delimiter=\",\", usecols=[1, 2], skiprows=1)",
        "transistor_count",
        "transistor_count",
        "year = data[:, 1]  # grab the second column and assign\ntransistor_count = data[:, 0]  # grab the first column and assign\n\nprint(\"year:\\t\\t\", year[:10])\nprint(\"trans. cnt:\\t\", transistor_count[:10])",
        "year:\t\t [1971. 1972. 1973. 1974. 1974. 1974. 1974. 1975. 1975. 1975.]\ntrans. cnt:\t [2250. 3500. 2500. 3000. 4100. 6000. 8000. 4528. 4000. 5000.]",
        "transistor_count",
        "transistor_count[i]",
        "yi = np.log(transistor_count)",
        "numpy.polynomial.Polynomial",
        "model = np.polynomial.Polynomial.fit(year, yi, deg=1)",
        "Polynomial.fit",
        "model = model.convert()\nmodel",
        "B, A = model",
        "print(f\"Rate of semiconductors added on a chip every 2 years: {np.exp(2 * A):.2f}\")",
        "Rate of semiconductors added on a chip every 2 years: 1.98",
        "plt.semilogy",
        "transistor_count",
        "fivethirtyeight",
        "plt.style.use",
        "transistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")",
        "Text(0, 0.5, '# of transistors\\nper microprocessor')",
        "year == 2017",
        "transistor_count2017 = transistor_count[year == 2017]\nprint(\n    transistor_count2017.max(), transistor_count2017.min(), transistor_count2017.mean()\n)\ny = np.linspace(2016.5, 2017.5)\nyour_model2017 = np.exp(B) * np.exp(A * y)\nMoore_Model2017 = Moores_law(y)\n\nplt.plot(\n    2017 * np.ones(np.sum(year == 2017)),\n    transistor_count2017,\n    \"ro\",\n    label=\"2017\",\n    alpha=0.2,\n)\nplt.plot(2017, transistor_count2017.mean(), \"g+\", markersize=20, mew=6)\n\nplt.plot(y, your_model2017, label=\"Your prediction\")\nplt.plot(y, Moore_Model2017, label=\"Moores law\")\nplt.ylabel(\"# of transistors\\nper microprocessor\")\nplt.legend()",
        "19200000000.0 250000000.0 7050000000.0",
        "<matplotlib.legend.Legend at 0x7fa70c3043d0>",
        "notes = \"the arrays in this file are the result of a linear regression model\\n\"\nnotes += \"the arrays include\\nyear: year of manufacture\\n\"\nnotes += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\"\nnotes += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B, A\n)\nnotes += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B_M, A_M\n)\nnotes += \"regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B\"\nprint(notes)",
        "the arrays in this file are the result of a linear regression model\nthe arrays include\nyear: year of manufacture\ntransistor_count: number of transistors reported by manufacturers in a given year\ntransistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\ntransistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\nregression_csts: linear regression constants A and B for log(transistor_count)=A*year+B",
        "np.savez(\n    \"mooreslaw_regression.npz\",\n    notes=notes,\n    year=year,\n    transistor_count=transistor_count,\n    transistor_count_predicted=transistor_count_predicted,\n    transistor_Moores_law=transistor_Moores_law,\n    regression_csts=(A, B),\n)",
        "results = np.load(\"mooreslaw_regression.npz\")",
        "print(results[\"regression_csts\"][1])",
        "-666.3264063536233",
        "air-quality-data.csv\nmooreslaw_regression.npz\nmooreslaw-tutorial.md\npairing.md\nsave-load-arrays.md\n_static\ntext_preprocessing.py\ntransistor_data.csv\ntutorial-air-quality-analysis.md\ntutorial-deep-learning-on-mnist.md\ntutorial-deep-reinforcement-learning-with-pong-from-pixels.md\ntutorial-ma.md\ntutorial-nlp-from-scratch\ntutorial-nlp-from-scratch.md\ntutorial-plotting-fractals\ntutorial-plotting-fractals.md\ntutorial-static_equilibrium.md\ntutorial-style-guide.md\ntutorial-svd.md\ntutorial-x-ray-image-processing\ntutorial-x-ray-image-processing.md\nwho_covid_19_sit_rep_time_series.csv",
        "head = \"the columns in this file are the result of a linear regression model\\n\"\nhead += \"the columns include\\nyear: year of manufacture\\n\"\nhead += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\"\nhead += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B, A\n)\nhead += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B_M, A_M\n)\nhead += \"year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\"\nprint(head)",
        "the columns in this file are the result of a linear regression model\nthe columns include\nyear: year of manufacture\ntransistor_count: number of transistors reported by manufacturers in a given year\ntransistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\ntransistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\nyear:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:",
        "transistor_count",
        "transistor_count_predicted",
        "transistor_Moores_law",
        ">>> year.shape\n(179,)\n>>> year[:,np.newaxis].shape\n(179,1)",
        "output = np.block(\n    [\n        year[:, np.newaxis],\n        transistor_count[:, np.newaxis],\n        transistor_count_predicted[:, np.newaxis],\n        transistor_Moores_law[:, np.newaxis],\n    ]\n)",
        "mooreslaw_regression.csv",
        "delimiter = ','",
        "header = head",
        "np.savetxt(\"mooreslaw_regression.csv\", X=output, delimiter=\",\", header=head)",
        "! head mooreslaw_regression.csv",
        "# the columns in this file are the result of a linear regression model\n# the columns include\n# year: year of manufacture\n# transistor_count: number of transistors reported by manufacturers in a given year\n# transistor_count_predicted: linear regression model = exp(-666.33)*exp(0.34*year)\n# transistor_Moores_law: Moores law =exp(-675.38)*exp(0.35*year)\n# year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\n1.971000000000000000e+03,2.250000000000000000e+03,1.130514785642591505e+03,2.249999999999916326e+03\n1.972000000000000000e+03,3.500000000000000000e+03,1.590908400344571419e+03,3.181980515339620069e+03\n1.973000000000000000e+03,2.500000000000000000e+03,2.238793840142739100e+03,4.500000000000097316e+03",
        "mooreslaw_regression.npz",
        "mooreslaw_regression.csv"
      ],
      "chunks": [
        {
          "content": "Binder Repository Open issue ipynb md",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "pdf Determining Moores Law with real data in NumPy Contents What youll do Skills youll learn What youll need Building Moores law as an exponential function Loading historical manufacturing data to your workspace Calculating the historical growth curve for transistors Sharing your results as zipped arrays and a csv Zipping the arrays into a file Creating your own comma separated value file Wrapping up References Determining Moores Law with real data in NumPy The number of transistors reported per a given chip plotted on a log scale in the y axis with the date of introduction on the linear scale x-axis",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "The blue data points are from a transistor count table The red line is an ordinary least squares prediction and the orange line is Moores law What youll do In 1965, engineer Gordon Moore predicted that transistors on a chip would double every two years in the coming decade [1] Youll compare Moores prediction against actual transistor counts in the 53 years following his prediction",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "You will determine the best-fit constants to describe the exponential growth of transistors on semiconductors compared to Moores Law Skills youll learn Load data from a * csv file Perform linear regression and predict exponential growth using ordinary least squares Youll compare exponential growth constants between models Share your analysis in a file: as NumPy zipped files * npz as a *",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "csv file Assess the amazing progress semiconductor manufacturers have made in the last five decades What youll need 1 These packages: NumPy Matplotlib imported with the following commands import matplotlib pyplot as plt import numpy as np 2 Since this is an exponential growth law you need a little background in doing math with natural logs and exponentials Youll use these NumPy and Matplotlib functions: np loadtxt: this function loads text into a NumPy array np",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "log: this function takes the natural log of all elements in a NumPy array np exp: this function takes the exponential of all elements in a NumPy array lambda: this is a minimal function definition for creating a function model plt semilogy: this function will plot x-y data onto a figure with a linear x-axis and \\(\\log_{10}\\) y-axis plt plot: this function will plot x-y data on linear axes slicing arrays: view parts of the data loaded into the workspace, slice the arrays e g",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "x[:10] for the first 10 values in the array, x boolean array indexing: to view parts of the data that match a given condition use boolean operations to index an array np block: to combine arrays into 2D arrays np newaxis: to change a 1D vector to a row or column vector np savez and np",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": "savetxt: these two functions will save your arrays in zipped array format and text, respectively Building Moores law as an exponential function Your empirical model assumes that the number of transistors per semiconductor follows an exponential growth, \\(\\log(\\text{transistor_count})= f(\\text{year}) = A\\cdot \\text{year}+B,\\) where \\(A\\) and \\(B\\) are fitting constants You use semiconductor manufacturers data to find the fitting constants",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "You determine these constants for Moores law by specifying the rate for added transistors, 2, and giving an initial number of transistors for a given year You state Moores law in an exponential form as follows, \\(\\text{transistor_count}= e{A_M\\cdot \\text{year} +B_M}",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "\\) Where \\(A_M\\) and \\(B_M\\) are constants that double the number of transistors every two years and start at 2250 transistors in 1971, \\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = 2 = \\dfrac{e{B_M}e{A_M \\text{year} + 2A_M}}{e{B_M}e{A_M \\text{year}}} = e{2A_M} \\rightarrow A_M = \\frac{\\log(2)}{2}\\) \\(\\log(2250) = \\frac{\\log(2)}{2}\\cdot 1971 + B_M \\rightarrow B_M = \\log(2250)-\\frac{\\log(2)}{2}\\cdot 1971\\) so Moores law stated as an exponential function is \\(\\log(\\text{transistor_count})= A_M\\cdot \\text{year}+B_M,\\) where \\(A_M=0",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "3466\\) \\(B_M=-675 4\\) Since the function represents Moores law, define it as a Python function using lambda A_M = np log(2) / 2 B_M = np log(2250) - A_M * 1971 Moores_law = lambda year: np exp(B_M) * np exp(A_M * year) In 1971, there were 2250 transistors on the Intel 4004 chip Use Moores_law to check the number of semiconductors Gordon Moore would expect in 1973 ML_1971 = Moores_law(1971) ML_1973 = Moores_law(1973) print(\"In 1973, G Moore expects {: 0f} transistors on Intels chips\"",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "format(ML_1973)) print(\"This is x{: 2f} more transistors than 1971\" format(ML_1973 / ML_1971)) In 1973, G Moore expects 4500 transistors on Intels chips This is x2 00 more transistors than 1971 Loading historical manufacturing data to your workspace Now, make a prediction based upon the historical data for semiconductors per chip The Transistor Count [3] each year is in the transistor_data csv file Before loading a *",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "csv file into a NumPy array, its a good idea to inspect the structure of the file first Then, locate the columns of interest and save them to a variable Save two columns of the file to the array, data Here, print out the first 10 rows of transistor_data csv The columns are Processor MOS transistor count Date of Introduction Designer MOSprocess Area Intel 4004 (4-bit 16-pin) 2250 1971 Intel 10,000 nm 12 mm\u00b2 head transistor_data",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "csv Processor,MOS transistor count,Date of Introduction,Designer,MOSprocess,Area Intel 4004 (4-bit 16-pin),2250,1971,Intel,\"10,000 nm\",12 mm\u00b2 Intel 8008 (8-bit 18-pin),3500,1972,Intel,\"10,000 nm\",14 mm\u00b2 NEC \u03bcCOM-4 (4-bit 42-pin),2500,1973,NEC,\"7,500 nm\",",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "Intel 4040 (4-bit 16-pin),3000,1974,Intel,\"10,000 nm\",12 mm\u00b2 Motorola 6800 (8-bit 40-pin),4100,1974,Motorola,\"6,000 nm\",16 mm\u00b2 Intel 8080 (8-bit 40-pin),6000,1974,Intel,\"6,000 nm\",20 mm\u00b2 TMS 1000 (4-bit 28-pin),8000,1974,Texas Instruments,\"8,000 nm\",11 mm\u00b2 MOS Technology 6502 (8-bit 40-pin),4528,1975,MOS Technology,\"8,000 nm\",21 mm\u00b2 Intersil IM6100 (12-bit 40-pin; clone of PDP-8),4000,1975,Intersil,, You dont need the columns that specify Processor, Designer, MOSprocess, or Area",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "That leaves the second and third columns, MOS transistor count and Date of Introduction, respectively Next, you load these two columns into a NumPy array using np loadtxt The extra options below will put the data in the desired format: delimiter = ',': specify delimeter as a comma , (this is the default behavior) usecols = [1,2]: import the second and third columns from the csv skiprows = 1: do not use the first row, because its a header row data = np loadtxt(\"transistor_data",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "csv\", delimiter=\",\", usecols=[1, 2], skiprows=1) You loaded the entire history of semiconducting into a NumPy array named data The first column is the MOS transistor count and the second column is the Date of Introduction in a four-digit year Next, make the data easier to read and manage by assigning the two columns to variables, year and transistor_count Print out the first 10 values by slicing the year and transistor_count arrays with [:10]",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Print these values out to check that you have the saved the data to the correct variables year = data[:, 1]  grab the second column and assign transistor_count = data[:, 0]  grab the first column and assign print(\"year:\\t\\t\", year[:10]) print(\"trans cnt:\\t\", transistor_count[:10]) year: [1971 1972 1973 1974 1974 1974 1974 1975 1975 1975 ] trans cnt: [2250 3500 2500 3000 4100 6000 8000 4528 4000 5000 ] You are creating a function that predicts the transistor count given a year",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "You have an independent variable, year, and a dependent variable, transistor_count Transform the dependent variable to log-scale, \\(y_i = \\log(\\) transistor_count[i] \\(),\\) resulting in a linear equation, \\(y_i = A\\cdot \\text{year} +B\\) yi = np log(transistor_count) Calculating the historical growth curve for transistors Your model assume that yi is a function of year",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "Now, find the best-fit model that minimizes the difference between \\(y_i\\) and \\(A\\cdot \\text{year} +B, \\) as such \\(\\min \\sumy_i - (A\\cdot \\text{year}_i + B)2 \\) This sum of squares error can be succinctly represented as arrays as such \\(\\sum\\mathbf{y}-\\mathbf{Z} [A,B]T2,\\) where \\(\\mathbf{y}\\) are the observations of the log of the number of transistors in a 1D array and \\(\\mathbf{Z}=[\\text{year}_i1,\\text{year}_i0]\\) are the polynomial terms for \\(\\text{year}_i\\) in the first and second columns",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "By creating this set of regressors in the \\(\\mathbf{Z}-\\)matrix you set up an ordinary least squares statistical model Z is a linear model with two parameters, i e a polynomial with degree 1 Therefore we can represent the model with numpy polynomial Polynomial and use the fitting functionality to determine the model parameters: model = np polynomial Polynomial fit(year, yi, deg=1) By default, Polynomial fit performs the fit in the domain determined by the independent variable (year in this case)",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "The coefficients for the unscaled and unshifted model can be recovered with the convert method: model = model convert() model \\[x \\mapsto \\text{-666 32640635} + \\text{0 34163208}\\,x\\] The individual parameters \\(A\\) and \\(B\\) are the coefficients of our linear model: B, A = model Did manufacturers double the transistor count every two years",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "You have the final formula, \\(\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = xFactor = \\dfrac{e{B}e{A( \\text{year} + 2)}}{e{B}e{A \\text{year}}} = e{2A}\\) where increase in number of transistors is \\(xFactor,\\) number of years is 2, and \\(A\\) is the best fit slope on the semilog function print(f\"Rate of semiconductors added on a chip every 2 years: {np exp(2 * A): 2f}\") Rate of semiconductors added on a chip every 2 years: 1",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "98 Based upon your least-squares regression model, the number of semiconductors per chip increased by a factor of \\(1 98\\) every two years You have a model that predicts the number of semiconductors each year Now compare your model to the actual manufacturing reports Plot the linear regression results and all of the transistor counts Here, use plt semilogy to plot the number of transistors on a log-scale and the year on a linear scale",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "You have defined a three arrays to get to a final model \\(y_i = \\log(\\text{transistor_count}),\\) \\(y_i = A \\cdot \\text{year} + B,\\) and \\(\\log(\\text{transistor_count}) = A\\cdot \\text{year} + B,\\) your variables, transistor_count, year, and yi all have the same dimensions, (179,) NumPy arrays need the same dimensions to make a plot The predicted number of transistors is now \\(\\text{transistor_count}_{\\text{predicted}} = eBe{A\\cdot \\text{year}}\\)",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": "In the next plot, use the fivethirtyeight style sheet The style sheet replicates https://fivethirtyeight com elements Change the matplotlib style with plt style use transistor_count_predicted = np exp(B) * np exp(A * year) transistor_Moores_law = Moores_law(year) plt style use(\"fivethirtyeight\") plt semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\") plt semilogy(year, transistor_count_predicted, label=\"linear regression\") plt",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "plot(year, transistor_Moores_law, label=\"Moore's Law\") plt title( \"MOS transistor count per microprocessor\\n\" + \"every two years \\n\" + \"Transistor count was x{: 2f} higher\" format(np exp(A * 2)) ) plt xlabel(\"year introduced\") plt legend(loc=\"center left\", bbox_to_anchor=(1, 0 5)) plt ylabel(\" of transistors\\nper microprocessor\") Text(0, 0",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "5, ' of transistors\\nper microprocessor') A scatter plot of MOS transistor count per microprocessor every two years with a red line for the ordinary least squares prediction and an orange line for Moores law The linear regression captures the increase in the number of transistors per semiconductors each year In 2015, semiconductor manufacturers claimed they could not keep up with Moores law anymore Your analysis shows that since 1971, the average increase in transistor count was x1",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "98 every 2 years, but Gordon Moore predicted it would be x2 every 2 years That is an amazing prediction Consider the year 2017 Compare the data to your linear regression model and Gordon Moores prediction First, get the transistor counts from the year 2017 You can do this with a Boolean comparator, year == 2017 Then, make a prediction for 2017 with Moores_law defined above and plugging in your best fit constants into your function \\(\\text{transistor_count} = e{B}e{A\\cdot \\text{year}}\\)",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "A great way to compare these measurements is to compare your prediction and Moores prediction to the average transistor count and look at the range of reported values for that year Use the plt plot option, alpha=0 2, to increase the transparency of the data The more opaque the points appear, the more reported values lie on that measurement The green \\(+\\) is the average reported transistor count for 2017 Plot your predictions for \\pm\\frac{1}{2}years",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "transistor_count2017 = transistor_count[year == 2017] print( transistor_count2017 max(), transistor_count2017 min(), transistor_count2017 mean() ) y = np linspace(2016 5, 2017 5) your_model2017 = np exp(B) * np exp(A * y) Moore_Model2017 = Moores_law(y) plt plot( 2017 * np ones(np sum(year == 2017)), transistor_count2017, \"ro\", label=\"2017\", alpha=0 2, ) plt plot(2017, transistor_count2017 mean(), \"g+\", markersize=20, mew=6) plt plot(y, your_model2017, label=\"Your prediction\") plt",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "plot(y, Moore_Model2017, label=\"Moores law\") plt ylabel(\" of transistors\\nper microprocessor\") plt legend() 19200000000 0 250000000 0 7050000000 0 matplotlib legend Legend at 0x7fa70c3043d0 The result is that your model is close to the mean, but Gordon Moores prediction is closer to the maximum number of transistors per microprocessor produced in 2017",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "Even though semiconductor manufacturers thought that the growth would slow, once in 1975 and now again approaching 2025, manufacturers are still producing semiconductors every 2 years that nearly double the number of transistors The linear regression model is much better at predicting the average than extreme values because it satisfies the condition to minimize \\(\\sum y_i - A\\cdot \\text{year}[i]+B2\\) Sharing your results as zipped arrays and a csv The last step, is to share your findings",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": "You created new arrays that represent a linear regression model and Gordon Moores prediction You started this process by importing a csv file into a NumPy array using np loadtxt, to save your model use two approaches np savez: save NumPy arrays for other Python sessions np savetxt: save a csv file with the original data and your predicted data Zipping the arrays into a file Using np savez, you can save thousands of arrays and give them names The function np",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": "load will load the arrays back into the workspace as a dictionary Youll save a five arrays so the next user will have the year, transistor count, predicted transistor count, Gordon Moores predicted count, and fitting constants Add one more variable that other users can use to understand the model, notes",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "notes = \"the arrays in this file are the result of a linear regression model\\n\" notes += \"the arrays include\\nyear: year of manufacture\\n\" notes += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\" notes += \"transistor_count_predicted: linear regression model = exp({: 2f})*exp({: 2f}*year)\\n\" format( B, A ) notes += \"transistor_Moores_law: Moores law =exp({: 2f})*exp({: 2f}*year)\\n\"",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "format( B_M, A_M ) notes += \"regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B\" print(notes) the arrays in this file are the result of a linear regression model the arrays include year: year of manufacture transistor_count: number of transistors reported by manufacturers in a given year transistor_count_predicted: linear regression model = exp(-666 33)*exp(0 34*year) transistor_Moores_law: Moores law =exp(-675 38)*exp(0",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "35*year) regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B np savez( \"mooreslaw_regression npz\", notes=notes, year=year, transistor_count=transistor_count, transistor_count_predicted=transistor_count_predicted, transistor_Moores_law=transistor_Moores_law, regression_csts=(A, B), ) results = np load(\"mooreslaw_regression npz\") print(results[\"regression_csts\"][1]) -666 3264063536233 ls air-quality-data csv mooreslaw_regression npz mooreslaw-tutorial md pairing",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_37"
        },
        {
          "content": "md save-load-arrays md _static text_preprocessing py transistor_data csv tutorial-air-quality-analysis md tutorial-deep-learning-on-mnist md tutorial-deep-reinforcement-learning-with-pong-from-pixels md tutorial-ma md tutorial-nlp-from-scratch tutorial-nlp-from-scratch md tutorial-plotting-fractals tutorial-plotting-fractals md tutorial-static_equilibrium md tutorial-style-guide md tutorial-svd md tutorial-x-ray-image-processing tutorial-x-ray-image-processing md who_covid_19_sit_rep_time_series",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_38"
        },
        {
          "content": "csv The benefit of np savez is you can save hundreds of arrays with different shapes and types Here, you saved 4 arrays that are double precision floating point numbers shape = (179,), one array that was text, and one array of double precision floating point numbers shape = (2,) This is the preferred method for saving NumPy arrays for use in another analysis Creating your own comma separated value file If you want to share data and view the results in a table, then you have to create a text file",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_39"
        },
        {
          "content": "Save the data using np savetxt This function is more limited than np savez Delimited files, like csvs, need 2D arrays Prepare the data for export by creating a new 2D array whose columns contain the data of interest Use the header option to describe the data and the columns of the file Define another variable that contains file information as head",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_40"
        },
        {
          "content": "head = \"the columns in this file are the result of a linear regression model\\n\" head += \"the columns include\\nyear: year of manufacture\\n\" head += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\" head += \"transistor_count_predicted: linear regression model = exp({: 2f})*exp({: 2f}*year)\\n\" format( B, A ) head += \"transistor_Moores_law: Moores law =exp({: 2f})*exp({: 2f}*year)\\n\"",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_41"
        },
        {
          "content": "format( B_M, A_M ) head += \"year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\" print(head) the columns in this file are the result of a linear regression model the columns include year: year of manufacture transistor_count: number of transistors reported by manufacturers in a given year transistor_count_predicted: linear regression model = exp(-666 33)*exp(0 34*year) transistor_Moores_law: Moores law =exp(-675 38)*exp(0",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_42"
        },
        {
          "content": "35*year) year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law: Build a single 2D array to export to csv Tabular data is inherently two dimensional You need to organize your data to fit this 2D structure Use year, transistor_count, transistor_count_predicted, and transistor_Moores_law as the first through fourth columns, respectively Put the calculated constants in the header since they do not fit the (179,) shape The np",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_43"
        },
        {
          "content": "block function appends arrays together to create a new, larger array Arrange the 1D vectors as columns using np newaxis e g year shape (179,)  year[:,np newaxis] shape (179,1) output = np block( [ year[:, np newaxis], transistor_count[:, np newaxis], transistor_count_predicted[:, np newaxis], transistor_Moores_law[:, np newaxis], ] ) Creating the mooreslaw_regression csv with np",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_44"
        },
        {
          "content": "savetxt, use three options to create the desired file format: X = output : use output block to write the data into the file delimiter = ',' : use commas to separate columns in the file header = head : use the header head defined above np savetxt(\"mooreslaw_regression csv\", X=output, delimiter=\",\", header=head) head mooreslaw_regression",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_45"
        },
        {
          "content": "csv  the columns in this file are the result of a linear regression model  the columns include  year: year of manufacture  transistor_count: number of transistors reported by manufacturers in a given year  transistor_count_predicted: linear regression model = exp(-666 33)*exp(0 34*year)  transistor_Moores_law: Moores law =exp(-675 38)*exp(0 35*year)  year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law: 1 971000000000000000e+03,2 250000000000000000e+03,1",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_46"
        },
        {
          "content": "130514785642591505e+03,2 249999999999916326e+03 1 972000000000000000e+03,3 500000000000000000e+03,1 590908400344571419e+03,3 181980515339620069e+03 1 973000000000000000e+03,2 500000000000000000e+03,2 238793840142739100e+03,4 500000000000097316e+03 Wrapping up In conclusion, you have compared historical data for semiconductor manufacturers to Moores law and created a linear regression model to find the average number of transistors added to each microprocessor every two years",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_47"
        },
        {
          "content": "Gordon Moore predicted the number of transistors would double every two years from 1965 through 1975, but the average growth has maintained a consistent increase of \\(\\times 1 98 \\pm 0 01\\) every two years from 1971 through 2019 In 2015, Moore revised his prediction to say Moores law should hold until 2025 [2] You can share these results as a zipped NumPy array file, mooreslaw_regression npz, or as another csv, mooreslaw_regression csv",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_48"
        },
        {
          "content": "The amazing progress in semiconductor manufacturing has enabled new industries and computational power This analysis should give you a small insight into how incredible this growth has been over the last half-century References Moores Law Wikipedia article Accessed Oct 1, 2020 Courtland, Rachel Gordon Moore: The Man Whose Name Means Progress IEEE Spectrum 30 Mar 2015 Transistor Count Wikipedia article Accessed Oct 1, 2020",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_49"
        },
        {
          "content": "previous NumPy Applications next Deep learning on MNIST Contents What youll do Skills youll learn What youll need Building Moores law as an exponential function Loading historical manufacturing data to your workspace Calculating the historical growth curve for transistors Sharing your results as zipped arrays and a csv Zipping the arrays into a file Creating your own comma separated value file Wrapping up References By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/content/mooreslaw-tutorial.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_50"
        }
      ],
      "library": "numpy"
    },
    {
      "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
      "title": "Deep learning on MNIST \u2014 NumPy Tutorials",
      "content": "Binder Repository Open issue .ipynb .md .pdf Deep learning on MNIST Contents Prerequisites Table of contents 1. Load the MNIST dataset 2. Preprocess the data Convert the image data to the floating-point format Convert the labels to floating point through categorical/one-hot encoding 3. Build and train a small neural network from scratch Neural network building blocks with NumPy Model architecture and training summary Compose the model and begin training and testing it Next steps Deep learning on MNIST This tutorial demonstrates how to build a simple feedforward neural network (with one hidden layer) and train it from scratch with NumPy to recognize handwritten digit images. Your deep learning model  one of the most basic artificial neural networks that resembles the original multi-layer perceptron  will learn to classify digits from 0 to 9 from the MNIST dataset. The dataset contains 60,000 training and 10,000 test images and corresponding labels. Each training and test image is of size 784 (or 28x28 pixels)  this will be your input for the neural network. Based on the image inputs and their labels (supervised learning), your neural network will be trained to learn their features using forward propagation and backpropagation (reverse-mode differentiation). The final output of the network is a vector of 10 scores  one for each handwritten digit image. You will also evaluate how good your model is at classifying the images on the test set. This tutorial was adapted from the work by Andrew Trask (with the authors permission). Prerequisites The reader should have some knowledge of Python, NumPy array manipulation, and linear algebra. In addition, you should be familiar with main concepts of deep learning. To refresh the memory, you can take the Python and Linear algebra on n-dimensional arrays tutorials. You are advised to read the Deep learning paper published in 2015 by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, who are regarded as some of the pioneers of the field. You should also consider reading Andrew Trasks Grokking Deep Learning, which teaches deep learning with NumPy. In addition to NumPy, you will be utilizing the following Python standard modules for data loading and processing: urllib for URL handling request for URL opening gzip for gzip file decompression pickle to work with the pickle file format as well as: Matplotlib for data visualization This tutorial can be run locally in an isolated environment, such as Virtualenv or conda. You can use Jupyter Notebook or JupyterLab to run each notebook cell. Dont forget to set up NumPy and Matplotlib. Table of contents Load the MNIST dataset Preprocess the dataset Build and train a small neural network from scratch Next steps 1. Load the MNIST dataset In this section, you will download the zipped MNIST dataset files originally developed by Yann LeCuns research team. (More details of the MNIST dataset are available on Kaggle.) Then, you will transform them into 4 files of NumPy array type using built-in Python modules. Finally, you will split the arrays into training and test sets. 1. Define a variable to store the training/test image/label names of the MNIST dataset in a list: data_sources = { \"training_images\": \"train-images-idx3-ubyte.gz\",  60,000 training images. \"test_images\": \"t10k-images-idx3-ubyte.gz\",  10,000 test images. \"training_labels\": \"train-labels-idx1-ubyte.gz\",  60,000 training labels. \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",  10,000 test labels. } 2. Load the data. First check if the data is stored locally; if not, then download it. import requests import os data_dir = \"../_data\" os.makedirs(data_dir, exist_ok=True) base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\" for fname in data_sources.values(): fpath = os.path.join(data_dir, fname) if not os.path.exists(fpath): print(\"Downloading file: \" + fname) resp = requests.get(base_url + fname, stream=True, **request_opts) resp.raise_for_status()  Ensure download was succesful with open(fpath, \"wb\") as fh: for chunk in resp.iter_content(chunk_size=128): fh.write(chunk) 3. Decompress the 4 files and create 4 ndarrays, saving them into a dictionary. Each original image is of size 28x28 and neural networks normally expect a 1D vector input; therefore, you also need to reshape the images by multiplying 28 by 28 (784). import gzip import numpy as np mnist_dataset = {}  Images for key in (\"training_images\", \"test_images\"): with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file: mnist_dataset[key] = np.frombuffer( mnist_file.read(), np.uint8, offset=16 ).reshape(-1, 28 * 28)  Labels for key in (\"training_labels\", \"test_labels\"): with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file: mnist_dataset[key] = np.frombuffer(mnist_file.read(), np.uint8, offset=8) 4. Split the data into training and test sets using the standard notation of x for data and y for labels, calling the training and test set images x_train and x_test, and the labels y_train and y_test: x_train, y_train, x_test, y_test = ( mnist_dataset[\"training_images\"], mnist_dataset[\"training_labels\"], mnist_dataset[\"test_images\"], mnist_dataset[\"test_labels\"], ) 5. You can confirm that the shape of the image arrays is (60000, 784) and (10000, 784) for training and test sets, respectively, and the labels  (60000,) and (10000,): print( \"The shape of training images: {} and training labels: {}\".format( x_train.shape, y_train.shape ) ) print( \"The shape of test images: {} and test labels: {}\".format( x_test.shape, y_test.shape ) ) The shape of training images: (60000, 784) and training labels: (60000,) The shape of test images: (10000, 784) and test labels: (10000,) 6. And you can inspect some images using Matplotlib: import matplotlib.pyplot as plt  Take the 60,000th image (indexed at 59,999) from the training set,  reshape from (784, ) to (28, 28) to have a valid shape for displaying purposes. mnist_image = x_train[59999, :].reshape(28, 28)  Set the color mapping to grayscale to have a black background. plt.imshow(mnist_image, cmap=\"gray\")  Display the image. plt.show()  Display 5 random images from the training set. num_examples = 5 seed = 147197952744 rng = np.random.default_rng(seed) fig, axes = plt.subplots(1, num_examples) for sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes): ax.imshow(sample.reshape(28, 28), cmap=\"gray\") Above are five images taken from the MNIST training set. Various hand-drawn Arabic numerals are shown, with exact values chosen randomly with each run of the code. Note: You can also visualize a sample image as an array by printing x_train[59999]. Here, 59999 is your 60,000th training image sample (0 would be your first). Your output will be quite long and should contain an array of 8-bit integers: ... 0, 0, 38, 48, 48, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 62, 97, 198, 243, 254, 254, 212, 27, 0, 0, 0, 0, ...  Display the label of the 60,000th image (indexed at 59,999) from the training set. y_train[59999] np.uint8(8) 2. Preprocess the data Neural networks can work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type. When preprocessing the data, you should consider the following processes: vectorization and conversion to a floating-point format. Since the MNIST data is already vectorized and the arrays are of dtype uint8, your next challenge is to convert them to a floating-point format, such as float64 (double-precision): Normalizing the image data: a feature scaling procedure that can speed up the neural network training process by standardizing the distribution of your input data. One-hot/categorical encoding of the image labels. In practice, you can use different types of floating-point precision depending on your goals and you can find more information about that in the Nvidia and Google Cloud blog posts. Convert the image data to the floating-point format The images data contain 8-bit integers encoded in the [0, 255] interval with color values between 0 and 255. You will normalize them into floating-point arrays in the [0, 1] interval by dividing them by 255. 1. Check that the vectorized image data has type uint8: print(\"The data type of training images: {}\".format(x_train.dtype)) print(\"The data type of test images: {}\".format(x_test.dtype)) The data type of training images: uint8 The data type of test images: uint8 2. Normalize the arrays by dividing them by 255 (and thus promoting the data type from uint8 to float64) and then assign the train and test image data variables  x_train and x_test  to training_images and train_labels, respectively. To reduce the model training and evaluation time in this example, only a subset of the training and test images will be used. Both training_images and test_images will contain only 1,000 samples each out of the complete datasets of 60,000 and 10,000 images, respectively. These values can be controlled by changing the training_sample and test_sample below, up to their maximum values of 60,000 and 10,000. training_sample, test_sample = 1000, 1000 training_images = x_train[0:training_sample] / 255 test_images = x_test[0:test_sample] / 255 3. Confirm that the image data has changed to the floating-point format: print(\"The data type of training images: {}\".format(training_images.dtype)) print(\"The data type of test images: {}\".format(test_images.dtype)) The data type of training images: float64 The data type of test images: float64 Note: You can also check that normalization was successful by printing training_images[0] in a notebook cell. Your long output should contain an array of floating-point numbers: ... 0. , 0. , 0.01176471, 0.07058824, 0.07058824, 0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078, 0.65098039, 1. , 0.96862745, 0.49803922, 0. , ... Convert the labels to floating point through categorical/one-hot encoding You will use one-hot encoding to embed each digit label as an all-zero vector with np.zeros() and place 1 for a label index. As a result, your label data will be arrays with 1.0 (or 1.) in the position of each image label. Since there are 10 labels (from 0 to 9) in total, your arrays will look similar to this: array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]) 1. Confirm that the image label data are integers with dtype uint8: print(\"The data type of training labels: {}\".format(y_train.dtype)) print(\"The data type of test labels: {}\".format(y_test.dtype)) The data type of training labels: uint8 The data type of test labels: uint8 2. Define a function that performs one-hot encoding on arrays: def one_hot_encoding(labels, dimension=10):  Define a one-hot variable for an all-zero vector  with 10 dimensions (number labels from 0 to 9). one_hot_labels = labels[..., None] == np.arange(dimension)[None]  Return one-hot encoded labels. return one_hot_labels.astype(np.float64) 3. Encode the labels and assign the values to new variables: training_labels = one_hot_encoding(y_train[:training_sample]) test_labels = one_hot_encoding(y_test[:test_sample]) 4. Check that the data type has changed to floating point: print(\"The data type of training labels: {}\".format(training_labels.dtype)) print(\"The data type of test labels: {}\".format(test_labels.dtype)) The data type of training labels: float64 The data type of test labels: float64 5. Examine a few encoded labels: print(training_labels[0]) print(training_labels[1]) print(training_labels[2]) [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] and compare to the originals: print(y_train[0]) print(y_train[1]) print(y_train[2]) 5 0 4 You have finished preparing the dataset. 3. Build and train a small neural network from scratch In this section you will familiarize yourself with some high-level concepts of the basic building blocks of a deep learning model. You can refer to the original Deep learning research publication for more information. Afterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to identify handwritten digits from the MNIST dataset with a certain level of accuracy. Neural network building blocks with NumPy Layers: These building blocks work as data filters  they process data and learn representations from inputs to better predict the target outputs. You will use 1 hidden layer in your model to pass the inputs forward (forward propagation) and propagate the gradients/error derivatives of a loss function backward (backpropagation). These are input, hidden and output layers. In the hidden (middle) and output (last) layers, the neural network model will compute the weighted sum of inputs. To compute this process, you will use NumPys matrix multiplication function (the dot multiply or np.dot(layer, weights)). Note: For simplicity, the bias term is omitted in this example (there is no np.dot(layer, weights) + bias). Weights: These are important adjustable parameters that the neural network fine-tunes by forward and backward propagating the data. They are optimized through a process called gradient descent. Before the model training starts, the weights are randomly initialized with NumPys Generator.random(). The optimal weights should produce the highest prediction accuracy and the lowest error on the training and test sets. Activation function: Deep learning models are capable of determining non-linear relationships between inputs and outputs and these non-linear functions are usually applied to the output of each layer. You will use a rectified linear unit (ReLU) to the hidden layers output (for example, relu(np.dot(layer, weights)). Regularization: This technique helps prevent the neural network model from overfitting. In this example, you will use a method called dropout  dilution  that randomly sets a number of features in a layer to 0s. You will define it with NumPys Generator.integers() method and apply it to the hidden layer of the network. Loss function: The computation determines the quality of predictions by comparing the image labels (the truth) with the predicted values in the final layers output. For simplicity, you will use a basic total squared error using NumPys np.sum() function (for example, np.sum((final_layer_output - image_labels) ** 2)). Accuracy: This metric measures the accuracy of the networks ability to predict on the data it hasnt seen. Model architecture and training summary Here is a summary of the neural network model architecture and the training process: The input layer: It is the input for the network  the previously preprocessed data that is loaded from training_images into layer_0. The hidden (middle) layer: layer_1 takes the output from the previous layer and performs matrix-multiplication of the input by weights (weights_1) with NumPys np.dot()). Then, this output is passed through the ReLU activation function for non-linearity and then dropout is applied to help with overfitting. The output (last) layer: layer_2 ingests the output from layer_1 and repeats the same dot multiply process with weights_2. The final output returns 10 scores for each of the 0-9 digit labels. The network model ends with a size 10 layer  a 10-dimensional vector. Forward propagation, backpropagation, training loop: In the beginning of model training, your network randomly initializes the weights and feeds the input data forward through the hidden and output layers. This process is the forward pass or forward propagation. Then, the network propagates the signal from the loss function back through the hidden layer and adjusts the weights values with the help of the learning rate parameter (more on that later). Note: In more technical terms, you: Measure the error by comparing the real label of an image (the truth) with the prediction of the model. Differentiate the loss function. Ingest the gradients with the respect to the output, and backpropagate them with the respect to the inputs through the layer(s). Since the network contains tensor operations and weight matrices, backpropagation uses the chain rule. With each iteration (epoch) of the neural network training, this forward and backward propagation cycle adjusts the weights, which is reflected in the accuracy and error metrics. As you train the model, your goal is to minimize the error and maximize the accuracy on the training data, where the model learns from, as well as the test data, where you evaluate the model. Compose the model and begin training and testing it Having covered the main deep learning concepts and the neural network architecture, lets write the code. 1. Well start by creating a new random number generator, providing a seed for reproducibility: seed = 884736743 rng = np.random.default_rng(seed) 2. For the hidden layer, define the ReLU activation function for forward propagation and ReLUs derivative that will be used during backpropagation:  Define ReLU that returns the input if it's positive and 0 otherwise. def relu(x): return (x = 0) * x  Set up a derivative of the ReLU function that returns 1 for a positive input  and 0 otherwise. def relu2deriv(output): return output = 0 3. Set certain default values of hyperparameters, such as: Learning rate: learning_rate  helps limit the magnitude of weight updates to prevent them from overcorrecting. Epochs (iterations): epochs  the number of complete passes  forward and backward propagations  of the data through the network. This parameter can positively or negatively affect the results. The higher the iterations, the longer the learning process may take. Because this is a computationally intensive task, we have chosen a very low number of epochs (20). To get meaningful results, you should choose a much larger number. Size of the hidden (middle) layer in a network: hidden_size  different sizes of the hidden layer can affect the results during training and testing. Size of the input: pixels_per_image  you have established that the image input is 784 (28x28) (in pixels). Number of labels: num_labels  indicates the output number for the output layer where the predictions occur for 10 (0 to 9) handwritten digit labels. learning_rate = 0.005 epochs = 20 hidden_size = 100 pixels_per_image = 784 num_labels = 10 4. Initialize the weight vectors that will be used in the hidden and output layers with random values: weights_1 = 0.2 * rng.random((pixels_per_image, hidden_size)) - 0.1 weights_2 = 0.2 * rng.random((hidden_size, num_labels)) - 0.1 5. Set up the neural networks learning experiment with a training loop and start the training process. Note that the model is evaluated against the test set at each epoch to track its performance over the training epochs. Start the training process:  To store training and test set losses and accurate predictions  for visualization. store_training_loss = [] store_training_accurate_pred = [] store_test_loss = [] store_test_accurate_pred = []  This is a training loop.  Run the learning experiment for a defined number of epochs (iterations). for j in range(epochs):   Training step    Set the initial loss/error and the number of accurate predictions to zero. training_loss = 0.0 training_accurate_predictions = 0  For all images in the training set, perform a forward pass  and backpropagation and adjust the weights accordingly. for i in range(len(training_images)):  Forward propagation/forward pass:  1. The input layer:  Initialize the training image data as inputs. layer_0 = training_images[i]  2. The hidden layer:  Take in the training image data into the middle layer by  matrix-multiplying it by randomly initialized weights. layer_1 = np.dot(layer_0, weights_1)  3. Pass the hidden layer's output through the ReLU activation function. layer_1 = relu(layer_1)  4. Define the dropout function for regularization. dropout_mask = rng.integers(low=0, high=2, size=layer_1.shape)  5. Apply dropout to the hidden layer's output. layer_1 *= dropout_mask * 2  6. The output layer:  Ingest the output of the middle layer into the the final layer  by matrix-multiplying it by randomly initialized weights.  Produce a 10-dimension vector with 10 scores. layer_2 = np.dot(layer_1, weights_2)  Backpropagation/backward pass:  1. Measure the training error (loss function) between the actual  image labels (the truth) and the prediction by the model. training_loss += np.sum((training_labels[i] - layer_2) ** 2)  2. Increment the accurate prediction count. training_accurate_predictions += int( np.argmax(layer_2) == np.argmax(training_labels[i]) )  3. Differentiate the loss function/error. layer_2_delta = training_labels[i] - layer_2  4. Propagate the gradients of the loss function back through the hidden layer. layer_1_delta = np.dot(weights_2, layer_2_delta) * relu2deriv(layer_1)  5. Apply the dropout to the gradients. layer_1_delta *= dropout_mask  6. Update the weights for the middle and input layers  by multiplying them by the learning rate and the gradients. weights_1 += learning_rate * np.outer(layer_0, layer_1_delta) weights_2 += learning_rate * np.outer(layer_1, layer_2_delta)  Store training set losses and accurate predictions. store_training_loss.append(training_loss) store_training_accurate_pred.append(training_accurate_predictions)   Evaluation step    Evaluate model performance on the test set at each epoch.  Unlike the training step, the weights are not modified for each image  (or batch). Therefore the model can be applied to the test images in a  vectorized manner, eliminating the need to loop over each image  individually: results = relu(test_images  weights_1)  weights_2  Measure the error between the actual label (truth) and prediction values. test_loss = np.sum((test_labels - results) ** 2)  Measure prediction accuracy on test set test_accurate_predictions = np.sum( np.argmax(results, axis=1) == np.argmax(test_labels, axis=1) )  Store test set losses and accurate predictions. store_test_loss.append(test_loss) store_test_accurate_pred.append(test_accurate_predictions)  Summarize error and accuracy metrics at each epoch print( ( f\"Epoch: {j}\\n\" f\" Training set error: {training_loss / len(training_images):.3f}\\n\" f\" Training set accuracy: {training_accurate_predictions / len(training_images)}\\n\" f\" Test set error: {test_loss / len(test_images):.3f}\\n\" f\" Test set accuracy: {test_accurate_predictions / len(test_images)}\" ) ) Epoch: 0 Training set error: 0.898 Training set accuracy: 0.397 Test set error: 0.680 Test set accuracy: 0.582 Epoch: 1 Training set error: 0.656 Training set accuracy: 0.633 Test set error: 0.607 Test set accuracy: 0.641 Epoch: 2 Training set error: 0.592 Training set accuracy: 0.68 Test set error: 0.569 Test set accuracy: 0.679 Epoch: 3 Training set error: 0.556 Training set accuracy: 0.7 Test set error: 0.541 Test set accuracy: 0.708 Epoch: 4 Training set error: 0.534 Training set accuracy: 0.732 Test set error: 0.526 Test set accuracy: 0.729 Epoch: 5 Training set error: 0.515 Training set accuracy: 0.715 Test set error: 0.500 Test set accuracy: 0.739 Epoch: 6 Training set error: 0.495 Training set accuracy: 0.748 Test set error: 0.487 Test set accuracy: 0.753 Epoch: 7 Training set error: 0.483 Training set accuracy: 0.769 Test set error: 0.486 Test set accuracy: 0.747 Epoch: 8 Training set error: 0.473 Training set accuracy: 0.776 Test set error: 0.473 Test set accuracy: 0.752 Epoch: 9 Training set error: 0.460 Training set accuracy: 0.788 Test set error: 0.462 Test set accuracy: 0.762 Epoch: 10 Training set error: 0.465 Training set accuracy: 0.769 Test set error: 0.462 Test set accuracy: 0.767 Epoch: 11 Training set error: 0.443 Training set accuracy: 0.801 Test set error: 0.456 Test set accuracy: 0.775 Epoch: 12 Training set error: 0.448 Training set accuracy: 0.795 Test set error: 0.455 Test set accuracy: 0.772 Epoch: 13 Training set error: 0.438 Training set accuracy: 0.787 Test set error: 0.453 Test set accuracy: 0.778 Epoch: 14 Training set error: 0.446 Training set accuracy: 0.791 Test set error: 0.450 Test set accuracy: 0.779 Epoch: 15 Training set error: 0.441 Training set accuracy: 0.788 Test set error: 0.452 Test set accuracy: 0.772 Epoch: 16 Training set error: 0.437 Training set accuracy: 0.786 Test set error: 0.453 Test set accuracy: 0.772 Epoch: 17 Training set error: 0.436 Training set accuracy: 0.794 Test set error: 0.449 Test set accuracy: 0.778 Epoch: 18 Training set error: 0.433 Training set accuracy: 0.801 Test set error: 0.450 Test set accuracy: 0.774 Epoch: 19 Training set error: 0.429 Training set accuracy: 0.785 Test set error: 0.436 Test set accuracy: 0.784 The training process may take many minutes, depending on a number of factors, such as the processing power of the machine you are running the experiment on and the number of epochs. To reduce the waiting time, you can change the epoch (iteration) variable from 100 to a lower number, reset the runtime (which will reset the weights), and run the notebook cells again. After executing the cell above, you can visualize the training and test set errors and accuracy for an instance of this training process. epoch_range = np.arange(epochs) + 1  Starting from 1  The training set metrics. training_metrics = { \"accuracy\": np.asarray(store_training_accurate_pred) / len(training_images), \"error\": np.asarray(store_training_loss) / len(training_images), }  The test set metrics. test_metrics = { \"accuracy\": np.asarray(store_test_accurate_pred) / len(test_images), \"error\": np.asarray(store_test_loss) / len(test_images), }  Display the plots. fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5)) for ax, metrics, title in zip( axes, (training_metrics, test_metrics), (\"Training set\", \"Test set\") ):  Plot the metrics for metric, values in metrics.items(): ax.plot(epoch_range, values, label=metric.capitalize()) ax.set_title(title) ax.set_xlabel(\"Epochs\") ax.legend() plt.show() The training and testing error is shown above in the left and right plots, respectively. As the number of Epochs increases, the total error decreases and the accuracy increases. The accuracy rates that your model reaches during training and testing may be somewhat plausible but you may also find the error rates to be quite high. To reduce the error during training and testing, you can consider changing the simple loss function to, for example, categorical cross-entropy. Other possible solutions are discussed below. Next steps You have learned how to build and train a simple feed-forward neural network from scratch using just NumPy to classify handwritten MNIST digits. To further enhance and optimize your neural network model, you can consider one of a mixture of the following: Increase the training sample size from 1,000 to a higher number (up to 60,000). Use mini-batches and reduce the learning rate. Alter the architecture by introducing more hidden layers to make the network deeper. Combine the cross-entropy loss function with a softmax activation function in the last layer. Introduce convolutional layers: replace the feedforward network with a convolutional neural network architecture. Use a higher epoch size to train longer and add more regularization techniques, such as early stopping, to prevent overfitting. Introduce a validation set for an unbiased valuation of the model fit. Apply batch normalization for faster and more stable training. Tune other parameters, such as the learning rate and hidden layer size. Building a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning. However, for real-world applications you should use specialized frameworks  such as PyTorch, JAX, TensorFlow or MXNet  that provide NumPy-like APIs, have built-in automatic differentiation and GPU support, and are designed for high-performance numerical computing and machine learning. Finally, when developing a machine learning model, you should think about potential ethical issues and apply practices to avoid or mitigate those: Document a trained model with a Model Card - see the Model Cards for Model Reporting paper by Margaret Mitchell et al.. Document a dataset with a Datasheet - see the Datasheets for Datasets paper) by Timnit Gebru et al.. Consider the impact of your model - who is affected by it, who does it benefit - see the article and talk by Pratyusha Kalluri. For more resources, see this blog post by Rachel Thomas and the Radical AI podcast. (Credit to hsjeong5 for demonstrating how to download MNIST without the use of external libraries.) previous Determining Moores Law with real data in NumPy next X-ray image processing Contents Prerequisites Table of contents 1. Load the MNIST dataset 2. Preprocess the data Convert the image data to the floating-point format Convert the labels to floating point through categorical/one-hot encoding 3. Build and train a small neural network from scratch Neural network building blocks with NumPy Model architecture and training summary Compose the model and begin training and testing it Next steps By the NumPy community  Copyright 2020-2025, the NumPy community.",
      "code_blocks": [
        "data_sources = {\n    \"training_images\": \"train-images-idx3-ubyte.gz\",  # 60,000 training images.\n    \"test_images\": \"t10k-images-idx3-ubyte.gz\",  # 10,000 test images.\n    \"training_labels\": \"train-labels-idx1-ubyte.gz\",  # 60,000 training labels.\n    \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",  # 10,000 test labels.\n}",
        "import requests\nimport os\n\ndata_dir = \"../_data\"\nos.makedirs(data_dir, exist_ok=True)\n\nbase_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n\nfor fname in data_sources.values():\n    fpath = os.path.join(data_dir, fname)\n    if not os.path.exists(fpath):\n        print(\"Downloading file: \" + fname)\n        resp = requests.get(base_url + fname, stream=True, **request_opts)\n        resp.raise_for_status()  # Ensure download was succesful\n        with open(fpath, \"wb\") as fh:\n            for chunk in resp.iter_content(chunk_size=128):\n                fh.write(chunk)",
        "import gzip\nimport numpy as np\n\nmnist_dataset = {}\n\n# Images\nfor key in (\"training_images\", \"test_images\"):\n    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n        mnist_dataset[key] = np.frombuffer(\n            mnist_file.read(), np.uint8, offset=16\n        ).reshape(-1, 28 * 28)\n# Labels\nfor key in (\"training_labels\", \"test_labels\"):\n    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n        mnist_dataset[key] = np.frombuffer(mnist_file.read(), np.uint8, offset=8)",
        "x_train, y_train, x_test, y_test = (\n    mnist_dataset[\"training_images\"],\n    mnist_dataset[\"training_labels\"],\n    mnist_dataset[\"test_images\"],\n    mnist_dataset[\"test_labels\"],\n)",
        "(60000, 784)",
        "(10000, 784)",
        "print(\n    \"The shape of training images: {} and training labels: {}\".format(\n        x_train.shape, y_train.shape\n    )\n)\nprint(\n    \"The shape of test images: {} and test labels: {}\".format(\n        x_test.shape, y_test.shape\n    )\n)",
        "The shape of training images: (60000, 784) and training labels: (60000,)\nThe shape of test images: (10000, 784) and test labels: (10000,)",
        "import matplotlib.pyplot as plt\n\n# Take the 60,000th image (indexed at 59,999) from the training set,\n# reshape from (784, ) to (28, 28) to have a valid shape for displaying purposes.\nmnist_image = x_train[59999, :].reshape(28, 28)\n# Set the color mapping to grayscale to have a black background.\nplt.imshow(mnist_image, cmap=\"gray\")\n# Display the image.\nplt.show()",
        "# Display 5 random images from the training set.\nnum_examples = 5\nseed = 147197952744\nrng = np.random.default_rng(seed)\n\nfig, axes = plt.subplots(1, num_examples)\nfor sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes):\n    ax.imshow(sample.reshape(28, 28), cmap=\"gray\")",
        "x_train[59999]",
        "...\n         0,   0,  38,  48,  48,  22,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,  62,  97, 198, 243, 254, 254, 212,  27,   0,   0,   0,   0,\n...",
        "# Display the label of the 60,000th image (indexed at 59,999) from the training set.\ny_train[59999]",
        "np.uint8(8)",
        "print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))",
        "The data type of training images: uint8\nThe data type of test images: uint8",
        "training_images",
        "train_labels",
        "training_images",
        "test_images",
        "training_sample",
        "test_sample",
        "training_sample, test_sample = 1000, 1000\ntraining_images = x_train[0:training_sample] / 255\ntest_images = x_test[0:test_sample] / 255",
        "print(\"The data type of training images: {}\".format(training_images.dtype))\nprint(\"The data type of test images: {}\".format(test_images.dtype))",
        "The data type of training images: float64\nThe data type of test images: float64",
        "training_images[0]",
        "...\n       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n...",
        "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])",
        "print(\"The data type of training labels: {}\".format(y_train.dtype))\nprint(\"The data type of test labels: {}\".format(y_test.dtype))",
        "The data type of training labels: uint8\nThe data type of test labels: uint8",
        "def one_hot_encoding(labels, dimension=10):\n    # Define a one-hot variable for an all-zero vector\n    # with 10 dimensions (number labels from 0 to 9).\n    one_hot_labels = labels[..., None] == np.arange(dimension)[None]\n    # Return one-hot encoded labels.\n    return one_hot_labels.astype(np.float64)",
        "training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])",
        "print(\"The data type of training labels: {}\".format(training_labels.dtype))\nprint(\"The data type of test labels: {}\".format(test_labels.dtype))",
        "The data type of training labels: float64\nThe data type of test labels: float64",
        "print(training_labels[0])\nprint(training_labels[1])\nprint(training_labels[2])",
        "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]",
        "print(y_train[0])\nprint(y_train[1])\nprint(y_train[2])",
        "np.dot(layer, weights)",
        "np.dot(layer, weights) + bias",
        "Generator.random()",
        "relu(np.dot(layer, weights))",
        "Generator.integers()",
        "np.sum((final_layer_output - image_labels) ** 2)",
        "training_images",
        "seed = 884736743\nrng = np.random.default_rng(seed)",
        "# Define ReLU that returns the input if it's positive and 0 otherwise.\ndef relu(x):\n    return (x >= 0) * x\n\n\n# Set up a derivative of the ReLU function that returns 1 for a positive input\n# and 0 otherwise.\ndef relu2deriv(output):\n    return output >= 0",
        "learning_rate",
        "hidden_size",
        "pixels_per_image",
        "learning_rate = 0.005\nepochs = 20\nhidden_size = 100\npixels_per_image = 784\nnum_labels = 10",
        "weights_1 = 0.2 * rng.random((pixels_per_image, hidden_size)) - 0.1\nweights_2 = 0.2 * rng.random((hidden_size, num_labels)) - 0.1",
        "# To store training and test set losses and accurate predictions\n# for visualization.\nstore_training_loss = []\nstore_training_accurate_pred = []\nstore_test_loss = []\nstore_test_accurate_pred = []\n\n# This is a training loop.\n# Run the learning experiment for a defined number of epochs (iterations).\nfor j in range(epochs):\n\n    #################\n    # Training step #\n    #################\n\n    # Set the initial loss/error and the number of accurate predictions to zero.\n    training_loss = 0.0\n    training_accurate_predictions = 0\n\n    # For all images in the training set, perform a forward pass\n    # and backpropagation and adjust the weights accordingly.\n    for i in range(len(training_images)):\n        # Forward propagation/forward pass:\n        # 1. The input layer:\n        #    Initialize the training image data as inputs.\n        layer_0 = training_images[i]\n        # 2. The hidden layer:\n        #    Take in the training image data into the middle layer by\n        #    matrix-multiplying it by randomly initialized weights.\n        layer_1 = np.dot(layer_0, weights_1)\n        # 3. Pass the hidden layer's output through the ReLU activation function.\n        layer_1 = relu(layer_1)\n        # 4. Define the dropout function for regularization.\n        dropout_mask = rng.integers(low=0, high=2, size=layer_1.shape)\n        # 5. Apply dropout to the hidden layer's output.\n        layer_1 *= dropout_mask * 2\n        # 6. The output layer:\n        #    Ingest the output of the middle layer into the the final layer\n        #    by matrix-multiplying it by randomly initialized weights.\n        #    Produce a 10-dimension vector with 10 scores.\n        layer_2 = np.dot(layer_1, weights_2)\n\n        # Backpropagation/backward pass:\n        # 1. Measure the training error (loss function) between the actual\n        #    image labels (the truth) and the prediction by the model.\n        training_loss += np.sum((training_labels[i] - layer_2) ** 2)\n        # 2. Increment the accurate prediction count.\n        training_accurate_predictions += int(\n            np.argmax(layer_2) == np.argmax(training_labels[i])\n        )\n        # 3. Differentiate the loss function/error.\n        layer_2_delta = training_labels[i] - layer_2\n        # 4. Propagate the gradients of the loss function back through the hidden layer.\n        layer_1_delta = np.dot(weights_2, layer_2_delta) * relu2deriv(layer_1)\n        # 5. Apply the dropout to the gradients.\n        layer_1_delta *= dropout_mask\n        # 6. Update the weights for the middle and input layers\n        #    by multiplying them by the learning rate and the gradients.\n        weights_1 += learning_rate * np.outer(layer_0, layer_1_delta)\n        weights_2 += learning_rate * np.outer(layer_1, layer_2_delta)\n\n    # Store training set losses and accurate predictions.\n    store_training_loss.append(training_loss)\n    store_training_accurate_pred.append(training_accurate_predictions)\n\n    ###################\n    # Evaluation step #\n    ###################\n\n    # Evaluate model performance on the test set at each epoch.\n\n    # Unlike the training step, the weights are not modified for each image\n    # (or batch). Therefore the model can be applied to the test images in a\n    # vectorized manner, eliminating the need to loop over each image\n    # individually:\n\n    results = relu(test_images @ weights_1) @ weights_2\n\n    # Measure the error between the actual label (truth) and prediction values.\n    test_loss = np.sum((test_labels - results) ** 2)\n\n    # Measure prediction accuracy on test set\n    test_accurate_predictions = np.sum(\n        np.argmax(results, axis=1) == np.argmax(test_labels, axis=1)\n    )\n\n    # Store test set losses and accurate predictions.\n    store_test_loss.append(test_loss)\n    store_test_accurate_pred.append(test_accurate_predictions)\n\n    # Summarize error and accuracy metrics at each epoch\n    print(\n        (\n            f\"Epoch: {j}\\n\"\n            f\"  Training set error: {training_loss / len(training_images):.3f}\\n\"\n            f\"  Training set accuracy: {training_accurate_predictions / len(training_images)}\\n\"\n            f\"  Test set error: {test_loss / len(test_images):.3f}\\n\"\n            f\"  Test set accuracy: {test_accurate_predictions / len(test_images)}\"\n        )\n    )",
        "Epoch: 0\n  Training set error: 0.898\n  Training set accuracy: 0.397\n  Test set error: 0.680\n  Test set accuracy: 0.582",
        "Epoch: 1\n  Training set error: 0.656\n  Training set accuracy: 0.633\n  Test set error: 0.607\n  Test set accuracy: 0.641",
        "Epoch: 2\n  Training set error: 0.592\n  Training set accuracy: 0.68\n  Test set error: 0.569\n  Test set accuracy: 0.679",
        "Epoch: 3\n  Training set error: 0.556\n  Training set accuracy: 0.7\n  Test set error: 0.541\n  Test set accuracy: 0.708",
        "Epoch: 4\n  Training set error: 0.534\n  Training set accuracy: 0.732\n  Test set error: 0.526\n  Test set accuracy: 0.729",
        "Epoch: 5\n  Training set error: 0.515\n  Training set accuracy: 0.715\n  Test set error: 0.500\n  Test set accuracy: 0.739",
        "Epoch: 6\n  Training set error: 0.495\n  Training set accuracy: 0.748\n  Test set error: 0.487\n  Test set accuracy: 0.753",
        "Epoch: 7\n  Training set error: 0.483\n  Training set accuracy: 0.769\n  Test set error: 0.486\n  Test set accuracy: 0.747",
        "Epoch: 8\n  Training set error: 0.473\n  Training set accuracy: 0.776\n  Test set error: 0.473\n  Test set accuracy: 0.752",
        "Epoch: 9\n  Training set error: 0.460\n  Training set accuracy: 0.788\n  Test set error: 0.462\n  Test set accuracy: 0.762",
        "Epoch: 10\n  Training set error: 0.465\n  Training set accuracy: 0.769\n  Test set error: 0.462\n  Test set accuracy: 0.767",
        "Epoch: 11\n  Training set error: 0.443\n  Training set accuracy: 0.801\n  Test set error: 0.456\n  Test set accuracy: 0.775",
        "Epoch: 12\n  Training set error: 0.448\n  Training set accuracy: 0.795\n  Test set error: 0.455\n  Test set accuracy: 0.772",
        "Epoch: 13\n  Training set error: 0.438\n  Training set accuracy: 0.787\n  Test set error: 0.453\n  Test set accuracy: 0.778",
        "Epoch: 14\n  Training set error: 0.446\n  Training set accuracy: 0.791\n  Test set error: 0.450\n  Test set accuracy: 0.779",
        "Epoch: 15\n  Training set error: 0.441\n  Training set accuracy: 0.788\n  Test set error: 0.452\n  Test set accuracy: 0.772",
        "Epoch: 16\n  Training set error: 0.437\n  Training set accuracy: 0.786\n  Test set error: 0.453\n  Test set accuracy: 0.772",
        "Epoch: 17\n  Training set error: 0.436\n  Training set accuracy: 0.794\n  Test set error: 0.449\n  Test set accuracy: 0.778",
        "Epoch: 18\n  Training set error: 0.433\n  Training set accuracy: 0.801\n  Test set error: 0.450\n  Test set accuracy: 0.774",
        "Epoch: 19\n  Training set error: 0.429\n  Training set accuracy: 0.785\n  Test set error: 0.436\n  Test set accuracy: 0.784",
        "epoch_range = np.arange(epochs) + 1  # Starting from 1\n\n# The training set metrics.\ntraining_metrics = {\n    \"accuracy\": np.asarray(store_training_accurate_pred) / len(training_images),\n    \"error\": np.asarray(store_training_loss) / len(training_images),\n}\n\n# The test set metrics.\ntest_metrics = {\n    \"accuracy\": np.asarray(store_test_accurate_pred) / len(test_images),\n    \"error\": np.asarray(store_test_loss) / len(test_images),\n}\n\n# Display the plots.\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\nfor ax, metrics, title in zip(\n    axes, (training_metrics, test_metrics), (\"Training set\", \"Test set\")\n):\n    # Plot the metrics\n    for metric, values in metrics.items():\n        ax.plot(epoch_range, values, label=metric.capitalize())\n    ax.set_title(title)\n    ax.set_xlabel(\"Epochs\")\n    ax.legend()\nplt.show()"
      ],
      "chunks": [
        {
          "content": "Binder Repository Open issue ipynb md pdf Deep learning on MNIST Contents Prerequisites Table of contents 1 Load the MNIST dataset 2 Preprocess the data Convert the image data to the floating-point format Convert the labels to floating point through categorical/one-hot encoding 3",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_0"
        },
        {
          "content": "Build and train a small neural network from scratch Neural network building blocks with NumPy Model architecture and training summary Compose the model and begin training and testing it Next steps Deep learning on MNIST This tutorial demonstrates how to build a simple feedforward neural network (with one hidden layer) and train it from scratch with NumPy to recognize handwritten digit images",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_1"
        },
        {
          "content": "Your deep learning model  one of the most basic artificial neural networks that resembles the original multi-layer perceptron  will learn to classify digits from 0 to 9 from the MNIST dataset The dataset contains 60,000 training and 10,000 test images and corresponding labels Each training and test image is of size 784 (or 28x28 pixels)  this will be your input for the neural network",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_2"
        },
        {
          "content": "Based on the image inputs and their labels (supervised learning), your neural network will be trained to learn their features using forward propagation and backpropagation (reverse-mode differentiation) The final output of the network is a vector of 10 scores  one for each handwritten digit image You will also evaluate how good your model is at classifying the images on the test set This tutorial was adapted from the work by Andrew Trask (with the authors permission)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_3"
        },
        {
          "content": "Prerequisites The reader should have some knowledge of Python, NumPy array manipulation, and linear algebra In addition, you should be familiar with main concepts of deep learning To refresh the memory, you can take the Python and Linear algebra on n-dimensional arrays tutorials You are advised to read the Deep learning paper published in 2015 by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, who are regarded as some of the pioneers of the field",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_4"
        },
        {
          "content": "You should also consider reading Andrew Trasks Grokking Deep Learning, which teaches deep learning with NumPy In addition to NumPy, you will be utilizing the following Python standard modules for data loading and processing: urllib for URL handling request for URL opening gzip for gzip file decompression pickle to work with the pickle file format as well as: Matplotlib for data visualization This tutorial can be run locally in an isolated environment, such as Virtualenv or conda",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_5"
        },
        {
          "content": "You can use Jupyter Notebook or JupyterLab to run each notebook cell Dont forget to set up NumPy and Matplotlib Table of contents Load the MNIST dataset Preprocess the dataset Build and train a small neural network from scratch Next steps 1 Load the MNIST dataset In this section, you will download the zipped MNIST dataset files originally developed by Yann LeCuns research team (More details of the MNIST dataset are available on Kaggle",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_6"
        },
        {
          "content": ") Then, you will transform them into 4 files of NumPy array type using built-in Python modules Finally, you will split the arrays into training and test sets 1 Define a variable to store the training/test image/label names of the MNIST dataset in a list: data_sources = { \"training_images\": \"train-images-idx3-ubyte gz\",  60,000 training images \"test_images\": \"t10k-images-idx3-ubyte gz\",  10,000 test images \"training_labels\": \"train-labels-idx1-ubyte gz\",  60,000 training labels",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_7"
        },
        {
          "content": "\"test_labels\": \"t10k-labels-idx1-ubyte gz\",  10,000 test labels } 2 Load the data First check if the data is stored locally; if not, then download it import requests import os data_dir = \" /_data\" os makedirs(data_dir, exist_ok=True) base_url = \"https://ossci-datasets s3 amazonaws com/mnist/\" for fname in data_sources values(): fpath = os path join(data_dir, fname) if not os path exists(fpath): print(\"Downloading file: \" + fname) resp = requests",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_8"
        },
        {
          "content": "get(base_url + fname, stream=True, **request_opts) resp raise_for_status()  Ensure download was succesful with open(fpath, \"wb\") as fh: for chunk in resp iter_content(chunk_size=128): fh write(chunk) 3 Decompress the 4 files and create 4 ndarrays, saving them into a dictionary Each original image is of size 28x28 and neural networks normally expect a 1D vector input; therefore, you also need to reshape the images by multiplying 28 by 28 (784)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_9"
        },
        {
          "content": "import gzip import numpy as np mnist_dataset = {}  Images for key in (\"training_images\", \"test_images\"): with gzip open(os path join(data_dir, data_sources[key]), \"rb\") as mnist_file: mnist_dataset[key] = np frombuffer( mnist_file read(), np uint8, offset=16 ) reshape(-1, 28 * 28)  Labels for key in (\"training_labels\", \"test_labels\"): with gzip open(os path join(data_dir, data_sources[key]), \"rb\") as mnist_file: mnist_dataset[key] = np frombuffer(mnist_file read(), np uint8, offset=8) 4",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_10"
        },
        {
          "content": "Split the data into training and test sets using the standard notation of x for data and y for labels, calling the training and test set images x_train and x_test, and the labels y_train and y_test: x_train, y_train, x_test, y_test = ( mnist_dataset[\"training_images\"], mnist_dataset[\"training_labels\"], mnist_dataset[\"test_images\"], mnist_dataset[\"test_labels\"], ) 5",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_11"
        },
        {
          "content": "You can confirm that the shape of the image arrays is (60000, 784) and (10000, 784) for training and test sets, respectively, and the labels  (60000,) and (10000,): print( \"The shape of training images: {} and training labels: {}\" format( x_train shape, y_train shape ) ) print( \"The shape of test images: {} and test labels: {}\" format( x_test shape, y_test",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_12"
        },
        {
          "content": "shape ) ) The shape of training images: (60000, 784) and training labels: (60000,) The shape of test images: (10000, 784) and test labels: (10000,) 6 And you can inspect some images using Matplotlib: import matplotlib pyplot as plt  Take the 60,000th image (indexed at 59,999) from the training set,  reshape from (784, ) to (28, 28) to have a valid shape for displaying purposes mnist_image = x_train[59999, :] reshape(28, 28)  Set the color mapping to grayscale to have a black background plt",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_13"
        },
        {
          "content": "imshow(mnist_image, cmap=\"gray\")  Display the image plt show()  Display 5 random images from the training set num_examples = 5 seed = 147197952744 rng = np random default_rng(seed) fig, axes = plt subplots(1, num_examples) for sample, ax in zip(rng choice(x_train, size=num_examples, replace=False), axes): ax imshow(sample reshape(28, 28), cmap=\"gray\") Above are five images taken from the MNIST training set",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_14"
        },
        {
          "content": "Various hand-drawn Arabic numerals are shown, with exact values chosen randomly with each run of the code Note: You can also visualize a sample image as an array by printing x_train[59999] Here, 59999 is your 60,000th training image sample (0 would be your first) Your output will be quite long and should contain an array of 8-bit integers: 0, 0, 38, 48, 48, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 62, 97, 198, 243, 254, 254, 212, 27, 0, 0, 0, 0,",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_15"
        },
        {
          "content": "Display the label of the 60,000th image (indexed at 59,999) from the training set y_train[59999] np uint8(8) 2 Preprocess the data Neural networks can work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type When preprocessing the data, you should consider the following processes: vectorization and conversion to a floating-point format",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_16"
        },
        {
          "content": "Since the MNIST data is already vectorized and the arrays are of dtype uint8, your next challenge is to convert them to a floating-point format, such as float64 (double-precision): Normalizing the image data: a feature scaling procedure that can speed up the neural network training process by standardizing the distribution of your input data One-hot/categorical encoding of the image labels",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_17"
        },
        {
          "content": "In practice, you can use different types of floating-point precision depending on your goals and you can find more information about that in the Nvidia and Google Cloud blog posts Convert the image data to the floating-point format The images data contain 8-bit integers encoded in the [0, 255] interval with color values between 0 and 255 You will normalize them into floating-point arrays in the [0, 1] interval by dividing them by 255 1",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_18"
        },
        {
          "content": "Check that the vectorized image data has type uint8: print(\"The data type of training images: {}\" format(x_train dtype)) print(\"The data type of test images: {}\" format(x_test dtype)) The data type of training images: uint8 The data type of test images: uint8 2 Normalize the arrays by dividing them by 255 (and thus promoting the data type from uint8 to float64) and then assign the train and test image data variables  x_train and x_test  to training_images and train_labels, respectively",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_19"
        },
        {
          "content": "To reduce the model training and evaluation time in this example, only a subset of the training and test images will be used Both training_images and test_images will contain only 1,000 samples each out of the complete datasets of 60,000 and 10,000 images, respectively These values can be controlled by changing the training_sample and test_sample below, up to their maximum values of 60,000 and 10,000",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_20"
        },
        {
          "content": "training_sample, test_sample = 1000, 1000 training_images = x_train[0:training_sample] / 255 test_images = x_test[0:test_sample] / 255 3 Confirm that the image data has changed to the floating-point format: print(\"The data type of training images: {}\" format(training_images dtype)) print(\"The data type of test images: {}\" format(test_images",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_21"
        },
        {
          "content": "dtype)) The data type of training images: float64 The data type of test images: float64 Note: You can also check that normalization was successful by printing training_images[0] in a notebook cell Your long output should contain an array of floating-point numbers: 0 , 0 , 0 01176471, 0 07058824, 0 07058824, 0 07058824, 0 49411765, 0 53333333, 0 68627451, 0 10196078, 0 65098039, 1 , 0 96862745, 0 49803922, 0 ,",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_22"
        },
        {
          "content": "Convert the labels to floating point through categorical/one-hot encoding You will use one-hot encoding to embed each digit label as an all-zero vector with np zeros() and place 1 for a label index As a result, your label data will be arrays with 1 0 (or 1 ) in the position of each image label Since there are 10 labels (from 0 to 9) in total, your arrays will look similar to this: array([0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ]) 1",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_23"
        },
        {
          "content": "Confirm that the image label data are integers with dtype uint8: print(\"The data type of training labels: {}\" format(y_train dtype)) print(\"The data type of test labels: {}\" format(y_test dtype)) The data type of training labels: uint8 The data type of test labels: uint8 2 Define a function that performs one-hot encoding on arrays: def one_hot_encoding(labels, dimension=10):  Define a one-hot variable for an all-zero vector  with 10 dimensions (number labels from 0 to 9) one_hot_labels = labels[",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_24"
        },
        {
          "content": ", None] == np arange(dimension)[None]  Return one-hot encoded labels return one_hot_labels astype(np float64) 3 Encode the labels and assign the values to new variables: training_labels = one_hot_encoding(y_train[:training_sample]) test_labels = one_hot_encoding(y_test[:test_sample]) 4 Check that the data type has changed to floating point: print(\"The data type of training labels: {}\" format(training_labels dtype)) print(\"The data type of test labels: {}\" format(test_labels",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_25"
        },
        {
          "content": "dtype)) The data type of training labels: float64 The data type of test labels: float64 5 Examine a few encoded labels: print(training_labels[0]) print(training_labels[1]) print(training_labels[2]) [0 0 0 0 0 1 0 0 0 0 ] [1 0 0 0 0 0 0 0 0 0 ] [0 0 0 0 1 0 0 0 0 0 ] and compare to the originals: print(y_train[0]) print(y_train[1]) print(y_train[2]) 5 0 4 You have finished preparing the dataset 3",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_26"
        },
        {
          "content": "Build and train a small neural network from scratch In this section you will familiarize yourself with some high-level concepts of the basic building blocks of a deep learning model You can refer to the original Deep learning research publication for more information Afterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to identify handwritten digits from the MNIST dataset with a certain level of accuracy",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_27"
        },
        {
          "content": "Neural network building blocks with NumPy Layers: These building blocks work as data filters  they process data and learn representations from inputs to better predict the target outputs You will use 1 hidden layer in your model to pass the inputs forward (forward propagation) and propagate the gradients/error derivatives of a loss function backward (backpropagation) These are input, hidden and output layers",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_28"
        },
        {
          "content": "In the hidden (middle) and output (last) layers, the neural network model will compute the weighted sum of inputs To compute this process, you will use NumPys matrix multiplication function (the dot multiply or np dot(layer, weights)) Note: For simplicity, the bias term is omitted in this example (there is no np dot(layer, weights) + bias) Weights: These are important adjustable parameters that the neural network fine-tunes by forward and backward propagating the data",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_29"
        },
        {
          "content": "They are optimized through a process called gradient descent Before the model training starts, the weights are randomly initialized with NumPys Generator random() The optimal weights should produce the highest prediction accuracy and the lowest error on the training and test sets Activation function: Deep learning models are capable of determining non-linear relationships between inputs and outputs and these non-linear functions are usually applied to the output of each layer",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_30"
        },
        {
          "content": "You will use a rectified linear unit (ReLU) to the hidden layers output (for example, relu(np dot(layer, weights)) Regularization: This technique helps prevent the neural network model from overfitting In this example, you will use a method called dropout  dilution  that randomly sets a number of features in a layer to 0s You will define it with NumPys Generator integers() method and apply it to the hidden layer of the network",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_31"
        },
        {
          "content": "Loss function: The computation determines the quality of predictions by comparing the image labels (the truth) with the predicted values in the final layers output For simplicity, you will use a basic total squared error using NumPys np sum() function (for example, np sum((final_layer_output - image_labels) ** 2)) Accuracy: This metric measures the accuracy of the networks ability to predict on the data it hasnt seen",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_32"
        },
        {
          "content": "Model architecture and training summary Here is a summary of the neural network model architecture and the training process: The input layer: It is the input for the network  the previously preprocessed data that is loaded from training_images into layer_0 The hidden (middle) layer: layer_1 takes the output from the previous layer and performs matrix-multiplication of the input by weights (weights_1) with NumPys np dot())",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_33"
        },
        {
          "content": "Then, this output is passed through the ReLU activation function for non-linearity and then dropout is applied to help with overfitting The output (last) layer: layer_2 ingests the output from layer_1 and repeats the same dot multiply process with weights_2 The final output returns 10 scores for each of the 0-9 digit labels The network model ends with a size 10 layer  a 10-dimensional vector",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_34"
        },
        {
          "content": "Forward propagation, backpropagation, training loop: In the beginning of model training, your network randomly initializes the weights and feeds the input data forward through the hidden and output layers This process is the forward pass or forward propagation Then, the network propagates the signal from the loss function back through the hidden layer and adjusts the weights values with the help of the learning rate parameter (more on that later)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_35"
        },
        {
          "content": "Note: In more technical terms, you: Measure the error by comparing the real label of an image (the truth) with the prediction of the model Differentiate the loss function Ingest the gradients with the respect to the output, and backpropagate them with the respect to the inputs through the layer(s) Since the network contains tensor operations and weight matrices, backpropagation uses the chain rule",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_36"
        },
        {
          "content": "With each iteration (epoch) of the neural network training, this forward and backward propagation cycle adjusts the weights, which is reflected in the accuracy and error metrics As you train the model, your goal is to minimize the error and maximize the accuracy on the training data, where the model learns from, as well as the test data, where you evaluate the model",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_37"
        },
        {
          "content": "Compose the model and begin training and testing it Having covered the main deep learning concepts and the neural network architecture, lets write the code 1 Well start by creating a new random number generator, providing a seed for reproducibility: seed = 884736743 rng = np random default_rng(seed) 2",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_38"
        },
        {
          "content": "For the hidden layer, define the ReLU activation function for forward propagation and ReLUs derivative that will be used during backpropagation:  Define ReLU that returns the input if it's positive and 0 otherwise def relu(x): return (x = 0) * x  Set up a derivative of the ReLU function that returns 1 for a positive input  and 0 otherwise def relu2deriv(output): return output = 0 3",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_39"
        },
        {
          "content": "Set certain default values of hyperparameters, such as: Learning rate: learning_rate  helps limit the magnitude of weight updates to prevent them from overcorrecting Epochs (iterations): epochs  the number of complete passes  forward and backward propagations  of the data through the network This parameter can positively or negatively affect the results The higher the iterations, the longer the learning process may take",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_40"
        },
        {
          "content": "Because this is a computationally intensive task, we have chosen a very low number of epochs (20) To get meaningful results, you should choose a much larger number Size of the hidden (middle) layer in a network: hidden_size  different sizes of the hidden layer can affect the results during training and testing Size of the input: pixels_per_image  you have established that the image input is 784 (28x28) (in pixels)",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_41"
        },
        {
          "content": "Number of labels: num_labels  indicates the output number for the output layer where the predictions occur for 10 (0 to 9) handwritten digit labels learning_rate = 0 005 epochs = 20 hidden_size = 100 pixels_per_image = 784 num_labels = 10 4 Initialize the weight vectors that will be used in the hidden and output layers with random values: weights_1 = 0 2 * rng random((pixels_per_image, hidden_size)) - 0 1 weights_2 = 0 2 * rng random((hidden_size, num_labels)) - 0 1 5",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_42"
        },
        {
          "content": "Set up the neural networks learning experiment with a training loop and start the training process Note that the model is evaluated against the test set at each epoch to track its performance over the training epochs Start the training process:  To store training and test set losses and accurate predictions  for visualization store_training_loss = [] store_training_accurate_pred = [] store_test_loss = [] store_test_accurate_pred = []  This is a training loop",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_43"
        },
        {
          "content": "Run the learning experiment for a defined number of epochs (iterations) for j in range(epochs):   Training step    Set the initial loss/error and the number of accurate predictions to zero training_loss = 0 0 training_accurate_predictions = 0  For all images in the training set, perform a forward pass  and backpropagation and adjust the weights accordingly for i in range(len(training_images)):  Forward propagation/forward pass:  1 The input layer:  Initialize the training image data as inputs",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_44"
        },
        {
          "content": "layer_0 = training_images[i]  2 The hidden layer:  Take in the training image data into the middle layer by  matrix-multiplying it by randomly initialized weights layer_1 = np dot(layer_0, weights_1)  3 Pass the hidden layer's output through the ReLU activation function layer_1 = relu(layer_1)  4 Define the dropout function for regularization dropout_mask = rng integers(low=0, high=2, size=layer_1 shape)  5 Apply dropout to the hidden layer's output layer_1 *= dropout_mask * 2  6",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_45"
        },
        {
          "content": "The output layer:  Ingest the output of the middle layer into the the final layer  by matrix-multiplying it by randomly initialized weights Produce a 10-dimension vector with 10 scores layer_2 = np dot(layer_1, weights_2)  Backpropagation/backward pass:  1 Measure the training error (loss function) between the actual  image labels (the truth) and the prediction by the model training_loss += np sum((training_labels[i] - layer_2) ** 2)  2 Increment the accurate prediction count",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_46"
        },
        {
          "content": "training_accurate_predictions += int( np argmax(layer_2) == np argmax(training_labels[i]) )  3 Differentiate the loss function/error layer_2_delta = training_labels[i] - layer_2  4 Propagate the gradients of the loss function back through the hidden layer layer_1_delta = np dot(weights_2, layer_2_delta) * relu2deriv(layer_1)  5 Apply the dropout to the gradients layer_1_delta *= dropout_mask  6",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_47"
        },
        {
          "content": "Update the weights for the middle and input layers  by multiplying them by the learning rate and the gradients weights_1 += learning_rate * np outer(layer_0, layer_1_delta) weights_2 += learning_rate * np outer(layer_1, layer_2_delta)  Store training set losses and accurate predictions store_training_loss append(training_loss) store_training_accurate_pred append(training_accurate_predictions)   Evaluation step    Evaluate model performance on the test set at each epoch",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_48"
        },
        {
          "content": "Unlike the training step, the weights are not modified for each image  (or batch) Therefore the model can be applied to the test images in a  vectorized manner, eliminating the need to loop over each image  individually: results = relu(test_images  weights_1)  weights_2  Measure the error between the actual label (truth) and prediction values test_loss = np sum((test_labels - results) ** 2)  Measure prediction accuracy on test set test_accurate_predictions = np sum( np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_49"
        },
        {
          "content": "argmax(results, axis=1) == np argmax(test_labels, axis=1) )  Store test set losses and accurate predictions store_test_loss append(test_loss) store_test_accurate_pred append(test_accurate_predictions)  Summarize error and accuracy metrics at each epoch print( ( f\"Epoch: {j}\\n\" f\" Training set error: {training_loss / len(training_images): 3f}\\n\" f\" Training set accuracy: {training_accurate_predictions / len(training_images)}\\n\" f\" Test set error: {test_loss / len(test_images):",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_50"
        },
        {
          "content": "3f}\\n\" f\" Test set accuracy: {test_accurate_predictions / len(test_images)}\" ) ) Epoch: 0 Training set error: 0 898 Training set accuracy: 0 397 Test set error: 0 680 Test set accuracy: 0 582 Epoch: 1 Training set error: 0 656 Training set accuracy: 0 633 Test set error: 0 607 Test set accuracy: 0 641 Epoch: 2 Training set error: 0 592 Training set accuracy: 0 68 Test set error: 0 569 Test set accuracy: 0 679 Epoch: 3 Training set error: 0 556 Training set accuracy: 0 7 Test set error: 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_51"
        },
        {
          "content": "541 Test set accuracy: 0 708 Epoch: 4 Training set error: 0 534 Training set accuracy: 0 732 Test set error: 0 526 Test set accuracy: 0 729 Epoch: 5 Training set error: 0 515 Training set accuracy: 0 715 Test set error: 0 500 Test set accuracy: 0 739 Epoch: 6 Training set error: 0 495 Training set accuracy: 0 748 Test set error: 0 487 Test set accuracy: 0 753 Epoch: 7 Training set error: 0 483 Training set accuracy: 0 769 Test set error: 0 486 Test set accuracy: 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_52"
        },
        {
          "content": "747 Epoch: 8 Training set error: 0 473 Training set accuracy: 0 776 Test set error: 0 473 Test set accuracy: 0 752 Epoch: 9 Training set error: 0 460 Training set accuracy: 0 788 Test set error: 0 462 Test set accuracy: 0 762 Epoch: 10 Training set error: 0 465 Training set accuracy: 0 769 Test set error: 0 462 Test set accuracy: 0 767 Epoch: 11 Training set error: 0 443 Training set accuracy: 0 801 Test set error: 0 456 Test set accuracy: 0 775 Epoch: 12 Training set error: 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_53"
        },
        {
          "content": "448 Training set accuracy: 0 795 Test set error: 0 455 Test set accuracy: 0 772 Epoch: 13 Training set error: 0 438 Training set accuracy: 0 787 Test set error: 0 453 Test set accuracy: 0 778 Epoch: 14 Training set error: 0 446 Training set accuracy: 0 791 Test set error: 0 450 Test set accuracy: 0 779 Epoch: 15 Training set error: 0 441 Training set accuracy: 0 788 Test set error: 0 452 Test set accuracy: 0 772 Epoch: 16 Training set error: 0 437 Training set accuracy: 0 786 Test set error: 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_54"
        },
        {
          "content": "453 Test set accuracy: 0 772 Epoch: 17 Training set error: 0 436 Training set accuracy: 0 794 Test set error: 0 449 Test set accuracy: 0 778 Epoch: 18 Training set error: 0 433 Training set accuracy: 0 801 Test set error: 0 450 Test set accuracy: 0 774 Epoch: 19 Training set error: 0 429 Training set accuracy: 0 785 Test set error: 0 436 Test set accuracy: 0",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_55"
        },
        {
          "content": "784 The training process may take many minutes, depending on a number of factors, such as the processing power of the machine you are running the experiment on and the number of epochs To reduce the waiting time, you can change the epoch (iteration) variable from 100 to a lower number, reset the runtime (which will reset the weights), and run the notebook cells again",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_56"
        },
        {
          "content": "After executing the cell above, you can visualize the training and test set errors and accuracy for an instance of this training process epoch_range = np arange(epochs) + 1  Starting from 1  The training set metrics training_metrics = { \"accuracy\": np asarray(store_training_accurate_pred) / len(training_images), \"error\": np asarray(store_training_loss) / len(training_images), }  The test set metrics test_metrics = { \"accuracy\": np asarray(store_test_accurate_pred) / len(test_images), \"error\": np",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_57"
        },
        {
          "content": "asarray(store_test_loss) / len(test_images), }  Display the plots fig, axes = plt subplots(nrows=1, ncols=2, figsize=(15, 5)) for ax, metrics, title in zip( axes, (training_metrics, test_metrics), (\"Training set\", \"Test set\") ):  Plot the metrics for metric, values in metrics items(): ax plot(epoch_range, values, label=metric capitalize()) ax set_title(title) ax set_xlabel(\"Epochs\") ax legend() plt show() The training and testing error is shown above in the left and right plots, respectively",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_58"
        },
        {
          "content": "As the number of Epochs increases, the total error decreases and the accuracy increases The accuracy rates that your model reaches during training and testing may be somewhat plausible but you may also find the error rates to be quite high To reduce the error during training and testing, you can consider changing the simple loss function to, for example, categorical cross-entropy Other possible solutions are discussed below",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_59"
        },
        {
          "content": "Next steps You have learned how to build and train a simple feed-forward neural network from scratch using just NumPy to classify handwritten MNIST digits To further enhance and optimize your neural network model, you can consider one of a mixture of the following: Increase the training sample size from 1,000 to a higher number (up to 60,000) Use mini-batches and reduce the learning rate Alter the architecture by introducing more hidden layers to make the network deeper",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_60"
        },
        {
          "content": "Combine the cross-entropy loss function with a softmax activation function in the last layer Introduce convolutional layers: replace the feedforward network with a convolutional neural network architecture Use a higher epoch size to train longer and add more regularization techniques, such as early stopping, to prevent overfitting Introduce a validation set for an unbiased valuation of the model fit Apply batch normalization for faster and more stable training",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_61"
        },
        {
          "content": "Tune other parameters, such as the learning rate and hidden layer size Building a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning However, for real-world applications you should use specialized frameworks  such as PyTorch, JAX, TensorFlow or MXNet  that provide NumPy-like APIs, have built-in automatic differentiation and GPU support, and are designed for high-performance numerical computing and machine learning",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_62"
        },
        {
          "content": "Finally, when developing a machine learning model, you should think about potential ethical issues and apply practices to avoid or mitigate those: Document a trained model with a Model Card - see the Model Cards for Model Reporting paper by Margaret Mitchell et al Document a dataset with a Datasheet - see the Datasheets for Datasets paper) by Timnit Gebru et al Consider the impact of your model - who is affected by it, who does it benefit - see the article and talk by Pratyusha Kalluri",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_63"
        },
        {
          "content": "For more resources, see this blog post by Rachel Thomas and the Radical AI podcast (Credit to hsjeong5 for demonstrating how to download MNIST without the use of external libraries ) previous Determining Moores Law with real data in NumPy next X-ray image processing Contents Prerequisites Table of contents 1 Load the MNIST dataset 2 Preprocess the data Convert the image data to the floating-point format Convert the labels to floating point through categorical/one-hot encoding 3",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_64"
        },
        {
          "content": "Build and train a small neural network from scratch Neural network building blocks with NumPy Model architecture and training summary Compose the model and begin training and testing it Next steps By the NumPy community  Copyright 2020-2025, the NumPy community",
          "url": "https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html#main-content",
          "library": "numpy",
          "chunk_id": "numpy_65"
        }
      ],
      "library": "numpy"
    }
  ],
  "scraped_at": 9508.94999625
}